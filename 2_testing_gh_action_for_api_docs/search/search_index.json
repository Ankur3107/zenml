{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the ZenML Api Docs Utils The utils module contains utility functions handling analytics, reading and writing YAML data as well as other general purpose functions. Orchestrators An orchestrator is a special kind of backend that manages the running of each step of the pipeline. Orchestrators administer the actual pipeline runs. You can think of it as the 'root' of any pipeline job that you run during your experimentation. ZenML supports a local orchestrator out of the box which allows you to run your pipelines in a local environment. We also support using Apache Airflow as the orchestrator to handle the steps of your pipeline. Metadata Stores The configuration of each pipeline, step, backend, and produced artifacts are all tracked within the metadata store. The metadata store is an SQL database, and can be sqlite or mysql . Metadata are the pieces of information tracked about the pipelines, experiments and configurations that you are running with ZenML. Metadata are stored inside the metadata store. Logger Integrations The ZenML integrations module contains sub-modules for each integration that we support. This includes orchestrators like Apache Airflow, visualization tools like the facets library, as well as deep learning libraries like PyTorch. Constants Runtime Configuration Repository Pipelines A ZenML pipeline is a sequence of tasks that execute in a specific order and yield artifacts. The artifacts are stored within the artifact store and indexed via the metadata store. Each individual task within a pipeline is known as a step. The standard pipelines within ZenML are designed to have easy interfaces to add pre-decided steps, with the order also pre-decided. Other sorts of pipelines can be created as well from scratch, building on the BasePipeline class. Pipelines can be written as simple functions. They are created by using decorators appropriate to the specific use case you have. The moment it is run , a pipeline is compiled and passed directly to the orchestrator. Io The io module handles file operations for the ZenML package. It offers a standard interface for reading, writing and manipulating files and directories. It is heavily influenced and inspired by the io module of tfx . Artifact Stores An artifact store is a place where artifacts are stored. These artifacts may have been produced by the pipeline steps, or they may be the data first ingested into a pipeline via an ingestion step. Definitions of the BaseArtifactStore class and the LocalArtifactStore that builds on it are in this module. Other artifact stores corresponding to specific integrations are to be found in the integrations module. For example, the GCPArtifactStore , used when running ZenML on Google Cloud Platform, is defined in integrations.gcp.artifact_stores . Container Registries Config The config module contains classes and functions that manage user-specific configuration. ZenML's configuration is stored in a file called .zenglobal.json , located on the user's directory for configuration files. (The exact location differs from operating system to operating system.) The GlobalConfig class is the main class in this module. It provides a Pydantic configuration object that is used to store and retrieve configuration. This GlobalConfig object handles the serialization and deserialization of the configuration options that are stored in the file in order to persist the configuration across sessions. Exceptions ZenML specific exception definitions Stack Materializers Materializers are used to convert a ZenML artifact into a specific format. They are most often used to handle the input or output of ZenML steps, and can be extended by building on the BaseMaterializer class. Enums Artifacts Artifacts are the data that power your experimentation and model training. It is actually steps that produce artifacts, which are then stored in the artifact store. Artifacts are written in the signature of a step like so: .. code:: python // Some code def my_step(first_artifact: int, second_artifact: torch.nn.Module -> int: # first_artifact is an integer # second_artifact is a torch.nn.Module return 1 Artifacts can be serialized and deserialized (i.e. written and read from the Artifact Store) in various ways like TFRecords or saved model pickles, depending on what the step produces.The serialization and deserialization logic of artifacts is defined by the appropriate Materializer. Steps A step is a single piece or stage of a ZenML pipeline. Think of each step as being one of the nodes of a Directed Acyclic Graph (or DAG). Steps are responsible for one aspect of processing or interacting with the data / artifacts in the pipeline. ZenML currently implements a basic step interface, but there will be other more customized interfaces (layered in a hierarchy) for specialized implementations. Conceptually, a Step is a discrete and independent part of a pipeline that is responsible for one particular aspect of data manipulation inside a ZenML pipeline. Steps can be subclassed from the BaseStep class, or used via our @step decorator. Post Execution After executing a pipeline, the user needs to be able to fetch it from history and perform certain tasks. The post_execution submodule provides a set of interfaces with which the user can interact with artifacts, the pipeline, steps, and the post-run pipeline object. Visualizers The visualizers module offers a way of constructing and displaying visualizations of steps and pipeline results. The BaseVisualizer class is at the root of all the other visualizers, including options to view the results of pipeline runs, steps and pipelines themselves.","title":"ZenML"},{"location":"#welcome-to-the-zenml-api-docs","text":"","title":"Welcome to the ZenML Api Docs"},{"location":"#utils","text":"The utils module contains utility functions handling analytics, reading and writing YAML data as well as other general purpose functions.","title":"Utils"},{"location":"#orchestrators","text":"An orchestrator is a special kind of backend that manages the running of each step of the pipeline. Orchestrators administer the actual pipeline runs. You can think of it as the 'root' of any pipeline job that you run during your experimentation. ZenML supports a local orchestrator out of the box which allows you to run your pipelines in a local environment. We also support using Apache Airflow as the orchestrator to handle the steps of your pipeline.","title":"Orchestrators"},{"location":"#metadata-stores","text":"The configuration of each pipeline, step, backend, and produced artifacts are all tracked within the metadata store. The metadata store is an SQL database, and can be sqlite or mysql . Metadata are the pieces of information tracked about the pipelines, experiments and configurations that you are running with ZenML. Metadata are stored inside the metadata store.","title":"Metadata Stores"},{"location":"#logger","text":"","title":"Logger"},{"location":"#integrations","text":"The ZenML integrations module contains sub-modules for each integration that we support. This includes orchestrators like Apache Airflow, visualization tools like the facets library, as well as deep learning libraries like PyTorch.","title":"Integrations"},{"location":"#constants","text":"","title":"Constants"},{"location":"#runtime-configuration","text":"","title":"Runtime Configuration"},{"location":"#repository","text":"","title":"Repository"},{"location":"#pipelines","text":"A ZenML pipeline is a sequence of tasks that execute in a specific order and yield artifacts. The artifacts are stored within the artifact store and indexed via the metadata store. Each individual task within a pipeline is known as a step. The standard pipelines within ZenML are designed to have easy interfaces to add pre-decided steps, with the order also pre-decided. Other sorts of pipelines can be created as well from scratch, building on the BasePipeline class. Pipelines can be written as simple functions. They are created by using decorators appropriate to the specific use case you have. The moment it is run , a pipeline is compiled and passed directly to the orchestrator.","title":"Pipelines"},{"location":"#io","text":"The io module handles file operations for the ZenML package. It offers a standard interface for reading, writing and manipulating files and directories. It is heavily influenced and inspired by the io module of tfx .","title":"Io"},{"location":"#artifact-stores","text":"An artifact store is a place where artifacts are stored. These artifacts may have been produced by the pipeline steps, or they may be the data first ingested into a pipeline via an ingestion step. Definitions of the BaseArtifactStore class and the LocalArtifactStore that builds on it are in this module. Other artifact stores corresponding to specific integrations are to be found in the integrations module. For example, the GCPArtifactStore , used when running ZenML on Google Cloud Platform, is defined in integrations.gcp.artifact_stores .","title":"Artifact Stores"},{"location":"#container-registries","text":"","title":"Container Registries"},{"location":"#config","text":"The config module contains classes and functions that manage user-specific configuration. ZenML's configuration is stored in a file called .zenglobal.json , located on the user's directory for configuration files. (The exact location differs from operating system to operating system.) The GlobalConfig class is the main class in this module. It provides a Pydantic configuration object that is used to store and retrieve configuration. This GlobalConfig object handles the serialization and deserialization of the configuration options that are stored in the file in order to persist the configuration across sessions.","title":"Config"},{"location":"#exceptions","text":"ZenML specific exception definitions","title":"Exceptions"},{"location":"#stack","text":"","title":"Stack"},{"location":"#materializers","text":"Materializers are used to convert a ZenML artifact into a specific format. They are most often used to handle the input or output of ZenML steps, and can be extended by building on the BaseMaterializer class.","title":"Materializers"},{"location":"#enums","text":"","title":"Enums"},{"location":"#artifacts","text":"Artifacts are the data that power your experimentation and model training. It is actually steps that produce artifacts, which are then stored in the artifact store. Artifacts are written in the signature of a step like so: .. code:: python // Some code def my_step(first_artifact: int, second_artifact: torch.nn.Module -> int: # first_artifact is an integer # second_artifact is a torch.nn.Module return 1 Artifacts can be serialized and deserialized (i.e. written and read from the Artifact Store) in various ways like TFRecords or saved model pickles, depending on what the step produces.The serialization and deserialization logic of artifacts is defined by the appropriate Materializer.","title":"Artifacts"},{"location":"#steps","text":"A step is a single piece or stage of a ZenML pipeline. Think of each step as being one of the nodes of a Directed Acyclic Graph (or DAG). Steps are responsible for one aspect of processing or interacting with the data / artifacts in the pipeline. ZenML currently implements a basic step interface, but there will be other more customized interfaces (layered in a hierarchy) for specialized implementations. Conceptually, a Step is a discrete and independent part of a pipeline that is responsible for one particular aspect of data manipulation inside a ZenML pipeline. Steps can be subclassed from the BaseStep class, or used via our @step decorator.","title":"Steps"},{"location":"#post-execution","text":"After executing a pipeline, the user needs to be able to fetch it from history and perform certain tasks. The post_execution submodule provides a set of interfaces with which the user can interact with artifacts, the pipeline, steps, and the post-run pipeline object.","title":"Post Execution"},{"location":"#visualizers","text":"The visualizers module offers a way of constructing and displaying visualizations of steps and pipeline results. The BaseVisualizer class is at the root of all the other visualizers, including options to view the results of pipeline runs, steps and pipelines themselves.","title":"Visualizers"},{"location":"cli/","text":"Cli zenml.cli special ZenML CLI The ZenML CLI tool is usually downloaded and installed via PyPI and a pip install zenml command. Please see the Installation & Setup section above for more information about that process. How to use the CLI Our CLI behaves similarly to many other CLIs for basic features. In order to find out which version of ZenML you are running, type: .. code:: bash zenml version If you ever need more information on exactly what a certain command will do, use the --help flag attached to the end of your command string. For example, to get a sense of all of the commands available to you while using the zenml command, type: .. code:: bash zenml --help If you were instead looking to know more about a specific command, you can type something like this: .. code:: bash zenml metadata-store register --help This will give you information about how to register a metadata store. (See below for more on that). Beginning a Project In order to start working on your project, initialize a ZenML repository within your current directory with ZenML\u2019s own config and resource management tools: .. code:: bash zenml init This is all you need to begin using all the MLOps goodness that ZenML provides! By default, zenml init will install its own hidden .zen folder inside the current directory from which you are running the command. You can also pass in a directory path manually using the --repo_path option: .. code:: bash zenml init --repo_path /path/to/dir If you wish to specify that you do not want analytics to be transmitted back to ZenML about your usage of the tool, pass in False to the --analytics_opt_in option: .. code:: bash zenml init --analytics_opt_in false If you wish to delete all data relating to your project from the directory, use the zenml clean command. This will: delete all pipelines delete all artifacts delete all metadata Note that the clean command is not implemented for the current version. Loading and using pre-built examples If you don\u2019t have a project of your own that you\u2019re currently working on, or if you just want to play around a bit and see some functional code, we\u2019ve got your back! You can use the ZenML CLI tool to download some pre-built examples. We know that working examples are a great way to get to know a tool, so we\u2019ve made some examples for you to use to get started. (This is something that will grow as we add more). To list all the examples available to you, type: .. code:: bash zenml example list If you want more detailed information about a specific example, use the info subcommand in combination with the name of the example, like this: .. code:: bash zenml example info quickstart If you want to pull all the examples into your current working directory (wherever you are executing the zenml command from in your terminal), the CLI will create a zenml_examples folder for you if it doesn\u2019t already exist whenever you use the pull subcommand. The default is to copy all the examples, like this: .. code:: bash zenml example pull If you\u2019d only like to pull a single example, add the name of that example (for example, quickstart ) as an argument to the same command, as follows: .. code:: bash zenml example pull quickstart If you would like to force-redownload the examples, use the --force or -f flag as in this example: .. code:: bash zenml example pull --force This will redownload all the examples afresh, using the same version of ZenML as you currently have installed. If for some reason you want to download examples corresponding to a previous release of ZenML, use the --version or -v flag to specify, as in the following example: .. code:: bash zenml example pull --force --version 0.3.8 If you wish to run the example, allowing the ZenML CLI to do the work of setting up whatever dependencies are required, use the run subcommand: .. code:: bash zenml example run quickstart Using integrations Integrations are the different pieces of a project stack that enable custom functionality. This ranges from bigger libraries like kubeflow for orchestration down to smaller visualization tools like facets . Our CLI is an easy way to get started with these integrations. To list all the integrations available to you, type: zenml integration list To see the requirements for a specific integration, use the requirements command: zenml integration requirements INTEGRATION_NAME If you wish to install the integration, using the requirements listed in the previous command, install allows you to do this for your local environment: zenml integration install INTEGRATION_NAME Note that if you don't specify a specific integration to be installed, the ZenML CLI will install all available integrations. Uninstalling a specific integration is as simple as typing: zenml integration uninstall INTEGRATION_NAME Customizing your Metadata Store The configuration of each pipeline, step, backend, and produced artifacts are all tracked within the metadata store. By default ZenML initializes your repository with a metadata store kept on your local machine. If you wish to register a new metadata store, do so with the register command: .. code:: bash zenml metadata-store register METADATA_STORE_NAME --type METADATA_STORE_TYPE [--OPTIONS] If you wish to list the metadata stores that have already been registered within your ZenML project / repository, type: .. code:: bash zenml metadata-store list If you wish to delete a particular metadata store, pass the name of the metadata store into the CLI with the following command: .. code:: bash zenml metadata-store delete METADATA_STORE_NAME Customizing your Artifact Store The artifact store is where all the inputs and outputs of your pipeline steps are stored. By default, ZenML initializes your repository with an artifact store with everything kept on your local machine. If you wish to register a new artifact store, do so with the register command: .. code:: bash zenml artifact-store register ARTIFACT_STORE_NAME --type ARTIFACT_STORE_TYPE [--OPTIONS] If you wish to list the artifact stores that have already been registered within your ZenML project / repository, type: .. code:: bash zenml artifact-store list If you wish to delete a particular artifact store, pass the name of the artifact store into the CLI with the following command: .. code:: bash zenml artifact-store delete ARTIFACT_STORE_NAME Customizing your Orchestrator An orchestrator is a special kind of backend that manages the running of each step of the pipeline. Orchestrators administer the actual pipeline runs. By default, ZenML initializes your repository with an orchestrator that runs everything on your local machine. If you wish to register a new orchestrator, do so with the register command: .. code:: bash zenml orchestrator register ORCHESTRATOR_NAME --type ORCHESTRATOR_TYPE [--ORCHESTRATOR_OPTIONS] If you wish to list the orchestrators that have already been registered within your ZenML project / repository, type: .. code:: bash zenml orchestrator list If you wish to delete a particular orchestrator, pass the name of the orchestrator into the CLI with the following command: .. code:: bash zenml orchestrator delete ORCHESTRATOR_NAME Customizing your Container Registry The container registry is where all the images that are used by a container-based orchestrator are stored. By default, a default ZenML local stack will not register a container registry. If you wish to register a new container registry, do so with the register command: zenml container-registry register REGISTRY_NAME --type REGISTRY_TYPE [--REGISTRY_OPTIONS] If you want the name of the current container registry, use the get command: zenml container-registry get To list all container registries available and registered for use, use the list command: zenml container-registry list For details about a particular container registry, use the describe command. By default (without a specific registry name passed in) it will describe the active or currently used container registry: zenml container-registry describe [REGISTRY_NAME] To delete a container registry (and all of its contents), use the delete command: zenml container-registry delete Administering the Stack The stack is a grouping of your artifact store, your metadata store and your orchestrator. With the ZenML tool, switching from a local stack to a distributed cloud environment can be accomplished with just a few CLI commands. To register a new stack, you must already have registered the individual components of the stack using the commands listed above. Use the zenml stack register command to register your stack. It takes four arguments as in the following example: .. code:: bash zenml stack register STACK_NAME -m METADATA_STORE_NAME -a ARTIFACT_STORE_NAME -o ORCHESTRATOR_NAME Each corresponding argument should be the name you passed in as an identifier for the artifact store, metadata store or orchestrator when you originally registered it. To list the stacks that you have registered within your current ZenML project, type: .. code:: bash zenml stack list To delete a stack that you have previously registered, type: .. code:: bash zenml stack delete STACK_NAME By default, ZenML uses a local stack whereby all pipelines run on your local computer. If you wish to set a different stack as the current active stack to be used when running your pipeline, type: .. code:: bash zenml stack set STACK_NAME This changes a configuration property within your local environment. To see which stack is currently set as the default active stack, type: .. code:: bash zenml stack get","title":"CLI docs"},{"location":"cli/#cli","text":"","title":"Cli"},{"location":"cli/#zenml.cli","text":"","title":"cli"},{"location":"cli/#zenml.cli--zenml-cli","text":"The ZenML CLI tool is usually downloaded and installed via PyPI and a pip install zenml command. Please see the Installation & Setup section above for more information about that process.","title":"ZenML CLI"},{"location":"cli/#zenml.cli--how-to-use-the-cli","text":"Our CLI behaves similarly to many other CLIs for basic features. In order to find out which version of ZenML you are running, type: .. code:: bash zenml version If you ever need more information on exactly what a certain command will do, use the --help flag attached to the end of your command string. For example, to get a sense of all of the commands available to you while using the zenml command, type: .. code:: bash zenml --help If you were instead looking to know more about a specific command, you can type something like this: .. code:: bash zenml metadata-store register --help This will give you information about how to register a metadata store. (See below for more on that).","title":"How to use the CLI"},{"location":"cli/#zenml.cli--beginning-a-project","text":"In order to start working on your project, initialize a ZenML repository within your current directory with ZenML\u2019s own config and resource management tools: .. code:: bash zenml init This is all you need to begin using all the MLOps goodness that ZenML provides! By default, zenml init will install its own hidden .zen folder inside the current directory from which you are running the command. You can also pass in a directory path manually using the --repo_path option: .. code:: bash zenml init --repo_path /path/to/dir If you wish to specify that you do not want analytics to be transmitted back to ZenML about your usage of the tool, pass in False to the --analytics_opt_in option: .. code:: bash zenml init --analytics_opt_in false If you wish to delete all data relating to your project from the directory, use the zenml clean command. This will: delete all pipelines delete all artifacts delete all metadata Note that the clean command is not implemented for the current version.","title":"Beginning a Project"},{"location":"cli/#zenml.cli--loading-and-using-pre-built-examples","text":"If you don\u2019t have a project of your own that you\u2019re currently working on, or if you just want to play around a bit and see some functional code, we\u2019ve got your back! You can use the ZenML CLI tool to download some pre-built examples. We know that working examples are a great way to get to know a tool, so we\u2019ve made some examples for you to use to get started. (This is something that will grow as we add more). To list all the examples available to you, type: .. code:: bash zenml example list If you want more detailed information about a specific example, use the info subcommand in combination with the name of the example, like this: .. code:: bash zenml example info quickstart If you want to pull all the examples into your current working directory (wherever you are executing the zenml command from in your terminal), the CLI will create a zenml_examples folder for you if it doesn\u2019t already exist whenever you use the pull subcommand. The default is to copy all the examples, like this: .. code:: bash zenml example pull If you\u2019d only like to pull a single example, add the name of that example (for example, quickstart ) as an argument to the same command, as follows: .. code:: bash zenml example pull quickstart If you would like to force-redownload the examples, use the --force or -f flag as in this example: .. code:: bash zenml example pull --force This will redownload all the examples afresh, using the same version of ZenML as you currently have installed. If for some reason you want to download examples corresponding to a previous release of ZenML, use the --version or -v flag to specify, as in the following example: .. code:: bash zenml example pull --force --version 0.3.8 If you wish to run the example, allowing the ZenML CLI to do the work of setting up whatever dependencies are required, use the run subcommand: .. code:: bash zenml example run quickstart","title":"Loading and using pre-built examples"},{"location":"cli/#zenml.cli--using-integrations","text":"Integrations are the different pieces of a project stack that enable custom functionality. This ranges from bigger libraries like kubeflow for orchestration down to smaller visualization tools like facets . Our CLI is an easy way to get started with these integrations. To list all the integrations available to you, type: zenml integration list To see the requirements for a specific integration, use the requirements command: zenml integration requirements INTEGRATION_NAME If you wish to install the integration, using the requirements listed in the previous command, install allows you to do this for your local environment: zenml integration install INTEGRATION_NAME Note that if you don't specify a specific integration to be installed, the ZenML CLI will install all available integrations. Uninstalling a specific integration is as simple as typing: zenml integration uninstall INTEGRATION_NAME","title":"Using integrations"},{"location":"cli/#zenml.cli--customizing-your-metadata-store","text":"The configuration of each pipeline, step, backend, and produced artifacts are all tracked within the metadata store. By default ZenML initializes your repository with a metadata store kept on your local machine. If you wish to register a new metadata store, do so with the register command: .. code:: bash zenml metadata-store register METADATA_STORE_NAME --type METADATA_STORE_TYPE [--OPTIONS] If you wish to list the metadata stores that have already been registered within your ZenML project / repository, type: .. code:: bash zenml metadata-store list If you wish to delete a particular metadata store, pass the name of the metadata store into the CLI with the following command: .. code:: bash zenml metadata-store delete METADATA_STORE_NAME","title":"Customizing your Metadata Store"},{"location":"cli/#zenml.cli--customizing-your-artifact-store","text":"The artifact store is where all the inputs and outputs of your pipeline steps are stored. By default, ZenML initializes your repository with an artifact store with everything kept on your local machine. If you wish to register a new artifact store, do so with the register command: .. code:: bash zenml artifact-store register ARTIFACT_STORE_NAME --type ARTIFACT_STORE_TYPE [--OPTIONS] If you wish to list the artifact stores that have already been registered within your ZenML project / repository, type: .. code:: bash zenml artifact-store list If you wish to delete a particular artifact store, pass the name of the artifact store into the CLI with the following command: .. code:: bash zenml artifact-store delete ARTIFACT_STORE_NAME","title":"Customizing your Artifact Store"},{"location":"cli/#zenml.cli--customizing-your-orchestrator","text":"An orchestrator is a special kind of backend that manages the running of each step of the pipeline. Orchestrators administer the actual pipeline runs. By default, ZenML initializes your repository with an orchestrator that runs everything on your local machine. If you wish to register a new orchestrator, do so with the register command: .. code:: bash zenml orchestrator register ORCHESTRATOR_NAME --type ORCHESTRATOR_TYPE [--ORCHESTRATOR_OPTIONS] If you wish to list the orchestrators that have already been registered within your ZenML project / repository, type: .. code:: bash zenml orchestrator list If you wish to delete a particular orchestrator, pass the name of the orchestrator into the CLI with the following command: .. code:: bash zenml orchestrator delete ORCHESTRATOR_NAME","title":"Customizing your Orchestrator"},{"location":"cli/#zenml.cli--customizing-your-container-registry","text":"The container registry is where all the images that are used by a container-based orchestrator are stored. By default, a default ZenML local stack will not register a container registry. If you wish to register a new container registry, do so with the register command: zenml container-registry register REGISTRY_NAME --type REGISTRY_TYPE [--REGISTRY_OPTIONS] If you want the name of the current container registry, use the get command: zenml container-registry get To list all container registries available and registered for use, use the list command: zenml container-registry list For details about a particular container registry, use the describe command. By default (without a specific registry name passed in) it will describe the active or currently used container registry: zenml container-registry describe [REGISTRY_NAME] To delete a container registry (and all of its contents), use the delete command: zenml container-registry delete","title":"Customizing your Container Registry"},{"location":"cli/#zenml.cli--administering-the-stack","text":"The stack is a grouping of your artifact store, your metadata store and your orchestrator. With the ZenML tool, switching from a local stack to a distributed cloud environment can be accomplished with just a few CLI commands. To register a new stack, you must already have registered the individual components of the stack using the commands listed above. Use the zenml stack register command to register your stack. It takes four arguments as in the following example: .. code:: bash zenml stack register STACK_NAME -m METADATA_STORE_NAME -a ARTIFACT_STORE_NAME -o ORCHESTRATOR_NAME Each corresponding argument should be the name you passed in as an identifier for the artifact store, metadata store or orchestrator when you originally registered it. To list the stacks that you have registered within your current ZenML project, type: .. code:: bash zenml stack list To delete a stack that you have previously registered, type: .. code:: bash zenml stack delete STACK_NAME By default, ZenML uses a local stack whereby all pipelines run on your local computer. If you wish to set a different stack as the current active stack to be used when running your pipeline, type: .. code:: bash zenml stack set STACK_NAME This changes a configuration property within your local environment. To see which stack is currently set as the default active stack, type: .. code:: bash zenml stack get","title":"Administering the Stack"},{"location":"api_docs/artifact_stores/","text":"Artifact Stores zenml.artifact_stores special An artifact store is a place where artifacts are stored. These artifacts may have been produced by the pipeline steps, or they may be the data first ingested into a pipeline via an ingestion step. Definitions of the BaseArtifactStore class and the LocalArtifactStore that builds on it are in this module. Other artifact stores corresponding to specific integrations are to be found in the integrations module. For example, the GCPArtifactStore , used when running ZenML on Google Cloud Platform, is defined in integrations.gcp.artifact_stores . base_artifact_store BaseArtifactStore ( StackComponent , ABC ) pydantic-model Base class for all ZenML artifact stores. Attributes: Name Type Description path str The root path of the artifact store. Source code in zenml/artifact_stores/base_artifact_store.py class BaseArtifactStore ( StackComponent , ABC ): \"\"\"Base class for all ZenML artifact stores. Attributes: path: The root path of the artifact store. \"\"\" path : str @property def type ( self ) -> StackComponentType : \"\"\"The component type.\"\"\" return StackComponentType . ARTIFACT_STORE @property @abstractmethod def flavor ( self ) -> ArtifactStoreFlavor : \"\"\"The artifact store flavor.\"\"\" flavor : ArtifactStoreFlavor property readonly The artifact store flavor. type : StackComponentType property readonly The component type. local_artifact_store LocalArtifactStore ( BaseArtifactStore ) pydantic-model Artifact Store for local artifacts. Source code in zenml/artifact_stores/local_artifact_store.py class LocalArtifactStore ( BaseArtifactStore ): \"\"\"Artifact Store for local artifacts.\"\"\" supports_local_execution = True supports_remote_execution = False @property def flavor ( self ) -> ArtifactStoreFlavor : \"\"\"The artifact store flavor.\"\"\" return ArtifactStoreFlavor . LOCAL @validator ( \"path\" ) def ensure_path_is_local ( cls , path : str ) -> str : \"\"\"Ensures that the artifact store path is local.\"\"\" if fileio . is_remote ( path ): raise ValueError ( f \"Path ' { path } ' specified for LocalArtifactStore is not a \" f \"local path.\" ) return path flavor : ArtifactStoreFlavor property readonly The artifact store flavor. ensure_path_is_local ( path ) classmethod Ensures that the artifact store path is local. Source code in zenml/artifact_stores/local_artifact_store.py @validator ( \"path\" ) def ensure_path_is_local ( cls , path : str ) -> str : \"\"\"Ensures that the artifact store path is local.\"\"\" if fileio . is_remote ( path ): raise ValueError ( f \"Path ' { path } ' specified for LocalArtifactStore is not a \" f \"local path.\" ) return path","title":"Artifact Stores"},{"location":"api_docs/artifact_stores/#artifact-stores","text":"","title":"Artifact Stores"},{"location":"api_docs/artifact_stores/#zenml.artifact_stores","text":"An artifact store is a place where artifacts are stored. These artifacts may have been produced by the pipeline steps, or they may be the data first ingested into a pipeline via an ingestion step. Definitions of the BaseArtifactStore class and the LocalArtifactStore that builds on it are in this module. Other artifact stores corresponding to specific integrations are to be found in the integrations module. For example, the GCPArtifactStore , used when running ZenML on Google Cloud Platform, is defined in integrations.gcp.artifact_stores .","title":"artifact_stores"},{"location":"api_docs/artifact_stores/#zenml.artifact_stores.base_artifact_store","text":"","title":"base_artifact_store"},{"location":"api_docs/artifact_stores/#zenml.artifact_stores.base_artifact_store.BaseArtifactStore","text":"Base class for all ZenML artifact stores. Attributes: Name Type Description path str The root path of the artifact store. Source code in zenml/artifact_stores/base_artifact_store.py class BaseArtifactStore ( StackComponent , ABC ): \"\"\"Base class for all ZenML artifact stores. Attributes: path: The root path of the artifact store. \"\"\" path : str @property def type ( self ) -> StackComponentType : \"\"\"The component type.\"\"\" return StackComponentType . ARTIFACT_STORE @property @abstractmethod def flavor ( self ) -> ArtifactStoreFlavor : \"\"\"The artifact store flavor.\"\"\"","title":"BaseArtifactStore"},{"location":"api_docs/artifact_stores/#zenml.artifact_stores.base_artifact_store.BaseArtifactStore.flavor","text":"The artifact store flavor.","title":"flavor"},{"location":"api_docs/artifact_stores/#zenml.artifact_stores.base_artifact_store.BaseArtifactStore.type","text":"The component type.","title":"type"},{"location":"api_docs/artifact_stores/#zenml.artifact_stores.local_artifact_store","text":"","title":"local_artifact_store"},{"location":"api_docs/artifact_stores/#zenml.artifact_stores.local_artifact_store.LocalArtifactStore","text":"Artifact Store for local artifacts. Source code in zenml/artifact_stores/local_artifact_store.py class LocalArtifactStore ( BaseArtifactStore ): \"\"\"Artifact Store for local artifacts.\"\"\" supports_local_execution = True supports_remote_execution = False @property def flavor ( self ) -> ArtifactStoreFlavor : \"\"\"The artifact store flavor.\"\"\" return ArtifactStoreFlavor . LOCAL @validator ( \"path\" ) def ensure_path_is_local ( cls , path : str ) -> str : \"\"\"Ensures that the artifact store path is local.\"\"\" if fileio . is_remote ( path ): raise ValueError ( f \"Path ' { path } ' specified for LocalArtifactStore is not a \" f \"local path.\" ) return path","title":"LocalArtifactStore"},{"location":"api_docs/artifact_stores/#zenml.artifact_stores.local_artifact_store.LocalArtifactStore.flavor","text":"The artifact store flavor.","title":"flavor"},{"location":"api_docs/artifact_stores/#zenml.artifact_stores.local_artifact_store.LocalArtifactStore.ensure_path_is_local","text":"Ensures that the artifact store path is local. Source code in zenml/artifact_stores/local_artifact_store.py @validator ( \"path\" ) def ensure_path_is_local ( cls , path : str ) -> str : \"\"\"Ensures that the artifact store path is local.\"\"\" if fileio . is_remote ( path ): raise ValueError ( f \"Path ' { path } ' specified for LocalArtifactStore is not a \" f \"local path.\" ) return path","title":"ensure_path_is_local()"},{"location":"api_docs/artifacts/","text":"Artifacts zenml.artifacts special Artifacts are the data that power your experimentation and model training. It is actually steps that produce artifacts, which are then stored in the artifact store. Artifacts are written in the signature of a step like so: .. code:: python // Some code def my_step(first_artifact: int, second_artifact: torch.nn.Module -> int: # first_artifact is an integer # second_artifact is a torch.nn.Module return 1 Artifacts can be serialized and deserialized (i.e. written and read from the Artifact Store) in various ways like TFRecords or saved model pickles, depending on what the step produces.The serialization and deserialization logic of artifacts is defined by the appropriate Materializer. base_artifact The below code is copied from the TFX source repo with minor changes. All credits go to the TFX team for the core implementation BaseArtifact ( Artifact ) Base class for all ZenML artifacts. Every implementation of an artifact needs to inherit this class. While inheriting from this class there are a few things to consider: Upon creation, each artifact class needs to be given a unique TYPE_NAME. Your artifact can feature different properties under the parameter PROPERTIES which will be tracked throughout your pipeline runs. Source code in zenml/artifacts/base_artifact.py class BaseArtifact ( Artifact ): \"\"\"Base class for all ZenML artifacts. Every implementation of an artifact needs to inherit this class. While inheriting from this class there are a few things to consider: - Upon creation, each artifact class needs to be given a unique TYPE_NAME. - Your artifact can feature different properties under the parameter PROPERTIES which will be tracked throughout your pipeline runs. \"\"\" # TODO [ENG-172]: Write about the materializers TYPE_NAME : str = \"BaseArtifact\" # type: ignore[assignment] PROPERTIES : Dict [ str , Property ] = { # type: ignore[assignment] MATERIALIZER_PROPERTY_KEY : MATERIALIZER_PROPERTY , DATATYPE_PROPERTY_KEY : DATATYPE_PROPERTY , } _MLMD_ARTIFACT_TYPE : Any = None def __init__ ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Init method for BaseArtifact\"\"\" self . set_zenml_artifact_type () super ( BaseArtifact , self ) . __init__ ( * args , ** kwargs ) @classmethod def set_zenml_artifact_type ( cls ) -> None : \"\"\"Set the type of the artifact.\"\"\" type_name = cls . TYPE_NAME if not ( type_name and isinstance ( type_name , str )): raise ValueError ( ( \"The Artifact subclass %s must override the TYPE_NAME attribute \" \"with a string type name identifier (got %r instead).\" ) % ( cls , type_name ) ) artifact_type = metadata_store_pb2 . ArtifactType () artifact_type . name = type_name if cls . PROPERTIES : # Perform validation on PROPERTIES dictionary. if not isinstance ( cls . PROPERTIES , dict ): raise ValueError ( \"Artifact subclass %s .PROPERTIES is not a dictionary.\" % cls ) for key , value in cls . PROPERTIES . items (): if not ( isinstance ( key , ( str , bytes )) and isinstance ( value , Property ) ): raise ValueError ( ( \"Artifact subclass %s .PROPERTIES dictionary must have keys of \" \"type string and values of type artifact.Property.\" ) % cls ) # Populate ML Metadata artifact properties dictionary. for key , value in cls . PROPERTIES . items (): artifact_type . properties [ key ] = value . mlmd_type () # type: ignore[no-untyped-call] cls . _MLMD_ARTIFACT_TYPE = artifact_type __init__ ( self , * args , ** kwargs ) special Init method for BaseArtifact Source code in zenml/artifacts/base_artifact.py def __init__ ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Init method for BaseArtifact\"\"\" self . set_zenml_artifact_type () super ( BaseArtifact , self ) . __init__ ( * args , ** kwargs ) set_zenml_artifact_type () classmethod Set the type of the artifact. Source code in zenml/artifacts/base_artifact.py @classmethod def set_zenml_artifact_type ( cls ) -> None : \"\"\"Set the type of the artifact.\"\"\" type_name = cls . TYPE_NAME if not ( type_name and isinstance ( type_name , str )): raise ValueError ( ( \"The Artifact subclass %s must override the TYPE_NAME attribute \" \"with a string type name identifier (got %r instead).\" ) % ( cls , type_name ) ) artifact_type = metadata_store_pb2 . ArtifactType () artifact_type . name = type_name if cls . PROPERTIES : # Perform validation on PROPERTIES dictionary. if not isinstance ( cls . PROPERTIES , dict ): raise ValueError ( \"Artifact subclass %s .PROPERTIES is not a dictionary.\" % cls ) for key , value in cls . PROPERTIES . items (): if not ( isinstance ( key , ( str , bytes )) and isinstance ( value , Property ) ): raise ValueError ( ( \"Artifact subclass %s .PROPERTIES dictionary must have keys of \" \"type string and values of type artifact.Property.\" ) % cls ) # Populate ML Metadata artifact properties dictionary. for key , value in cls . PROPERTIES . items (): artifact_type . properties [ key ] = value . mlmd_type () # type: ignore[no-untyped-call] cls . _MLMD_ARTIFACT_TYPE = artifact_type data_analysis_artifact DataAnalysisArtifact ( BaseArtifact ) Class for all ZenML data analysis artifacts. This should act as a base class for all artifacts generated from processes such as data profiling, data drift analyses, model drift detection etc. Source code in zenml/artifacts/data_analysis_artifact.py class DataAnalysisArtifact ( BaseArtifact ): \"\"\"Class for all ZenML data analysis artifacts. This should act as a base class for all artifacts generated from processes such as data profiling, data drift analyses, model drift detection etc. \"\"\" TYPE_NAME = \"DataAnalysisArtifact\" data_artifact DataArtifact ( BaseArtifact ) Class for all ZenML data artifacts. Source code in zenml/artifacts/data_artifact.py class DataArtifact ( BaseArtifact ): \"\"\"Class for all ZenML data artifacts.\"\"\" TYPE_NAME = \"DataArtifact\" model_artifact ModelArtifact ( BaseArtifact ) Class for all ZenML model artifacts. Source code in zenml/artifacts/model_artifact.py class ModelArtifact ( BaseArtifact ): \"\"\"Class for all ZenML model artifacts.\"\"\" TYPE_NAME = \"ModelArtifact\" schema_artifact SchemaArtifact ( BaseArtifact ) Class for all ZenML schema artifacts. Source code in zenml/artifacts/schema_artifact.py class SchemaArtifact ( BaseArtifact ): \"\"\"Class for all ZenML schema artifacts.\"\"\" TYPE_NAME = \"SchemaArtifact\" statistics_artifact StatisticsArtifact ( BaseArtifact ) Class for all ZenML statistics artifacts. Source code in zenml/artifacts/statistics_artifact.py class StatisticsArtifact ( BaseArtifact ): \"\"\"Class for all ZenML statistics artifacts.\"\"\" TYPE_NAME = \"StatisticsArtifact\" type_registry ArtifactTypeRegistry A registry to keep track of which datatypes map to which artifact types Source code in zenml/artifacts/type_registry.py class ArtifactTypeRegistry ( object ): \"\"\"A registry to keep track of which datatypes map to which artifact types\"\"\" def __init__ ( self ) -> None : \"\"\"Initialization with an empty registry\"\"\" self . _artifact_types : Dict [ Type [ Any ], List [ Type [ \"BaseArtifact\" ]]] = {} def register_integration ( self , key : Type [ Any ], type_ : List [ Type [ \"BaseArtifact\" ]] ) -> None : \"\"\"Method to register an integration within the registry Args: key: any datatype type_: the list of artifact type that the given datatypes is associated with \"\"\" self . _artifact_types [ key ] = type_ def get_artifact_type ( self , key : Type [ Any ]) -> List [ Type [ \"BaseArtifact\" ]]: \"\"\"Method to extract the list of artifact types given the data type\"\"\" return self . _artifact_types [ key ] __init__ ( self ) special Initialization with an empty registry Source code in zenml/artifacts/type_registry.py def __init__ ( self ) -> None : \"\"\"Initialization with an empty registry\"\"\" self . _artifact_types : Dict [ Type [ Any ], List [ Type [ \"BaseArtifact\" ]]] = {} get_artifact_type ( self , key ) Method to extract the list of artifact types given the data type Source code in zenml/artifacts/type_registry.py def get_artifact_type ( self , key : Type [ Any ]) -> List [ Type [ \"BaseArtifact\" ]]: \"\"\"Method to extract the list of artifact types given the data type\"\"\" return self . _artifact_types [ key ] register_integration ( self , key , type_ ) Method to register an integration within the registry Parameters: Name Type Description Default key Type[Any] any datatype required type_ List[Type[BaseArtifact]] the list of artifact type that the given datatypes is associated with required Source code in zenml/artifacts/type_registry.py def register_integration ( self , key : Type [ Any ], type_ : List [ Type [ \"BaseArtifact\" ]] ) -> None : \"\"\"Method to register an integration within the registry Args: key: any datatype type_: the list of artifact type that the given datatypes is associated with \"\"\" self . _artifact_types [ key ] = type_","title":"Artifacts"},{"location":"api_docs/artifacts/#artifacts","text":"","title":"Artifacts"},{"location":"api_docs/artifacts/#zenml.artifacts","text":"Artifacts are the data that power your experimentation and model training. It is actually steps that produce artifacts, which are then stored in the artifact store. Artifacts are written in the signature of a step like so: .. code:: python // Some code def my_step(first_artifact: int, second_artifact: torch.nn.Module -> int: # first_artifact is an integer # second_artifact is a torch.nn.Module return 1 Artifacts can be serialized and deserialized (i.e. written and read from the Artifact Store) in various ways like TFRecords or saved model pickles, depending on what the step produces.The serialization and deserialization logic of artifacts is defined by the appropriate Materializer.","title":"artifacts"},{"location":"api_docs/artifacts/#zenml.artifacts.base_artifact","text":"The below code is copied from the TFX source repo with minor changes. All credits go to the TFX team for the core implementation","title":"base_artifact"},{"location":"api_docs/artifacts/#zenml.artifacts.base_artifact.BaseArtifact","text":"Base class for all ZenML artifacts. Every implementation of an artifact needs to inherit this class. While inheriting from this class there are a few things to consider: Upon creation, each artifact class needs to be given a unique TYPE_NAME. Your artifact can feature different properties under the parameter PROPERTIES which will be tracked throughout your pipeline runs. Source code in zenml/artifacts/base_artifact.py class BaseArtifact ( Artifact ): \"\"\"Base class for all ZenML artifacts. Every implementation of an artifact needs to inherit this class. While inheriting from this class there are a few things to consider: - Upon creation, each artifact class needs to be given a unique TYPE_NAME. - Your artifact can feature different properties under the parameter PROPERTIES which will be tracked throughout your pipeline runs. \"\"\" # TODO [ENG-172]: Write about the materializers TYPE_NAME : str = \"BaseArtifact\" # type: ignore[assignment] PROPERTIES : Dict [ str , Property ] = { # type: ignore[assignment] MATERIALIZER_PROPERTY_KEY : MATERIALIZER_PROPERTY , DATATYPE_PROPERTY_KEY : DATATYPE_PROPERTY , } _MLMD_ARTIFACT_TYPE : Any = None def __init__ ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Init method for BaseArtifact\"\"\" self . set_zenml_artifact_type () super ( BaseArtifact , self ) . __init__ ( * args , ** kwargs ) @classmethod def set_zenml_artifact_type ( cls ) -> None : \"\"\"Set the type of the artifact.\"\"\" type_name = cls . TYPE_NAME if not ( type_name and isinstance ( type_name , str )): raise ValueError ( ( \"The Artifact subclass %s must override the TYPE_NAME attribute \" \"with a string type name identifier (got %r instead).\" ) % ( cls , type_name ) ) artifact_type = metadata_store_pb2 . ArtifactType () artifact_type . name = type_name if cls . PROPERTIES : # Perform validation on PROPERTIES dictionary. if not isinstance ( cls . PROPERTIES , dict ): raise ValueError ( \"Artifact subclass %s .PROPERTIES is not a dictionary.\" % cls ) for key , value in cls . PROPERTIES . items (): if not ( isinstance ( key , ( str , bytes )) and isinstance ( value , Property ) ): raise ValueError ( ( \"Artifact subclass %s .PROPERTIES dictionary must have keys of \" \"type string and values of type artifact.Property.\" ) % cls ) # Populate ML Metadata artifact properties dictionary. for key , value in cls . PROPERTIES . items (): artifact_type . properties [ key ] = value . mlmd_type () # type: ignore[no-untyped-call] cls . _MLMD_ARTIFACT_TYPE = artifact_type","title":"BaseArtifact"},{"location":"api_docs/artifacts/#zenml.artifacts.base_artifact.BaseArtifact.__init__","text":"Init method for BaseArtifact Source code in zenml/artifacts/base_artifact.py def __init__ ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Init method for BaseArtifact\"\"\" self . set_zenml_artifact_type () super ( BaseArtifact , self ) . __init__ ( * args , ** kwargs )","title":"__init__()"},{"location":"api_docs/artifacts/#zenml.artifacts.base_artifact.BaseArtifact.set_zenml_artifact_type","text":"Set the type of the artifact. Source code in zenml/artifacts/base_artifact.py @classmethod def set_zenml_artifact_type ( cls ) -> None : \"\"\"Set the type of the artifact.\"\"\" type_name = cls . TYPE_NAME if not ( type_name and isinstance ( type_name , str )): raise ValueError ( ( \"The Artifact subclass %s must override the TYPE_NAME attribute \" \"with a string type name identifier (got %r instead).\" ) % ( cls , type_name ) ) artifact_type = metadata_store_pb2 . ArtifactType () artifact_type . name = type_name if cls . PROPERTIES : # Perform validation on PROPERTIES dictionary. if not isinstance ( cls . PROPERTIES , dict ): raise ValueError ( \"Artifact subclass %s .PROPERTIES is not a dictionary.\" % cls ) for key , value in cls . PROPERTIES . items (): if not ( isinstance ( key , ( str , bytes )) and isinstance ( value , Property ) ): raise ValueError ( ( \"Artifact subclass %s .PROPERTIES dictionary must have keys of \" \"type string and values of type artifact.Property.\" ) % cls ) # Populate ML Metadata artifact properties dictionary. for key , value in cls . PROPERTIES . items (): artifact_type . properties [ key ] = value . mlmd_type () # type: ignore[no-untyped-call] cls . _MLMD_ARTIFACT_TYPE = artifact_type","title":"set_zenml_artifact_type()"},{"location":"api_docs/artifacts/#zenml.artifacts.data_analysis_artifact","text":"","title":"data_analysis_artifact"},{"location":"api_docs/artifacts/#zenml.artifacts.data_analysis_artifact.DataAnalysisArtifact","text":"Class for all ZenML data analysis artifacts. This should act as a base class for all artifacts generated from processes such as data profiling, data drift analyses, model drift detection etc. Source code in zenml/artifacts/data_analysis_artifact.py class DataAnalysisArtifact ( BaseArtifact ): \"\"\"Class for all ZenML data analysis artifacts. This should act as a base class for all artifacts generated from processes such as data profiling, data drift analyses, model drift detection etc. \"\"\" TYPE_NAME = \"DataAnalysisArtifact\"","title":"DataAnalysisArtifact"},{"location":"api_docs/artifacts/#zenml.artifacts.data_artifact","text":"","title":"data_artifact"},{"location":"api_docs/artifacts/#zenml.artifacts.data_artifact.DataArtifact","text":"Class for all ZenML data artifacts. Source code in zenml/artifacts/data_artifact.py class DataArtifact ( BaseArtifact ): \"\"\"Class for all ZenML data artifacts.\"\"\" TYPE_NAME = \"DataArtifact\"","title":"DataArtifact"},{"location":"api_docs/artifacts/#zenml.artifacts.model_artifact","text":"","title":"model_artifact"},{"location":"api_docs/artifacts/#zenml.artifacts.model_artifact.ModelArtifact","text":"Class for all ZenML model artifacts. Source code in zenml/artifacts/model_artifact.py class ModelArtifact ( BaseArtifact ): \"\"\"Class for all ZenML model artifacts.\"\"\" TYPE_NAME = \"ModelArtifact\"","title":"ModelArtifact"},{"location":"api_docs/artifacts/#zenml.artifacts.schema_artifact","text":"","title":"schema_artifact"},{"location":"api_docs/artifacts/#zenml.artifacts.schema_artifact.SchemaArtifact","text":"Class for all ZenML schema artifacts. Source code in zenml/artifacts/schema_artifact.py class SchemaArtifact ( BaseArtifact ): \"\"\"Class for all ZenML schema artifacts.\"\"\" TYPE_NAME = \"SchemaArtifact\"","title":"SchemaArtifact"},{"location":"api_docs/artifacts/#zenml.artifacts.statistics_artifact","text":"","title":"statistics_artifact"},{"location":"api_docs/artifacts/#zenml.artifacts.statistics_artifact.StatisticsArtifact","text":"Class for all ZenML statistics artifacts. Source code in zenml/artifacts/statistics_artifact.py class StatisticsArtifact ( BaseArtifact ): \"\"\"Class for all ZenML statistics artifacts.\"\"\" TYPE_NAME = \"StatisticsArtifact\"","title":"StatisticsArtifact"},{"location":"api_docs/artifacts/#zenml.artifacts.type_registry","text":"","title":"type_registry"},{"location":"api_docs/artifacts/#zenml.artifacts.type_registry.ArtifactTypeRegistry","text":"A registry to keep track of which datatypes map to which artifact types Source code in zenml/artifacts/type_registry.py class ArtifactTypeRegistry ( object ): \"\"\"A registry to keep track of which datatypes map to which artifact types\"\"\" def __init__ ( self ) -> None : \"\"\"Initialization with an empty registry\"\"\" self . _artifact_types : Dict [ Type [ Any ], List [ Type [ \"BaseArtifact\" ]]] = {} def register_integration ( self , key : Type [ Any ], type_ : List [ Type [ \"BaseArtifact\" ]] ) -> None : \"\"\"Method to register an integration within the registry Args: key: any datatype type_: the list of artifact type that the given datatypes is associated with \"\"\" self . _artifact_types [ key ] = type_ def get_artifact_type ( self , key : Type [ Any ]) -> List [ Type [ \"BaseArtifact\" ]]: \"\"\"Method to extract the list of artifact types given the data type\"\"\" return self . _artifact_types [ key ]","title":"ArtifactTypeRegistry"},{"location":"api_docs/artifacts/#zenml.artifacts.type_registry.ArtifactTypeRegistry.__init__","text":"Initialization with an empty registry Source code in zenml/artifacts/type_registry.py def __init__ ( self ) -> None : \"\"\"Initialization with an empty registry\"\"\" self . _artifact_types : Dict [ Type [ Any ], List [ Type [ \"BaseArtifact\" ]]] = {}","title":"__init__()"},{"location":"api_docs/artifacts/#zenml.artifacts.type_registry.ArtifactTypeRegistry.get_artifact_type","text":"Method to extract the list of artifact types given the data type Source code in zenml/artifacts/type_registry.py def get_artifact_type ( self , key : Type [ Any ]) -> List [ Type [ \"BaseArtifact\" ]]: \"\"\"Method to extract the list of artifact types given the data type\"\"\" return self . _artifact_types [ key ]","title":"get_artifact_type()"},{"location":"api_docs/artifacts/#zenml.artifacts.type_registry.ArtifactTypeRegistry.register_integration","text":"Method to register an integration within the registry Parameters: Name Type Description Default key Type[Any] any datatype required type_ List[Type[BaseArtifact]] the list of artifact type that the given datatypes is associated with required Source code in zenml/artifacts/type_registry.py def register_integration ( self , key : Type [ Any ], type_ : List [ Type [ \"BaseArtifact\" ]] ) -> None : \"\"\"Method to register an integration within the registry Args: key: any datatype type_: the list of artifact type that the given datatypes is associated with \"\"\" self . _artifact_types [ key ] = type_","title":"register_integration()"},{"location":"api_docs/config/","text":"Config zenml.config special The config module contains classes and functions that manage user-specific configuration. ZenML's configuration is stored in a file called .zenglobal.json , located on the user's directory for configuration files. (The exact location differs from operating system to operating system.) The GlobalConfig class is the main class in this module. It provides a Pydantic configuration object that is used to store and retrieve configuration. This GlobalConfig object handles the serialization and deserialization of the configuration options that are stored in the file in order to persist the configuration across sessions. config_keys ConfigKeys Class to validate dictionary configurations. Source code in zenml/config/config_keys.py class ConfigKeys : \"\"\"Class to validate dictionary configurations.\"\"\" @classmethod def get_keys ( cls ) -> Tuple [ List [ str ], List [ str ]]: \"\"\"Gets all the required and optional config keys for this class. Returns: A tuple (required, optional) which are lists of the required/optional keys for this class. \"\"\" keys = { key : value for key , value in cls . __dict__ . items () if not isinstance ( value , classmethod ) and not isinstance ( value , staticmethod ) and not callable ( value ) and not key . startswith ( \"__\" ) } required = [ v for k , v in keys . items () if not k . endswith ( \"_\" )] optional = [ v for k , v in keys . items () if k . endswith ( \"_\" )] return required , optional @classmethod def key_check ( cls , config : Dict [ str , Any ]) -> None : \"\"\"Checks whether a configuration dict contains all required keys and no unknown keys. Args: config: The configuration dict to verify. Raises: AssertionError: If the dictionary contains unknown keys or is missing any required key. \"\"\" assert isinstance ( config , dict ), \"Please specify a dict for {} \" . format ( cls . __name__ ) # Required and optional keys for the config dict required , optional = cls . get_keys () # Check for missing keys missing_keys = [ k for k in required if k not in config . keys ()] assert len ( missing_keys ) == 0 , \"Missing key(s) {} in {} \" . format ( missing_keys , cls . __name__ ) # Check for unknown keys unknown_keys = [ k for k in config . keys () if k not in required and k not in optional ] assert ( len ( unknown_keys ) == 0 ), \"Unknown key(s) {} in {} . Required keys : {} \" \"Optional Keys: {} \" . format ( unknown_keys , cls . __name__ , required , optional , ) get_keys () classmethod Gets all the required and optional config keys for this class. Returns: Type Description Tuple[List[str], List[str]] A tuple (required, optional) which are lists of the required/optional keys for this class. Source code in zenml/config/config_keys.py @classmethod def get_keys ( cls ) -> Tuple [ List [ str ], List [ str ]]: \"\"\"Gets all the required and optional config keys for this class. Returns: A tuple (required, optional) which are lists of the required/optional keys for this class. \"\"\" keys = { key : value for key , value in cls . __dict__ . items () if not isinstance ( value , classmethod ) and not isinstance ( value , staticmethod ) and not callable ( value ) and not key . startswith ( \"__\" ) } required = [ v for k , v in keys . items () if not k . endswith ( \"_\" )] optional = [ v for k , v in keys . items () if k . endswith ( \"_\" )] return required , optional key_check ( config ) classmethod Checks whether a configuration dict contains all required keys and no unknown keys. Parameters: Name Type Description Default config Dict[str, Any] The configuration dict to verify. required Exceptions: Type Description AssertionError If the dictionary contains unknown keys or is missing any required key. Source code in zenml/config/config_keys.py @classmethod def key_check ( cls , config : Dict [ str , Any ]) -> None : \"\"\"Checks whether a configuration dict contains all required keys and no unknown keys. Args: config: The configuration dict to verify. Raises: AssertionError: If the dictionary contains unknown keys or is missing any required key. \"\"\" assert isinstance ( config , dict ), \"Please specify a dict for {} \" . format ( cls . __name__ ) # Required and optional keys for the config dict required , optional = cls . get_keys () # Check for missing keys missing_keys = [ k for k in required if k not in config . keys ()] assert len ( missing_keys ) == 0 , \"Missing key(s) {} in {} \" . format ( missing_keys , cls . __name__ ) # Check for unknown keys unknown_keys = [ k for k in config . keys () if k not in required and k not in optional ] assert ( len ( unknown_keys ) == 0 ), \"Unknown key(s) {} in {} . Required keys : {} \" \"Optional Keys: {} \" . format ( unknown_keys , cls . __name__ , required , optional , ) PipelineConfigurationKeys ( ConfigKeys ) Keys for a pipeline configuration dict. Source code in zenml/config/config_keys.py class PipelineConfigurationKeys ( ConfigKeys ): \"\"\"Keys for a pipeline configuration dict.\"\"\" NAME = \"name\" STEPS = \"steps\" StepConfigurationKeys ( ConfigKeys ) Keys for a step configuration dict. Source code in zenml/config/config_keys.py class StepConfigurationKeys ( ConfigKeys ): \"\"\"Keys for a step configuration dict.\"\"\" SOURCE_ = \"source\" PARAMETERS_ = \"parameters\" MATERIALIZERS_ = \"materializers\" global_config GlobalConfig ( BaseModel ) pydantic-model Stores global configuration options. Configuration options are read from a config file, but can be overwritten by environment variables. See GlobalConfig.__getattribute__ for more details. Attributes: Name Type Description user_id UUID Unique user id. analytics_opt_in bool If a user agreed to sending analytics or not. Source code in zenml/config/global_config.py class GlobalConfig ( BaseModel ): \"\"\"Stores global configuration options. Configuration options are read from a config file, but can be overwritten by environment variables. See `GlobalConfig.__getattribute__` for more details. Attributes: user_id: Unique user id. analytics_opt_in: If a user agreed to sending analytics or not. \"\"\" user_id : uuid . UUID = Field ( default_factory = uuid . uuid4 , allow_mutation = False ) analytics_opt_in : bool = True def __init__ ( self ) -> None : \"\"\"Initializes a GlobalConfig object using values from the config file. If the config file doesn't exist yet, we try to read values from the legacy (ZenML version < 0.6) config file. \"\"\" config_values = self . _read_config () super () . __init__ ( ** config_values ) if not fileio . file_exists ( self . config_file ()): # the config file hasn't been written to disk, make sure to persist # the unique user id fileio . create_dir_recursive_if_not_exists ( self . config_directory ()) self . _write_config () def __setattr__ ( self , key : str , value : Any ) -> None : \"\"\"Sets an attribute on the global config and persists the new value.\"\"\" super () . __setattr__ ( key , value ) self . _write_config () def __getattribute__ ( self , key : str ) -> Any : \"\"\"Gets an attribute value for a specific key. If a value for this attribute was specified using an environment variable called `ZENML_$(ATTRIBUTE_NAME)` and its value can be parsed to the attribute type, the value from this environment variable is returned instead. \"\"\" value = super () . __getattribute__ ( key ) environment_variable_name = f \"ZENML_ { key . upper () } \" try : environment_variable_value = os . environ [ environment_variable_name ] # set the environment variable value to leverage pydantics type # conversion and validation super () . __setattr__ ( key , environment_variable_value ) return_value = super () . __getattribute__ ( key ) # set back the old value as we don't want to permanently store # the environment variable value here super () . __setattr__ ( key , value ) return return_value except ( ValidationError , KeyError , TypeError ): return value def _read_config ( self ) -> Dict [ str , Any ]: \"\"\"Reads configuration options from disk. If the config file doesn't exist yet, this method falls back to reading options from a legacy config file or returns an empty dictionary. \"\"\" legacy_config_file = os . path . join ( GlobalConfig . config_directory (), LEGACY_CONFIG_FILE_NAME ) config_values = {} if fileio . file_exists ( self . config_file ()): config_values = cast ( Dict [ str , Any ], yaml_utils . read_yaml ( self . config_file ()) ) elif fileio . file_exists ( legacy_config_file ): config_values = cast ( Dict [ str , Any ], yaml_utils . read_json ( legacy_config_file ) ) return config_values def _write_config ( self ) -> None : \"\"\"Writes the global configuration options to disk.\"\"\" yaml_dict = json . loads ( self . json ()) yaml_utils . write_yaml ( self . config_file (), yaml_dict ) @staticmethod def config_directory () -> str : \"\"\"Path to the global configuration directory.\"\"\" # TODO [ENG-370]: Remove the util method to get global config directory, # the remaining codebase should use `GlobalConfig.config_directory()` # instead. return get_global_config_directory () @staticmethod def config_file () -> str : \"\"\"Path to the file where global configuration options are stored.\"\"\" return os . path . join ( GlobalConfig . config_directory (), \"config.yaml\" ) class Config : \"\"\"Pydantic configuration class.\"\"\" # Validate attributes when assigning them. We need to set this in order # to have a mix of mutable and immutable attributes validate_assignment = True # Ignore extra attributes from configs of previous ZenML versions extra = \"ignore\" Config Pydantic configuration class. Source code in zenml/config/global_config.py class Config : \"\"\"Pydantic configuration class.\"\"\" # Validate attributes when assigning them. We need to set this in order # to have a mix of mutable and immutable attributes validate_assignment = True # Ignore extra attributes from configs of previous ZenML versions extra = \"ignore\" __getattribute__ ( self , key ) special Gets an attribute value for a specific key. If a value for this attribute was specified using an environment variable called ZENML_$(ATTRIBUTE_NAME) and its value can be parsed to the attribute type, the value from this environment variable is returned instead. Source code in zenml/config/global_config.py def __getattribute__ ( self , key : str ) -> Any : \"\"\"Gets an attribute value for a specific key. If a value for this attribute was specified using an environment variable called `ZENML_$(ATTRIBUTE_NAME)` and its value can be parsed to the attribute type, the value from this environment variable is returned instead. \"\"\" value = super () . __getattribute__ ( key ) environment_variable_name = f \"ZENML_ { key . upper () } \" try : environment_variable_value = os . environ [ environment_variable_name ] # set the environment variable value to leverage pydantics type # conversion and validation super () . __setattr__ ( key , environment_variable_value ) return_value = super () . __getattribute__ ( key ) # set back the old value as we don't want to permanently store # the environment variable value here super () . __setattr__ ( key , value ) return return_value except ( ValidationError , KeyError , TypeError ): return value __init__ ( self ) special Initializes a GlobalConfig object using values from the config file. If the config file doesn't exist yet, we try to read values from the legacy (ZenML version < 0.6) config file. Source code in zenml/config/global_config.py def __init__ ( self ) -> None : \"\"\"Initializes a GlobalConfig object using values from the config file. If the config file doesn't exist yet, we try to read values from the legacy (ZenML version < 0.6) config file. \"\"\" config_values = self . _read_config () super () . __init__ ( ** config_values ) if not fileio . file_exists ( self . config_file ()): # the config file hasn't been written to disk, make sure to persist # the unique user id fileio . create_dir_recursive_if_not_exists ( self . config_directory ()) self . _write_config () __setattr__ ( self , key , value ) special Sets an attribute on the global config and persists the new value. Source code in zenml/config/global_config.py def __setattr__ ( self , key : str , value : Any ) -> None : \"\"\"Sets an attribute on the global config and persists the new value.\"\"\" super () . __setattr__ ( key , value ) self . _write_config () config_directory () staticmethod Path to the global configuration directory. Source code in zenml/config/global_config.py @staticmethod def config_directory () -> str : \"\"\"Path to the global configuration directory.\"\"\" # TODO [ENG-370]: Remove the util method to get global config directory, # the remaining codebase should use `GlobalConfig.config_directory()` # instead. return get_global_config_directory () config_file () staticmethod Path to the file where global configuration options are stored. Source code in zenml/config/global_config.py @staticmethod def config_file () -> str : \"\"\"Path to the file where global configuration options are stored.\"\"\" return os . path . join ( GlobalConfig . config_directory (), \"config.yaml\" )","title":"Config"},{"location":"api_docs/config/#config","text":"","title":"Config"},{"location":"api_docs/config/#zenml.config","text":"The config module contains classes and functions that manage user-specific configuration. ZenML's configuration is stored in a file called .zenglobal.json , located on the user's directory for configuration files. (The exact location differs from operating system to operating system.) The GlobalConfig class is the main class in this module. It provides a Pydantic configuration object that is used to store and retrieve configuration. This GlobalConfig object handles the serialization and deserialization of the configuration options that are stored in the file in order to persist the configuration across sessions.","title":"config"},{"location":"api_docs/config/#zenml.config.config_keys","text":"","title":"config_keys"},{"location":"api_docs/config/#zenml.config.config_keys.ConfigKeys","text":"Class to validate dictionary configurations. Source code in zenml/config/config_keys.py class ConfigKeys : \"\"\"Class to validate dictionary configurations.\"\"\" @classmethod def get_keys ( cls ) -> Tuple [ List [ str ], List [ str ]]: \"\"\"Gets all the required and optional config keys for this class. Returns: A tuple (required, optional) which are lists of the required/optional keys for this class. \"\"\" keys = { key : value for key , value in cls . __dict__ . items () if not isinstance ( value , classmethod ) and not isinstance ( value , staticmethod ) and not callable ( value ) and not key . startswith ( \"__\" ) } required = [ v for k , v in keys . items () if not k . endswith ( \"_\" )] optional = [ v for k , v in keys . items () if k . endswith ( \"_\" )] return required , optional @classmethod def key_check ( cls , config : Dict [ str , Any ]) -> None : \"\"\"Checks whether a configuration dict contains all required keys and no unknown keys. Args: config: The configuration dict to verify. Raises: AssertionError: If the dictionary contains unknown keys or is missing any required key. \"\"\" assert isinstance ( config , dict ), \"Please specify a dict for {} \" . format ( cls . __name__ ) # Required and optional keys for the config dict required , optional = cls . get_keys () # Check for missing keys missing_keys = [ k for k in required if k not in config . keys ()] assert len ( missing_keys ) == 0 , \"Missing key(s) {} in {} \" . format ( missing_keys , cls . __name__ ) # Check for unknown keys unknown_keys = [ k for k in config . keys () if k not in required and k not in optional ] assert ( len ( unknown_keys ) == 0 ), \"Unknown key(s) {} in {} . Required keys : {} \" \"Optional Keys: {} \" . format ( unknown_keys , cls . __name__ , required , optional , )","title":"ConfigKeys"},{"location":"api_docs/config/#zenml.config.config_keys.ConfigKeys.get_keys","text":"Gets all the required and optional config keys for this class. Returns: Type Description Tuple[List[str], List[str]] A tuple (required, optional) which are lists of the required/optional keys for this class. Source code in zenml/config/config_keys.py @classmethod def get_keys ( cls ) -> Tuple [ List [ str ], List [ str ]]: \"\"\"Gets all the required and optional config keys for this class. Returns: A tuple (required, optional) which are lists of the required/optional keys for this class. \"\"\" keys = { key : value for key , value in cls . __dict__ . items () if not isinstance ( value , classmethod ) and not isinstance ( value , staticmethod ) and not callable ( value ) and not key . startswith ( \"__\" ) } required = [ v for k , v in keys . items () if not k . endswith ( \"_\" )] optional = [ v for k , v in keys . items () if k . endswith ( \"_\" )] return required , optional","title":"get_keys()"},{"location":"api_docs/config/#zenml.config.config_keys.ConfigKeys.key_check","text":"Checks whether a configuration dict contains all required keys and no unknown keys. Parameters: Name Type Description Default config Dict[str, Any] The configuration dict to verify. required Exceptions: Type Description AssertionError If the dictionary contains unknown keys or is missing any required key. Source code in zenml/config/config_keys.py @classmethod def key_check ( cls , config : Dict [ str , Any ]) -> None : \"\"\"Checks whether a configuration dict contains all required keys and no unknown keys. Args: config: The configuration dict to verify. Raises: AssertionError: If the dictionary contains unknown keys or is missing any required key. \"\"\" assert isinstance ( config , dict ), \"Please specify a dict for {} \" . format ( cls . __name__ ) # Required and optional keys for the config dict required , optional = cls . get_keys () # Check for missing keys missing_keys = [ k for k in required if k not in config . keys ()] assert len ( missing_keys ) == 0 , \"Missing key(s) {} in {} \" . format ( missing_keys , cls . __name__ ) # Check for unknown keys unknown_keys = [ k for k in config . keys () if k not in required and k not in optional ] assert ( len ( unknown_keys ) == 0 ), \"Unknown key(s) {} in {} . Required keys : {} \" \"Optional Keys: {} \" . format ( unknown_keys , cls . __name__ , required , optional , )","title":"key_check()"},{"location":"api_docs/config/#zenml.config.config_keys.PipelineConfigurationKeys","text":"Keys for a pipeline configuration dict. Source code in zenml/config/config_keys.py class PipelineConfigurationKeys ( ConfigKeys ): \"\"\"Keys for a pipeline configuration dict.\"\"\" NAME = \"name\" STEPS = \"steps\"","title":"PipelineConfigurationKeys"},{"location":"api_docs/config/#zenml.config.config_keys.StepConfigurationKeys","text":"Keys for a step configuration dict. Source code in zenml/config/config_keys.py class StepConfigurationKeys ( ConfigKeys ): \"\"\"Keys for a step configuration dict.\"\"\" SOURCE_ = \"source\" PARAMETERS_ = \"parameters\" MATERIALIZERS_ = \"materializers\"","title":"StepConfigurationKeys"},{"location":"api_docs/config/#zenml.config.global_config","text":"","title":"global_config"},{"location":"api_docs/config/#zenml.config.global_config.GlobalConfig","text":"Stores global configuration options. Configuration options are read from a config file, but can be overwritten by environment variables. See GlobalConfig.__getattribute__ for more details. Attributes: Name Type Description user_id UUID Unique user id. analytics_opt_in bool If a user agreed to sending analytics or not. Source code in zenml/config/global_config.py class GlobalConfig ( BaseModel ): \"\"\"Stores global configuration options. Configuration options are read from a config file, but can be overwritten by environment variables. See `GlobalConfig.__getattribute__` for more details. Attributes: user_id: Unique user id. analytics_opt_in: If a user agreed to sending analytics or not. \"\"\" user_id : uuid . UUID = Field ( default_factory = uuid . uuid4 , allow_mutation = False ) analytics_opt_in : bool = True def __init__ ( self ) -> None : \"\"\"Initializes a GlobalConfig object using values from the config file. If the config file doesn't exist yet, we try to read values from the legacy (ZenML version < 0.6) config file. \"\"\" config_values = self . _read_config () super () . __init__ ( ** config_values ) if not fileio . file_exists ( self . config_file ()): # the config file hasn't been written to disk, make sure to persist # the unique user id fileio . create_dir_recursive_if_not_exists ( self . config_directory ()) self . _write_config () def __setattr__ ( self , key : str , value : Any ) -> None : \"\"\"Sets an attribute on the global config and persists the new value.\"\"\" super () . __setattr__ ( key , value ) self . _write_config () def __getattribute__ ( self , key : str ) -> Any : \"\"\"Gets an attribute value for a specific key. If a value for this attribute was specified using an environment variable called `ZENML_$(ATTRIBUTE_NAME)` and its value can be parsed to the attribute type, the value from this environment variable is returned instead. \"\"\" value = super () . __getattribute__ ( key ) environment_variable_name = f \"ZENML_ { key . upper () } \" try : environment_variable_value = os . environ [ environment_variable_name ] # set the environment variable value to leverage pydantics type # conversion and validation super () . __setattr__ ( key , environment_variable_value ) return_value = super () . __getattribute__ ( key ) # set back the old value as we don't want to permanently store # the environment variable value here super () . __setattr__ ( key , value ) return return_value except ( ValidationError , KeyError , TypeError ): return value def _read_config ( self ) -> Dict [ str , Any ]: \"\"\"Reads configuration options from disk. If the config file doesn't exist yet, this method falls back to reading options from a legacy config file or returns an empty dictionary. \"\"\" legacy_config_file = os . path . join ( GlobalConfig . config_directory (), LEGACY_CONFIG_FILE_NAME ) config_values = {} if fileio . file_exists ( self . config_file ()): config_values = cast ( Dict [ str , Any ], yaml_utils . read_yaml ( self . config_file ()) ) elif fileio . file_exists ( legacy_config_file ): config_values = cast ( Dict [ str , Any ], yaml_utils . read_json ( legacy_config_file ) ) return config_values def _write_config ( self ) -> None : \"\"\"Writes the global configuration options to disk.\"\"\" yaml_dict = json . loads ( self . json ()) yaml_utils . write_yaml ( self . config_file (), yaml_dict ) @staticmethod def config_directory () -> str : \"\"\"Path to the global configuration directory.\"\"\" # TODO [ENG-370]: Remove the util method to get global config directory, # the remaining codebase should use `GlobalConfig.config_directory()` # instead. return get_global_config_directory () @staticmethod def config_file () -> str : \"\"\"Path to the file where global configuration options are stored.\"\"\" return os . path . join ( GlobalConfig . config_directory (), \"config.yaml\" ) class Config : \"\"\"Pydantic configuration class.\"\"\" # Validate attributes when assigning them. We need to set this in order # to have a mix of mutable and immutable attributes validate_assignment = True # Ignore extra attributes from configs of previous ZenML versions extra = \"ignore\"","title":"GlobalConfig"},{"location":"api_docs/config/#zenml.config.global_config.GlobalConfig.Config","text":"Pydantic configuration class. Source code in zenml/config/global_config.py class Config : \"\"\"Pydantic configuration class.\"\"\" # Validate attributes when assigning them. We need to set this in order # to have a mix of mutable and immutable attributes validate_assignment = True # Ignore extra attributes from configs of previous ZenML versions extra = \"ignore\"","title":"Config"},{"location":"api_docs/config/#zenml.config.global_config.GlobalConfig.__getattribute__","text":"Gets an attribute value for a specific key. If a value for this attribute was specified using an environment variable called ZENML_$(ATTRIBUTE_NAME) and its value can be parsed to the attribute type, the value from this environment variable is returned instead. Source code in zenml/config/global_config.py def __getattribute__ ( self , key : str ) -> Any : \"\"\"Gets an attribute value for a specific key. If a value for this attribute was specified using an environment variable called `ZENML_$(ATTRIBUTE_NAME)` and its value can be parsed to the attribute type, the value from this environment variable is returned instead. \"\"\" value = super () . __getattribute__ ( key ) environment_variable_name = f \"ZENML_ { key . upper () } \" try : environment_variable_value = os . environ [ environment_variable_name ] # set the environment variable value to leverage pydantics type # conversion and validation super () . __setattr__ ( key , environment_variable_value ) return_value = super () . __getattribute__ ( key ) # set back the old value as we don't want to permanently store # the environment variable value here super () . __setattr__ ( key , value ) return return_value except ( ValidationError , KeyError , TypeError ): return value","title":"__getattribute__()"},{"location":"api_docs/config/#zenml.config.global_config.GlobalConfig.__init__","text":"Initializes a GlobalConfig object using values from the config file. If the config file doesn't exist yet, we try to read values from the legacy (ZenML version < 0.6) config file. Source code in zenml/config/global_config.py def __init__ ( self ) -> None : \"\"\"Initializes a GlobalConfig object using values from the config file. If the config file doesn't exist yet, we try to read values from the legacy (ZenML version < 0.6) config file. \"\"\" config_values = self . _read_config () super () . __init__ ( ** config_values ) if not fileio . file_exists ( self . config_file ()): # the config file hasn't been written to disk, make sure to persist # the unique user id fileio . create_dir_recursive_if_not_exists ( self . config_directory ()) self . _write_config ()","title":"__init__()"},{"location":"api_docs/config/#zenml.config.global_config.GlobalConfig.__setattr__","text":"Sets an attribute on the global config and persists the new value. Source code in zenml/config/global_config.py def __setattr__ ( self , key : str , value : Any ) -> None : \"\"\"Sets an attribute on the global config and persists the new value.\"\"\" super () . __setattr__ ( key , value ) self . _write_config ()","title":"__setattr__()"},{"location":"api_docs/config/#zenml.config.global_config.GlobalConfig.config_directory","text":"Path to the global configuration directory. Source code in zenml/config/global_config.py @staticmethod def config_directory () -> str : \"\"\"Path to the global configuration directory.\"\"\" # TODO [ENG-370]: Remove the util method to get global config directory, # the remaining codebase should use `GlobalConfig.config_directory()` # instead. return get_global_config_directory ()","title":"config_directory()"},{"location":"api_docs/config/#zenml.config.global_config.GlobalConfig.config_file","text":"Path to the file where global configuration options are stored. Source code in zenml/config/global_config.py @staticmethod def config_file () -> str : \"\"\"Path to the file where global configuration options are stored.\"\"\" return os . path . join ( GlobalConfig . config_directory (), \"config.yaml\" )","title":"config_file()"},{"location":"api_docs/constants/","text":"Constants zenml.constants handle_bool_env_var ( var , default = False ) Converts normal env var to boolean Source code in zenml/constants.py def handle_bool_env_var ( var : str , default : bool = False ) -> bool : \"\"\"Converts normal env var to boolean\"\"\" value = os . getenv ( var ) if value in [ \"1\" , \"y\" , \"yes\" , \"True\" , \"true\" ]: return True return default handle_int_env_var ( var , default = 0 ) Converts normal env var to int Source code in zenml/constants.py def handle_int_env_var ( var : str , default : int = 0 ) -> int : \"\"\"Converts normal env var to int\"\"\" value = os . getenv ( var , \"\" ) try : return int ( value ) except ( ValueError , TypeError ): return default","title":"Constants"},{"location":"api_docs/constants/#constants","text":"","title":"Constants"},{"location":"api_docs/constants/#zenml.constants","text":"","title":"constants"},{"location":"api_docs/constants/#zenml.constants.handle_bool_env_var","text":"Converts normal env var to boolean Source code in zenml/constants.py def handle_bool_env_var ( var : str , default : bool = False ) -> bool : \"\"\"Converts normal env var to boolean\"\"\" value = os . getenv ( var ) if value in [ \"1\" , \"y\" , \"yes\" , \"True\" , \"true\" ]: return True return default","title":"handle_bool_env_var()"},{"location":"api_docs/constants/#zenml.constants.handle_int_env_var","text":"Converts normal env var to int Source code in zenml/constants.py def handle_int_env_var ( var : str , default : int = 0 ) -> int : \"\"\"Converts normal env var to int\"\"\" value = os . getenv ( var , \"\" ) try : return int ( value ) except ( ValueError , TypeError ): return default","title":"handle_int_env_var()"},{"location":"api_docs/container_registries/","text":"Container Registries zenml.container_registries special base_container_registry BaseContainerRegistry ( StackComponent ) pydantic-model Base class for all ZenML container registries. Attributes: Name Type Description uri str The URI of the container registry. Source code in zenml/container_registries/base_container_registry.py class BaseContainerRegistry ( StackComponent ): \"\"\"Base class for all ZenML container registries. Attributes: uri: The URI of the container registry. \"\"\" uri : str supports_local_execution = True supports_remote_execution = True @property def type ( self ) -> StackComponentType : \"\"\"The component type.\"\"\" return StackComponentType . CONTAINER_REGISTRY @property def flavor ( self ) -> ContainerRegistryFlavor : \"\"\"The container registry flavor.\"\"\" return ContainerRegistryFlavor . DEFAULT flavor : ContainerRegistryFlavor property readonly The container registry flavor. type : StackComponentType property readonly The component type.","title":"Container Registries"},{"location":"api_docs/container_registries/#container-registries","text":"","title":"Container Registries"},{"location":"api_docs/container_registries/#zenml.container_registries","text":"","title":"container_registries"},{"location":"api_docs/container_registries/#zenml.container_registries.base_container_registry","text":"","title":"base_container_registry"},{"location":"api_docs/container_registries/#zenml.container_registries.base_container_registry.BaseContainerRegistry","text":"Base class for all ZenML container registries. Attributes: Name Type Description uri str The URI of the container registry. Source code in zenml/container_registries/base_container_registry.py class BaseContainerRegistry ( StackComponent ): \"\"\"Base class for all ZenML container registries. Attributes: uri: The URI of the container registry. \"\"\" uri : str supports_local_execution = True supports_remote_execution = True @property def type ( self ) -> StackComponentType : \"\"\"The component type.\"\"\" return StackComponentType . CONTAINER_REGISTRY @property def flavor ( self ) -> ContainerRegistryFlavor : \"\"\"The container registry flavor.\"\"\" return ContainerRegistryFlavor . DEFAULT","title":"BaseContainerRegistry"},{"location":"api_docs/container_registries/#zenml.container_registries.base_container_registry.BaseContainerRegistry.flavor","text":"The container registry flavor.","title":"flavor"},{"location":"api_docs/container_registries/#zenml.container_registries.base_container_registry.BaseContainerRegistry.type","text":"The component type.","title":"type"},{"location":"api_docs/enums/","text":"Enums zenml.enums ArtifactStoreFlavor ( StackComponentFlavor ) All supported artifact store flavors. Source code in zenml/enums.py class ArtifactStoreFlavor ( StackComponentFlavor ): \"\"\"All supported artifact store flavors.\"\"\" LOCAL = \"local\" GCP = \"gcp\" ContainerRegistryFlavor ( StackComponentFlavor ) All supported container registry flavors. Source code in zenml/enums.py class ContainerRegistryFlavor ( StackComponentFlavor ): \"\"\"All supported container registry flavors.\"\"\" DEFAULT = \"default\" ExecutionStatus ( Enum ) Enum that represents the current status of a step or pipeline run. Source code in zenml/enums.py class ExecutionStatus ( Enum ): \"\"\"Enum that represents the current status of a step or pipeline run.\"\"\" FAILED = \"failed\" COMPLETED = \"completed\" RUNNING = \"running\" CACHED = \"cached\" LoggingLevels ( Enum ) Enum for logging levels. Source code in zenml/enums.py class LoggingLevels ( Enum ): \"\"\"Enum for logging levels.\"\"\" NOTSET = logging . NOTSET ERROR = logging . ERROR WARN = logging . WARN INFO = logging . INFO DEBUG = logging . DEBUG CRITICAL = logging . CRITICAL MetadataStoreFlavor ( StackComponentFlavor ) All supported metadata store flavors. Source code in zenml/enums.py class MetadataStoreFlavor ( StackComponentFlavor ): \"\"\"All supported metadata store flavors.\"\"\" SQLITE = \"sqlite\" MYSQL = \"mysql\" KUBEFLOW = \"kubeflow\" OrchestratorFlavor ( StackComponentFlavor ) All supported orchestrator flavors. Source code in zenml/enums.py class OrchestratorFlavor ( StackComponentFlavor ): \"\"\"All supported orchestrator flavors.\"\"\" LOCAL = \"local\" KUBEFLOW = \"kubeflow\" AIRFLOW = \"airflow\" StackComponentFlavor ( str , Enum ) Abstract base class for all stack component flavors. Source code in zenml/enums.py class StackComponentFlavor ( str , Enum ): \"\"\"Abstract base class for all stack component flavors.\"\"\" StackComponentType ( str , Enum ) All possible types a StackComponent can have. Source code in zenml/enums.py class StackComponentType ( str , Enum ): \"\"\"All possible types a `StackComponent` can have.\"\"\" ORCHESTRATOR = \"orchestrator\" METADATA_STORE = \"metadata_store\" ARTIFACT_STORE = \"artifact_store\" CONTAINER_REGISTRY = \"container_registry\" @property def plural ( self ) -> str : \"\"\"Returns the plural of the enum value.\"\"\" if self == StackComponentType . CONTAINER_REGISTRY : return \"container_registries\" return f \" { self . value } s\"","title":"Enums"},{"location":"api_docs/enums/#enums","text":"","title":"Enums"},{"location":"api_docs/enums/#zenml.enums","text":"","title":"enums"},{"location":"api_docs/enums/#zenml.enums.ArtifactStoreFlavor","text":"All supported artifact store flavors. Source code in zenml/enums.py class ArtifactStoreFlavor ( StackComponentFlavor ): \"\"\"All supported artifact store flavors.\"\"\" LOCAL = \"local\" GCP = \"gcp\"","title":"ArtifactStoreFlavor"},{"location":"api_docs/enums/#zenml.enums.ContainerRegistryFlavor","text":"All supported container registry flavors. Source code in zenml/enums.py class ContainerRegistryFlavor ( StackComponentFlavor ): \"\"\"All supported container registry flavors.\"\"\" DEFAULT = \"default\"","title":"ContainerRegistryFlavor"},{"location":"api_docs/enums/#zenml.enums.ExecutionStatus","text":"Enum that represents the current status of a step or pipeline run. Source code in zenml/enums.py class ExecutionStatus ( Enum ): \"\"\"Enum that represents the current status of a step or pipeline run.\"\"\" FAILED = \"failed\" COMPLETED = \"completed\" RUNNING = \"running\" CACHED = \"cached\"","title":"ExecutionStatus"},{"location":"api_docs/enums/#zenml.enums.LoggingLevels","text":"Enum for logging levels. Source code in zenml/enums.py class LoggingLevels ( Enum ): \"\"\"Enum for logging levels.\"\"\" NOTSET = logging . NOTSET ERROR = logging . ERROR WARN = logging . WARN INFO = logging . INFO DEBUG = logging . DEBUG CRITICAL = logging . CRITICAL","title":"LoggingLevels"},{"location":"api_docs/enums/#zenml.enums.MetadataStoreFlavor","text":"All supported metadata store flavors. Source code in zenml/enums.py class MetadataStoreFlavor ( StackComponentFlavor ): \"\"\"All supported metadata store flavors.\"\"\" SQLITE = \"sqlite\" MYSQL = \"mysql\" KUBEFLOW = \"kubeflow\"","title":"MetadataStoreFlavor"},{"location":"api_docs/enums/#zenml.enums.OrchestratorFlavor","text":"All supported orchestrator flavors. Source code in zenml/enums.py class OrchestratorFlavor ( StackComponentFlavor ): \"\"\"All supported orchestrator flavors.\"\"\" LOCAL = \"local\" KUBEFLOW = \"kubeflow\" AIRFLOW = \"airflow\"","title":"OrchestratorFlavor"},{"location":"api_docs/enums/#zenml.enums.StackComponentFlavor","text":"Abstract base class for all stack component flavors. Source code in zenml/enums.py class StackComponentFlavor ( str , Enum ): \"\"\"Abstract base class for all stack component flavors.\"\"\"","title":"StackComponentFlavor"},{"location":"api_docs/enums/#zenml.enums.StackComponentType","text":"All possible types a StackComponent can have. Source code in zenml/enums.py class StackComponentType ( str , Enum ): \"\"\"All possible types a `StackComponent` can have.\"\"\" ORCHESTRATOR = \"orchestrator\" METADATA_STORE = \"metadata_store\" ARTIFACT_STORE = \"artifact_store\" CONTAINER_REGISTRY = \"container_registry\" @property def plural ( self ) -> str : \"\"\"Returns the plural of the enum value.\"\"\" if self == StackComponentType . CONTAINER_REGISTRY : return \"container_registries\" return f \" { self . value } s\"","title":"StackComponentType"},{"location":"api_docs/exceptions/","text":"Exceptions zenml.exceptions ZenML specific exception definitions AlreadyExistsException ( Exception ) Raises exception when the name already exist in the system but an action is trying to create a resource with the same name. Source code in zenml/exceptions.py class AlreadyExistsException ( Exception ): \"\"\"Raises exception when the `name` already exist in the system but an action is trying to create a resource with the same name.\"\"\" def __init__ ( self , message : Optional [ str ] = None , name : str = \"\" , resource_type : str = \"\" , ): if message is None : message = f \" { resource_type } ` { name } ` already exists!\" super () . __init__ ( message ) ArtifactInterfaceError ( Exception ) Raises exception when interacting with the Artifact interface in an unsupported way. Source code in zenml/exceptions.py class ArtifactInterfaceError ( Exception ): \"\"\"Raises exception when interacting with the Artifact interface in an unsupported way.\"\"\" DoesNotExistException ( Exception ) Raises exception when the entity does not exist in the system but an action is being done that requires it to be present. Source code in zenml/exceptions.py class DoesNotExistException ( Exception ): \"\"\"Raises exception when the entity does not exist in the system but an action is being done that requires it to be present.\"\"\" def __init__ ( self , message : str ): super () . __init__ ( message ) DuplicateRunNameError ( RuntimeError ) Raises exception when a run with the same name already exists. Source code in zenml/exceptions.py class DuplicateRunNameError ( RuntimeError ): \"\"\"Raises exception when a run with the same name already exists.\"\"\" def __init__ ( self , message : str = \"Unable to run a pipeline with a run name that \" \"already exists.\" , ): super () . __init__ ( message ) EmptyDatasourceException ( Exception ) Raises exception when a datasource data is accessed without running an associated pipeline. Source code in zenml/exceptions.py class EmptyDatasourceException ( Exception ): \"\"\"Raises exception when a datasource data is accessed without running an associated pipeline.\"\"\" def __init__ ( self , message : str = \"This datasource has not been used in \" \"any pipelines, therefore the associated data has no \" \"versions. Please use this datasource in any ZenML \" \"pipeline with `pipeline.add_datasource(\" \"datasource)`\" , ): super () . __init__ ( message ) GitException ( Exception ) Raises exception when a problem occurs in git resolution. Source code in zenml/exceptions.py class GitException ( Exception ): \"\"\"Raises exception when a problem occurs in git resolution.\"\"\" def __init__ ( self , message : str = \"There is a problem with git resolution. \" \"Please make sure that all relevant files \" \"are committed.\" , ): super () . __init__ ( message ) InitializationException ( Exception ) Raised when an error occurred during initialization of a ZenML repository. Source code in zenml/exceptions.py class InitializationException ( Exception ): \"\"\"Raised when an error occurred during initialization of a ZenML repository.\"\"\" IntegrationError ( Exception ) Raises exceptions when a requested integration can not be activated. Source code in zenml/exceptions.py class IntegrationError ( Exception ): \"\"\"Raises exceptions when a requested integration can not be activated.\"\"\" MissingStepParameterError ( Exception ) Raises exceptions when a step parameter is missing when running a pipeline. Source code in zenml/exceptions.py class MissingStepParameterError ( Exception ): \"\"\"Raises exceptions when a step parameter is missing when running a pipeline.\"\"\" def __init__ ( self , step_name : str , missing_parameters : List [ str ], config_class : Type [ \"BaseStepConfig\" ], ): \"\"\" Initializes a MissingStepParameterError object. Args: step_name: Name of the step for which one or more parameters are missing. missing_parameters: Names of all parameters which are missing. config_class: Class of the configuration object for which the parameters are missing. \"\"\" message = textwrap . fill ( textwrap . dedent ( f \"\"\" Missing parameters { missing_parameters } for ' { step_name } ' step. There are three ways to solve this issue: (1) Specify a default value in the configuration class ` { config_class . __name__ } ` (2) Specify the parameters in code when creating the pipeline: `my_pipeline( { step_name } (config= { config_class . __name__ } (...))` (3) Specify the parameters in a yaml configuration file and pass it to the pipeline: `my_pipeline(...).with_config('path_to_yaml')` \"\"\" ) ) super () . __init__ ( message ) __init__ ( self , step_name , missing_parameters , config_class ) special Initializes a MissingStepParameterError object. Parameters: Name Type Description Default step_name str Name of the step for which one or more parameters are missing. required missing_parameters List[str] Names of all parameters which are missing. required config_class Type[BaseStepConfig] Class of the configuration object for which the parameters are missing. required Source code in zenml/exceptions.py def __init__ ( self , step_name : str , missing_parameters : List [ str ], config_class : Type [ \"BaseStepConfig\" ], ): \"\"\" Initializes a MissingStepParameterError object. Args: step_name: Name of the step for which one or more parameters are missing. missing_parameters: Names of all parameters which are missing. config_class: Class of the configuration object for which the parameters are missing. \"\"\" message = textwrap . fill ( textwrap . dedent ( f \"\"\" Missing parameters { missing_parameters } for ' { step_name } ' step. There are three ways to solve this issue: (1) Specify a default value in the configuration class ` { config_class . __name__ } ` (2) Specify the parameters in code when creating the pipeline: `my_pipeline( { step_name } (config= { config_class . __name__ } (...))` (3) Specify the parameters in a yaml configuration file and pass it to the pipeline: `my_pipeline(...).with_config('path_to_yaml')` \"\"\" ) ) super () . __init__ ( message ) PipelineConfigurationError ( Exception ) Raises exceptions when a pipeline configuration contains invalid values. Source code in zenml/exceptions.py class PipelineConfigurationError ( Exception ): \"\"\"Raises exceptions when a pipeline configuration contains invalid values.\"\"\" PipelineInterfaceError ( Exception ) Raises exception when interacting with the Pipeline interface in an unsupported way. Source code in zenml/exceptions.py class PipelineInterfaceError ( Exception ): \"\"\"Raises exception when interacting with the Pipeline interface in an unsupported way.\"\"\" PipelineNotSucceededException ( Exception ) Raises exception when trying to fetch artifacts from a not succeeded pipeline. Source code in zenml/exceptions.py class PipelineNotSucceededException ( Exception ): \"\"\"Raises exception when trying to fetch artifacts from a not succeeded pipeline.\"\"\" def __init__ ( self , name : str = \"\" , message : str = \" {} is not yet completed successfully.\" , ): super () . __init__ ( message . format ( name )) ProvisioningError ( Exception ) Raised when an error occurs when provisioning resources for a StackComponent. Source code in zenml/exceptions.py class ProvisioningError ( Exception ): \"\"\"Raised when an error occurs when provisioning resources for a StackComponent.\"\"\" RepositoryNotFoundError ( Exception ) Raised when no ZenML repository directory is found when creating a ZenML repository instance. Source code in zenml/exceptions.py class RepositoryNotFoundError ( Exception ): \"\"\"Raised when no ZenML repository directory is found when creating a ZenML repository instance.\"\"\" StackComponentExistsError ( Exception ) Raised when trying to register a stack component with a name that already exists. Source code in zenml/exceptions.py class StackComponentExistsError ( Exception ): \"\"\"Raised when trying to register a stack component with a name that already exists.\"\"\" StackExistsError ( Exception ) Raised when trying to register a stack with a name that already exists. Source code in zenml/exceptions.py class StackExistsError ( Exception ): \"\"\"Raised when trying to register a stack with a name that already exists.\"\"\" StackValidationError ( Exception ) Raised when a stack configuration is not valid. Source code in zenml/exceptions.py class StackValidationError ( Exception ): \"\"\"Raised when a stack configuration is not valid.\"\"\" StepContextError ( Exception ) Raises exception when interacting with a StepContext in an unsupported way. Source code in zenml/exceptions.py class StepContextError ( Exception ): \"\"\"Raises exception when interacting with a StepContext in an unsupported way.\"\"\" StepInterfaceError ( Exception ) Raises exception when interacting with the Step interface in an unsupported way. Source code in zenml/exceptions.py class StepInterfaceError ( Exception ): \"\"\"Raises exception when interacting with the Step interface in an unsupported way.\"\"\"","title":"Exceptions"},{"location":"api_docs/exceptions/#exceptions","text":"","title":"Exceptions"},{"location":"api_docs/exceptions/#zenml.exceptions","text":"ZenML specific exception definitions","title":"exceptions"},{"location":"api_docs/exceptions/#zenml.exceptions.AlreadyExistsException","text":"Raises exception when the name already exist in the system but an action is trying to create a resource with the same name. Source code in zenml/exceptions.py class AlreadyExistsException ( Exception ): \"\"\"Raises exception when the `name` already exist in the system but an action is trying to create a resource with the same name.\"\"\" def __init__ ( self , message : Optional [ str ] = None , name : str = \"\" , resource_type : str = \"\" , ): if message is None : message = f \" { resource_type } ` { name } ` already exists!\" super () . __init__ ( message )","title":"AlreadyExistsException"},{"location":"api_docs/exceptions/#zenml.exceptions.ArtifactInterfaceError","text":"Raises exception when interacting with the Artifact interface in an unsupported way. Source code in zenml/exceptions.py class ArtifactInterfaceError ( Exception ): \"\"\"Raises exception when interacting with the Artifact interface in an unsupported way.\"\"\"","title":"ArtifactInterfaceError"},{"location":"api_docs/exceptions/#zenml.exceptions.DoesNotExistException","text":"Raises exception when the entity does not exist in the system but an action is being done that requires it to be present. Source code in zenml/exceptions.py class DoesNotExistException ( Exception ): \"\"\"Raises exception when the entity does not exist in the system but an action is being done that requires it to be present.\"\"\" def __init__ ( self , message : str ): super () . __init__ ( message )","title":"DoesNotExistException"},{"location":"api_docs/exceptions/#zenml.exceptions.DuplicateRunNameError","text":"Raises exception when a run with the same name already exists. Source code in zenml/exceptions.py class DuplicateRunNameError ( RuntimeError ): \"\"\"Raises exception when a run with the same name already exists.\"\"\" def __init__ ( self , message : str = \"Unable to run a pipeline with a run name that \" \"already exists.\" , ): super () . __init__ ( message )","title":"DuplicateRunNameError"},{"location":"api_docs/exceptions/#zenml.exceptions.EmptyDatasourceException","text":"Raises exception when a datasource data is accessed without running an associated pipeline. Source code in zenml/exceptions.py class EmptyDatasourceException ( Exception ): \"\"\"Raises exception when a datasource data is accessed without running an associated pipeline.\"\"\" def __init__ ( self , message : str = \"This datasource has not been used in \" \"any pipelines, therefore the associated data has no \" \"versions. Please use this datasource in any ZenML \" \"pipeline with `pipeline.add_datasource(\" \"datasource)`\" , ): super () . __init__ ( message )","title":"EmptyDatasourceException"},{"location":"api_docs/exceptions/#zenml.exceptions.GitException","text":"Raises exception when a problem occurs in git resolution. Source code in zenml/exceptions.py class GitException ( Exception ): \"\"\"Raises exception when a problem occurs in git resolution.\"\"\" def __init__ ( self , message : str = \"There is a problem with git resolution. \" \"Please make sure that all relevant files \" \"are committed.\" , ): super () . __init__ ( message )","title":"GitException"},{"location":"api_docs/exceptions/#zenml.exceptions.InitializationException","text":"Raised when an error occurred during initialization of a ZenML repository. Source code in zenml/exceptions.py class InitializationException ( Exception ): \"\"\"Raised when an error occurred during initialization of a ZenML repository.\"\"\"","title":"InitializationException"},{"location":"api_docs/exceptions/#zenml.exceptions.IntegrationError","text":"Raises exceptions when a requested integration can not be activated. Source code in zenml/exceptions.py class IntegrationError ( Exception ): \"\"\"Raises exceptions when a requested integration can not be activated.\"\"\"","title":"IntegrationError"},{"location":"api_docs/exceptions/#zenml.exceptions.MissingStepParameterError","text":"Raises exceptions when a step parameter is missing when running a pipeline. Source code in zenml/exceptions.py class MissingStepParameterError ( Exception ): \"\"\"Raises exceptions when a step parameter is missing when running a pipeline.\"\"\" def __init__ ( self , step_name : str , missing_parameters : List [ str ], config_class : Type [ \"BaseStepConfig\" ], ): \"\"\" Initializes a MissingStepParameterError object. Args: step_name: Name of the step for which one or more parameters are missing. missing_parameters: Names of all parameters which are missing. config_class: Class of the configuration object for which the parameters are missing. \"\"\" message = textwrap . fill ( textwrap . dedent ( f \"\"\" Missing parameters { missing_parameters } for ' { step_name } ' step. There are three ways to solve this issue: (1) Specify a default value in the configuration class ` { config_class . __name__ } ` (2) Specify the parameters in code when creating the pipeline: `my_pipeline( { step_name } (config= { config_class . __name__ } (...))` (3) Specify the parameters in a yaml configuration file and pass it to the pipeline: `my_pipeline(...).with_config('path_to_yaml')` \"\"\" ) ) super () . __init__ ( message )","title":"MissingStepParameterError"},{"location":"api_docs/exceptions/#zenml.exceptions.MissingStepParameterError.__init__","text":"Initializes a MissingStepParameterError object. Parameters: Name Type Description Default step_name str Name of the step for which one or more parameters are missing. required missing_parameters List[str] Names of all parameters which are missing. required config_class Type[BaseStepConfig] Class of the configuration object for which the parameters are missing. required Source code in zenml/exceptions.py def __init__ ( self , step_name : str , missing_parameters : List [ str ], config_class : Type [ \"BaseStepConfig\" ], ): \"\"\" Initializes a MissingStepParameterError object. Args: step_name: Name of the step for which one or more parameters are missing. missing_parameters: Names of all parameters which are missing. config_class: Class of the configuration object for which the parameters are missing. \"\"\" message = textwrap . fill ( textwrap . dedent ( f \"\"\" Missing parameters { missing_parameters } for ' { step_name } ' step. There are three ways to solve this issue: (1) Specify a default value in the configuration class ` { config_class . __name__ } ` (2) Specify the parameters in code when creating the pipeline: `my_pipeline( { step_name } (config= { config_class . __name__ } (...))` (3) Specify the parameters in a yaml configuration file and pass it to the pipeline: `my_pipeline(...).with_config('path_to_yaml')` \"\"\" ) ) super () . __init__ ( message )","title":"__init__()"},{"location":"api_docs/exceptions/#zenml.exceptions.PipelineConfigurationError","text":"Raises exceptions when a pipeline configuration contains invalid values. Source code in zenml/exceptions.py class PipelineConfigurationError ( Exception ): \"\"\"Raises exceptions when a pipeline configuration contains invalid values.\"\"\"","title":"PipelineConfigurationError"},{"location":"api_docs/exceptions/#zenml.exceptions.PipelineInterfaceError","text":"Raises exception when interacting with the Pipeline interface in an unsupported way. Source code in zenml/exceptions.py class PipelineInterfaceError ( Exception ): \"\"\"Raises exception when interacting with the Pipeline interface in an unsupported way.\"\"\"","title":"PipelineInterfaceError"},{"location":"api_docs/exceptions/#zenml.exceptions.PipelineNotSucceededException","text":"Raises exception when trying to fetch artifacts from a not succeeded pipeline. Source code in zenml/exceptions.py class PipelineNotSucceededException ( Exception ): \"\"\"Raises exception when trying to fetch artifacts from a not succeeded pipeline.\"\"\" def __init__ ( self , name : str = \"\" , message : str = \" {} is not yet completed successfully.\" , ): super () . __init__ ( message . format ( name ))","title":"PipelineNotSucceededException"},{"location":"api_docs/exceptions/#zenml.exceptions.ProvisioningError","text":"Raised when an error occurs when provisioning resources for a StackComponent. Source code in zenml/exceptions.py class ProvisioningError ( Exception ): \"\"\"Raised when an error occurs when provisioning resources for a StackComponent.\"\"\"","title":"ProvisioningError"},{"location":"api_docs/exceptions/#zenml.exceptions.RepositoryNotFoundError","text":"Raised when no ZenML repository directory is found when creating a ZenML repository instance. Source code in zenml/exceptions.py class RepositoryNotFoundError ( Exception ): \"\"\"Raised when no ZenML repository directory is found when creating a ZenML repository instance.\"\"\"","title":"RepositoryNotFoundError"},{"location":"api_docs/exceptions/#zenml.exceptions.StackComponentExistsError","text":"Raised when trying to register a stack component with a name that already exists. Source code in zenml/exceptions.py class StackComponentExistsError ( Exception ): \"\"\"Raised when trying to register a stack component with a name that already exists.\"\"\"","title":"StackComponentExistsError"},{"location":"api_docs/exceptions/#zenml.exceptions.StackExistsError","text":"Raised when trying to register a stack with a name that already exists. Source code in zenml/exceptions.py class StackExistsError ( Exception ): \"\"\"Raised when trying to register a stack with a name that already exists.\"\"\"","title":"StackExistsError"},{"location":"api_docs/exceptions/#zenml.exceptions.StackValidationError","text":"Raised when a stack configuration is not valid. Source code in zenml/exceptions.py class StackValidationError ( Exception ): \"\"\"Raised when a stack configuration is not valid.\"\"\"","title":"StackValidationError"},{"location":"api_docs/exceptions/#zenml.exceptions.StepContextError","text":"Raises exception when interacting with a StepContext in an unsupported way. Source code in zenml/exceptions.py class StepContextError ( Exception ): \"\"\"Raises exception when interacting with a StepContext in an unsupported way.\"\"\"","title":"StepContextError"},{"location":"api_docs/exceptions/#zenml.exceptions.StepInterfaceError","text":"Raises exception when interacting with the Step interface in an unsupported way. Source code in zenml/exceptions.py class StepInterfaceError ( Exception ): \"\"\"Raises exception when interacting with the Step interface in an unsupported way.\"\"\"","title":"StepInterfaceError"},{"location":"api_docs/integrations/","text":"Integrations zenml.integrations special The ZenML integrations module contains sub-modules for each integration that we support. This includes orchestrators like Apache Airflow, visualization tools like the facets library, as well as deep learning libraries like PyTorch. airflow special The Airflow integration sub-module powers an alternative to the local orchestrator. You can enable it by registering the Airflow orchestrator with the CLI tool, then bootstrap using the zenml orchestrator up command. AirflowIntegration ( Integration ) Definition of Airflow Integration for ZenML. Source code in zenml/integrations/airflow/__init__.py class AirflowIntegration ( Integration ): \"\"\"Definition of Airflow Integration for ZenML.\"\"\" NAME = AIRFLOW REQUIREMENTS = [ \"apache-airflow==2.2.0\" ] @classmethod def activate ( cls ): \"\"\"Activates all classes required for the airflow integration.\"\"\" from zenml.integrations.airflow import orchestrators # noqa activate () classmethod Activates all classes required for the airflow integration. Source code in zenml/integrations/airflow/__init__.py @classmethod def activate ( cls ): \"\"\"Activates all classes required for the airflow integration.\"\"\" from zenml.integrations.airflow import orchestrators # noqa orchestrators special airflow_component Definition for Airflow component for TFX. AirflowComponent ( PythonOperator ) Airflow-specific TFX Component. This class wrap a component run into its own PythonOperator in Airflow. Source code in zenml/integrations/airflow/orchestrators/airflow_component.py class AirflowComponent ( python . PythonOperator ): \"\"\"Airflow-specific TFX Component. This class wrap a component run into its own PythonOperator in Airflow. \"\"\" def __init__ ( self , * , parent_dag : airflow . DAG , pipeline_node : pipeline_pb2 . PipelineNode , mlmd_connection : metadata . Metadata , pipeline_info : pipeline_pb2 . PipelineInfo , pipeline_runtime_spec : pipeline_pb2 . PipelineRuntimeSpec , executor_spec : Optional [ message . Message ] = None , custom_driver_spec : Optional [ message . Message ] = None ) -> None : \"\"\"Constructs an Airflow implementation of TFX component. Args: parent_dag: The airflow DAG that this component is contained in. pipeline_node: The specification of the node to launch. mlmd_connection: ML metadata connection info. pipeline_info: The information of the pipeline that this node runs in. pipeline_runtime_spec: The runtime information of the pipeline that this node runs in. executor_spec: Specification for the executor of the node. custom_driver_spec: Specification for custom driver. \"\"\" launcher_callable = functools . partial ( _airflow_component_launcher , pipeline_node = pipeline_node , mlmd_connection = mlmd_connection , pipeline_info = pipeline_info , pipeline_runtime_spec = pipeline_runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) super () . __init__ ( task_id = pipeline_node . node_info . id , provide_context = True , python_callable = launcher_callable , dag = parent_dag , ) __init__ ( self , * , parent_dag , pipeline_node , mlmd_connection , pipeline_info , pipeline_runtime_spec , executor_spec = None , custom_driver_spec = None ) special Constructs an Airflow implementation of TFX component. Parameters: Name Type Description Default parent_dag DAG The airflow DAG that this component is contained in. required pipeline_node PipelineNode The specification of the node to launch. required mlmd_connection Metadata ML metadata connection info. required pipeline_info PipelineInfo The information of the pipeline that this node runs in. required pipeline_runtime_spec PipelineRuntimeSpec The runtime information of the pipeline that this node runs in. required executor_spec Optional[google.protobuf.message.Message] Specification for the executor of the node. None custom_driver_spec Optional[google.protobuf.message.Message] Specification for custom driver. None Source code in zenml/integrations/airflow/orchestrators/airflow_component.py def __init__ ( self , * , parent_dag : airflow . DAG , pipeline_node : pipeline_pb2 . PipelineNode , mlmd_connection : metadata . Metadata , pipeline_info : pipeline_pb2 . PipelineInfo , pipeline_runtime_spec : pipeline_pb2 . PipelineRuntimeSpec , executor_spec : Optional [ message . Message ] = None , custom_driver_spec : Optional [ message . Message ] = None ) -> None : \"\"\"Constructs an Airflow implementation of TFX component. Args: parent_dag: The airflow DAG that this component is contained in. pipeline_node: The specification of the node to launch. mlmd_connection: ML metadata connection info. pipeline_info: The information of the pipeline that this node runs in. pipeline_runtime_spec: The runtime information of the pipeline that this node runs in. executor_spec: Specification for the executor of the node. custom_driver_spec: Specification for custom driver. \"\"\" launcher_callable = functools . partial ( _airflow_component_launcher , pipeline_node = pipeline_node , mlmd_connection = mlmd_connection , pipeline_info = pipeline_info , pipeline_runtime_spec = pipeline_runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) super () . __init__ ( task_id = pipeline_node . node_info . id , provide_context = True , python_callable = launcher_callable , dag = parent_dag , ) airflow_dag_runner Definition of Airflow TFX runner. This is an unmodified copy from the TFX source code (outside of superficial, stylistic changes) AirflowDagRunner ( TfxRunner ) Tfx runner on Airflow. Source code in zenml/integrations/airflow/orchestrators/airflow_dag_runner.py class AirflowDagRunner ( tfx_runner . TfxRunner ): \"\"\"Tfx runner on Airflow.\"\"\" def __init__ ( self , config : Optional [ Union [ Dict [ str , Any ], AirflowPipelineConfig ]] = None , ): \"\"\"Creates an instance of AirflowDagRunner. Args: config: Optional Airflow pipeline config for customizing the launching of each component. \"\"\" if isinstance ( config , dict ): warnings . warn ( \"Pass config as a dict type is going to deprecated in 0.1.16. \" \"Use AirflowPipelineConfig type instead.\" , PendingDeprecationWarning , ) config = AirflowPipelineConfig ( airflow_dag_config = config ) super () . __init__ ( config ) def run ( self , pipeline : tfx_pipeline . Pipeline , run_name : str = \"\" ) -> \"airflow.DAG\" : \"\"\"Deploys given logical pipeline on Airflow. Args: pipeline: Logical pipeline containing pipeline args and comps. run_name: Optional name for the run. Returns: An Airflow DAG. \"\"\" # Only import these when needed. import airflow # noqa from zenml.integrations.airflow.orchestrators import airflow_component # Merge airflow-specific configs with pipeline args airflow_dag = airflow . DAG ( dag_id = pipeline . pipeline_info . pipeline_name , ** ( typing . cast ( AirflowPipelineConfig , self . _config ) . airflow_dag_config ), is_paused_upon_creation = False , catchup = False , # no backfill ) if \"tmp_dir\" not in pipeline . additional_pipeline_args : tmp_dir = os . path . join ( pipeline . pipeline_info . pipeline_root , \".temp\" , \"\" ) pipeline . additional_pipeline_args [ \"tmp_dir\" ] = tmp_dir for component in pipeline . components : if isinstance ( component , base_component . BaseComponent ): component . _resolve_pip_dependencies ( pipeline . pipeline_info . pipeline_root ) self . _replace_runtime_params ( component ) c = compiler . Compiler () pipeline = c . compile ( pipeline ) # Substitute the runtime parameter to be a concrete run_id runtime_parameter_utils . substitute_runtime_parameter ( pipeline , { \"pipeline-run-id\" : run_name , }, ) deployment_config = runner_utils . extract_local_deployment_config ( pipeline ) connection_config = ( Repository () . active_stack . metadata_store . get_tfx_metadata_config () ) component_impl_map = {} for node in pipeline . nodes : pipeline_node = node . pipeline_node node_id = pipeline_node . node_info . id executor_spec = runner_utils . extract_executor_spec ( deployment_config , node_id ) custom_driver_spec = runner_utils . extract_custom_driver_spec ( deployment_config , node_id ) current_airflow_component = airflow_component . AirflowComponent ( parent_dag = airflow_dag , pipeline_node = pipeline_node , mlmd_connection = connection_config , pipeline_info = pipeline . pipeline_info , pipeline_runtime_spec = pipeline . runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) component_impl_map [ node_id ] = current_airflow_component for upstream_node in node . pipeline_node . upstream_nodes : assert ( upstream_node in component_impl_map ), \"Components is not in topological order\" current_airflow_component . set_upstream ( component_impl_map [ upstream_node ] ) return airflow_dag def _replace_runtime_params ( self , comp : base_node . BaseNode ) -> base_node . BaseNode : \"\"\"Replaces runtime params for dynamic Airflow parameter execution. Args: comp: TFX component to be parsed. Returns: Returns edited component. \"\"\" for k , prop in comp . exec_properties . copy () . items (): if isinstance ( prop , RuntimeParameter ): # Airflow only supports string parameters. if prop . ptype != str : raise RuntimeError ( f \"RuntimeParameter in Airflow does not support \" f \" { prop . ptype } . The only ptype supported is string.\" ) # If the default is a template, drop the template markers # when inserting it into the .get() default argument below. # Otherwise, provide the default as a quoted string. default = cast ( str , prop . default ) if default . startswith ( \"{{\" ) and default . endswith ( \"}}\" ): default = default [ 2 : - 2 ] else : default = json . dumps ( default ) template_field = '{{ dag_run.conf.get(\" %s \", %s ) }}' % ( prop . name , default , ) comp . exec_properties [ k ] = template_field return comp __init__ ( self , config = None ) special Creates an instance of AirflowDagRunner. Parameters: Name Type Description Default config Union[Dict[str, Any], zenml.integrations.airflow.orchestrators.airflow_dag_runner.AirflowPipelineConfig] Optional Airflow pipeline config for customizing the None Source code in zenml/integrations/airflow/orchestrators/airflow_dag_runner.py def __init__ ( self , config : Optional [ Union [ Dict [ str , Any ], AirflowPipelineConfig ]] = None , ): \"\"\"Creates an instance of AirflowDagRunner. Args: config: Optional Airflow pipeline config for customizing the launching of each component. \"\"\" if isinstance ( config , dict ): warnings . warn ( \"Pass config as a dict type is going to deprecated in 0.1.16. \" \"Use AirflowPipelineConfig type instead.\" , PendingDeprecationWarning , ) config = AirflowPipelineConfig ( airflow_dag_config = config ) super () . __init__ ( config ) run ( self , pipeline , run_name = '' ) Deploys given logical pipeline on Airflow. Parameters: Name Type Description Default pipeline Pipeline Logical pipeline containing pipeline args and comps. required run_name str Optional name for the run. '' Returns: Type Description airflow.DAG An Airflow DAG. Source code in zenml/integrations/airflow/orchestrators/airflow_dag_runner.py def run ( self , pipeline : tfx_pipeline . Pipeline , run_name : str = \"\" ) -> \"airflow.DAG\" : \"\"\"Deploys given logical pipeline on Airflow. Args: pipeline: Logical pipeline containing pipeline args and comps. run_name: Optional name for the run. Returns: An Airflow DAG. \"\"\" # Only import these when needed. import airflow # noqa from zenml.integrations.airflow.orchestrators import airflow_component # Merge airflow-specific configs with pipeline args airflow_dag = airflow . DAG ( dag_id = pipeline . pipeline_info . pipeline_name , ** ( typing . cast ( AirflowPipelineConfig , self . _config ) . airflow_dag_config ), is_paused_upon_creation = False , catchup = False , # no backfill ) if \"tmp_dir\" not in pipeline . additional_pipeline_args : tmp_dir = os . path . join ( pipeline . pipeline_info . pipeline_root , \".temp\" , \"\" ) pipeline . additional_pipeline_args [ \"tmp_dir\" ] = tmp_dir for component in pipeline . components : if isinstance ( component , base_component . BaseComponent ): component . _resolve_pip_dependencies ( pipeline . pipeline_info . pipeline_root ) self . _replace_runtime_params ( component ) c = compiler . Compiler () pipeline = c . compile ( pipeline ) # Substitute the runtime parameter to be a concrete run_id runtime_parameter_utils . substitute_runtime_parameter ( pipeline , { \"pipeline-run-id\" : run_name , }, ) deployment_config = runner_utils . extract_local_deployment_config ( pipeline ) connection_config = ( Repository () . active_stack . metadata_store . get_tfx_metadata_config () ) component_impl_map = {} for node in pipeline . nodes : pipeline_node = node . pipeline_node node_id = pipeline_node . node_info . id executor_spec = runner_utils . extract_executor_spec ( deployment_config , node_id ) custom_driver_spec = runner_utils . extract_custom_driver_spec ( deployment_config , node_id ) current_airflow_component = airflow_component . AirflowComponent ( parent_dag = airflow_dag , pipeline_node = pipeline_node , mlmd_connection = connection_config , pipeline_info = pipeline . pipeline_info , pipeline_runtime_spec = pipeline . runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) component_impl_map [ node_id ] = current_airflow_component for upstream_node in node . pipeline_node . upstream_nodes : assert ( upstream_node in component_impl_map ), \"Components is not in topological order\" current_airflow_component . set_upstream ( component_impl_map [ upstream_node ] ) return airflow_dag AirflowPipelineConfig ( PipelineConfig ) Pipeline config for AirflowDagRunner. Source code in zenml/integrations/airflow/orchestrators/airflow_dag_runner.py class AirflowPipelineConfig ( pipeline_config . PipelineConfig ): \"\"\"Pipeline config for AirflowDagRunner.\"\"\" def __init__ ( self , airflow_dag_config : Optional [ Dict [ str , Any ]] = None , ** kwargs : Any ): \"\"\"Creates an instance of AirflowPipelineConfig. Args: airflow_dag_config: Configs of Airflow DAG model. See https://airflow.apache.org/_api/airflow/models/dag/index.html#airflow.models.dag.DAG for the full spec. **kwargs: keyword args for PipelineConfig. \"\"\" super () . __init__ ( ** kwargs ) self . airflow_dag_config = airflow_dag_config or {} __init__ ( self , airflow_dag_config = None , ** kwargs ) special Creates an instance of AirflowPipelineConfig. Parameters: Name Type Description Default airflow_dag_config Optional[Dict[str, Any]] Configs of Airflow DAG model. See https://airflow.apache.org/_api/airflow/models/dag/index.html#airflow.models.dag.DAG for the full spec. None **kwargs Any keyword args for PipelineConfig. {} Source code in zenml/integrations/airflow/orchestrators/airflow_dag_runner.py def __init__ ( self , airflow_dag_config : Optional [ Dict [ str , Any ]] = None , ** kwargs : Any ): \"\"\"Creates an instance of AirflowPipelineConfig. Args: airflow_dag_config: Configs of Airflow DAG model. See https://airflow.apache.org/_api/airflow/models/dag/index.html#airflow.models.dag.DAG for the full spec. **kwargs: keyword args for PipelineConfig. \"\"\" super () . __init__ ( ** kwargs ) self . airflow_dag_config = airflow_dag_config or {} airflow_orchestrator AirflowOrchestrator ( BaseOrchestrator ) pydantic-model Orchestrator responsible for running pipelines using Airflow. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py class AirflowOrchestrator ( BaseOrchestrator ): \"\"\"Orchestrator responsible for running pipelines using Airflow.\"\"\" airflow_home : str = \"\" airflow_config : Optional [ Dict [ str , Any ]] = {} schedule_interval_minutes : int = 1 supports_local_execution = True supports_remote_execution = False def __init__ ( self , ** values : Any ): \"\"\"Sets environment variables to configure airflow.\"\"\" super () . __init__ ( ** values ) self . _set_env () @property def flavor ( self ) -> OrchestratorFlavor : \"\"\"The orchestrator flavor.\"\"\" return OrchestratorFlavor . AIRFLOW @root_validator def set_airflow_home ( cls , values : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Sets airflow home according to orchestrator UUID.\"\"\" if \"uuid\" not in values : raise ValueError ( \"`uuid` needs to exist for AirflowOrchestrator.\" ) values [ \"airflow_home\" ] = os . path . join ( zenml . io . utils . get_global_config_directory (), AIRFLOW_ROOT_DIR , str ( values [ \"uuid\" ]), ) return values @property def dags_directory ( self ) -> str : \"\"\"Returns path to the airflow dags directory.\"\"\" return os . path . join ( self . airflow_home , \"dags\" ) @property def pid_file ( self ) -> str : \"\"\"Returns path to the daemon PID file.\"\"\" return os . path . join ( self . airflow_home , \"airflow_daemon.pid\" ) @property def log_file ( self ) -> str : \"\"\"Returns path to the airflow log file.\"\"\" return os . path . join ( self . airflow_home , \"airflow_orchestrator.log\" ) @property def password_file ( self ) -> str : \"\"\"Returns path to the webserver password file.\"\"\" return os . path . join ( self . airflow_home , \"standalone_admin_password.txt\" ) def _set_env ( self ) -> None : \"\"\"Sets environment variables to configure airflow.\"\"\" os . environ [ \"AIRFLOW_HOME\" ] = self . airflow_home os . environ [ \"AIRFLOW__CORE__DAGS_FOLDER\" ] = self . dags_directory os . environ [ \"AIRFLOW__CORE__DAG_DISCOVERY_SAFE_MODE\" ] = \"false\" os . environ [ \"AIRFLOW__CORE__LOAD_EXAMPLES\" ] = \"false\" # check the DAG folder every 10 seconds for new files os . environ [ \"AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL\" ] = \"10\" def _copy_to_dag_directory_if_necessary ( self , dag_filepath : str ): \"\"\"Copies the DAG module to the airflow DAGs directory if it's not already located there. Args: dag_filepath: Path to the file in which the DAG is defined. \"\"\" dags_directory = fileio . resolve_relative_path ( self . dags_directory ) if dags_directory == os . path . dirname ( dag_filepath ): logger . debug ( \"File is already in airflow DAGs directory.\" ) else : logger . debug ( \"Copying dag file ' %s ' to DAGs directory.\" , dag_filepath ) destination_path = os . path . join ( dags_directory , os . path . basename ( dag_filepath ) ) if fileio . file_exists ( destination_path ): logger . info ( \"File ' %s ' already exists, overwriting with new DAG file\" , destination_path , ) fileio . copy ( dag_filepath , destination_path , overwrite = True ) def _log_webserver_credentials ( self ): \"\"\"Logs URL and credentials to login to the airflow webserver. Raises: FileNotFoundError: If the password file does not exist. \"\"\" if fileio . file_exists ( self . password_file ): with open ( self . password_file ) as file : password = file . read () . strip () else : raise FileNotFoundError ( f \"Can't find password file ' { self . password_file } '\" ) logger . info ( \"To inspect your DAGs, login to http://0.0.0.0:8080 \" \"with username: admin password: %s \" , password , ) def runtime_options ( self ) -> Dict [ str , Any ]: \"\"\"Runtime options for the airflow orchestrator.\"\"\" return { DAG_FILEPATH_OPTION_KEY : None } def prepare_pipeline_deployment ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , runtime_configuration : \"RuntimeConfiguration\" , ) -> None : \"\"\"Checks whether airflow is running and copies the DAG file to the airflow DAGs directory. Raises: RuntimeError: If airflow is not running or no DAG filepath runtime option is provided. \"\"\" if not self . is_running : raise RuntimeError ( \"Airflow orchestrator is currently not running. \" \"Run `zenml orchestrator up` to start the \" \"orchestrator of the active stack.\" ) try : dag_filepath = runtime_configuration [ DAG_FILEPATH_OPTION_KEY ] except KeyError : raise RuntimeError ( f \"No DAG filepath found in runtime configuration. Make sure \" f \"to add the filepath to your airflow DAG file as a runtime \" f \"option (key: ' { DAG_FILEPATH_OPTION_KEY } ').\" ) self . _copy_to_dag_directory_if_necessary ( dag_filepath = dag_filepath ) @property def is_running ( self ) -> bool : \"\"\"Returns whether the airflow daemon is currently running.\"\"\" from airflow.cli.commands.standalone_command import StandaloneCommand from airflow.jobs.triggerer_job import TriggererJob daemon_running = daemon . check_if_daemon_is_running ( self . pid_file ) command = StandaloneCommand () webserver_port_open = command . port_open ( 8080 ) if not daemon_running : if webserver_port_open : raise RuntimeError ( \"The airflow daemon does not seem to be running but \" \"local port 8080 is occupied. Make sure the port is \" \"available and try again.\" ) # exit early so we don't check non-existing airflow databases return False # we can't use StandaloneCommand().is_ready() here as the # Airflow SequentialExecutor apparently does not send a heartbeat # while running a task which would result in this returning `False` # even if Airflow is running. airflow_running = webserver_port_open and command . job_running ( TriggererJob ) return airflow_running @property def is_provisioned ( self ) -> bool : \"\"\"Returns whether the airflow daemon is currently running.\"\"\" return self . is_running def provision ( self ) -> None : \"\"\"Ensures that Airflow is running.\"\"\" if self . is_running : logger . info ( \"Airflow is already running.\" ) self . _log_webserver_credentials () return if not fileio . file_exists ( self . dags_directory ): fileio . create_dir_recursive_if_not_exists ( self . dags_directory ) from airflow.cli.commands.standalone_command import StandaloneCommand try : command = StandaloneCommand () # Run the daemon with a working directory inside the current # zenml repo so the same repo will be used to run the DAGs daemon . run_as_daemon ( command . run , pid_file = self . pid_file , log_file = self . log_file , working_directory = zenml . io . utils . get_zenml_dir (), ) while not self . is_running : # Wait until the daemon started all the relevant airflow # processes time . sleep ( 0.1 ) self . _log_webserver_credentials () except Exception as e : logger . error ( e ) logger . error ( \"An error occurred while starting the Airflow daemon. If you \" \"want to start it manually, use the commands described in the \" \"official Airflow quickstart guide for running Airflow locally.\" ) self . down () def deprovision ( self ) -> None : \"\"\"Stops the airflow daemon if necessary and tears down resources.\"\"\" if self . is_running : daemon . stop_daemon ( self . pid_file , kill_children = True ) fileio . rm_dir ( self . airflow_home ) logger . info ( \"Airflow spun down.\" ) def run_pipeline ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , run_name : str ) -> Any : \"\"\"Schedules a pipeline to be run on Airflow. Returns: An Airflow DAG object that corresponds to the ZenML pipeline. \"\"\" self . airflow_config = { \"schedule_interval\" : datetime . timedelta ( minutes = self . schedule_interval_minutes ), # We set this in the past and turn catchup off and then it works \"start_date\" : datetime . datetime ( 2019 , 1 , 1 ), } runner = AirflowDagRunner ( AirflowPipelineConfig ( self . airflow_config )) tfx_pipeline = create_tfx_pipeline ( pipeline , stack = stack ) return runner . run ( tfx_pipeline , run_name = run_name ) dags_directory : str property readonly Returns path to the airflow dags directory. flavor : OrchestratorFlavor property readonly The orchestrator flavor. is_provisioned : bool property readonly Returns whether the airflow daemon is currently running. is_running : bool property readonly Returns whether the airflow daemon is currently running. log_file : str property readonly Returns path to the airflow log file. password_file : str property readonly Returns path to the webserver password file. pid_file : str property readonly Returns path to the daemon PID file. __init__ ( self , ** values ) special Sets environment variables to configure airflow. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py def __init__ ( self , ** values : Any ): \"\"\"Sets environment variables to configure airflow.\"\"\" super () . __init__ ( ** values ) self . _set_env () deprovision ( self ) Stops the airflow daemon if necessary and tears down resources. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py def deprovision ( self ) -> None : \"\"\"Stops the airflow daemon if necessary and tears down resources.\"\"\" if self . is_running : daemon . stop_daemon ( self . pid_file , kill_children = True ) fileio . rm_dir ( self . airflow_home ) logger . info ( \"Airflow spun down.\" ) prepare_pipeline_deployment ( self , pipeline , stack , runtime_configuration ) Checks whether airflow is running and copies the DAG file to the airflow DAGs directory. Exceptions: Type Description RuntimeError If airflow is not running or no DAG filepath runtime Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py def prepare_pipeline_deployment ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , runtime_configuration : \"RuntimeConfiguration\" , ) -> None : \"\"\"Checks whether airflow is running and copies the DAG file to the airflow DAGs directory. Raises: RuntimeError: If airflow is not running or no DAG filepath runtime option is provided. \"\"\" if not self . is_running : raise RuntimeError ( \"Airflow orchestrator is currently not running. \" \"Run `zenml orchestrator up` to start the \" \"orchestrator of the active stack.\" ) try : dag_filepath = runtime_configuration [ DAG_FILEPATH_OPTION_KEY ] except KeyError : raise RuntimeError ( f \"No DAG filepath found in runtime configuration. Make sure \" f \"to add the filepath to your airflow DAG file as a runtime \" f \"option (key: ' { DAG_FILEPATH_OPTION_KEY } ').\" ) self . _copy_to_dag_directory_if_necessary ( dag_filepath = dag_filepath ) provision ( self ) Ensures that Airflow is running. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py def provision ( self ) -> None : \"\"\"Ensures that Airflow is running.\"\"\" if self . is_running : logger . info ( \"Airflow is already running.\" ) self . _log_webserver_credentials () return if not fileio . file_exists ( self . dags_directory ): fileio . create_dir_recursive_if_not_exists ( self . dags_directory ) from airflow.cli.commands.standalone_command import StandaloneCommand try : command = StandaloneCommand () # Run the daemon with a working directory inside the current # zenml repo so the same repo will be used to run the DAGs daemon . run_as_daemon ( command . run , pid_file = self . pid_file , log_file = self . log_file , working_directory = zenml . io . utils . get_zenml_dir (), ) while not self . is_running : # Wait until the daemon started all the relevant airflow # processes time . sleep ( 0.1 ) self . _log_webserver_credentials () except Exception as e : logger . error ( e ) logger . error ( \"An error occurred while starting the Airflow daemon. If you \" \"want to start it manually, use the commands described in the \" \"official Airflow quickstart guide for running Airflow locally.\" ) self . down () run_pipeline ( self , pipeline , stack , run_name ) Schedules a pipeline to be run on Airflow. Returns: Type Description Any An Airflow DAG object that corresponds to the ZenML pipeline. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py def run_pipeline ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , run_name : str ) -> Any : \"\"\"Schedules a pipeline to be run on Airflow. Returns: An Airflow DAG object that corresponds to the ZenML pipeline. \"\"\" self . airflow_config = { \"schedule_interval\" : datetime . timedelta ( minutes = self . schedule_interval_minutes ), # We set this in the past and turn catchup off and then it works \"start_date\" : datetime . datetime ( 2019 , 1 , 1 ), } runner = AirflowDagRunner ( AirflowPipelineConfig ( self . airflow_config )) tfx_pipeline = create_tfx_pipeline ( pipeline , stack = stack ) return runner . run ( tfx_pipeline , run_name = run_name ) runtime_options ( self ) Runtime options for the airflow orchestrator. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py def runtime_options ( self ) -> Dict [ str , Any ]: \"\"\"Runtime options for the airflow orchestrator.\"\"\" return { DAG_FILEPATH_OPTION_KEY : None } set_airflow_home ( values ) classmethod Sets airflow home according to orchestrator UUID. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py @root_validator def set_airflow_home ( cls , values : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Sets airflow home according to orchestrator UUID.\"\"\" if \"uuid\" not in values : raise ValueError ( \"`uuid` needs to exist for AirflowOrchestrator.\" ) values [ \"airflow_home\" ] = os . path . join ( zenml . io . utils . get_global_config_directory (), AIRFLOW_ROOT_DIR , str ( values [ \"uuid\" ]), ) return values dash special DashIntegration ( Integration ) Definition of Dash integration for ZenML. Source code in zenml/integrations/dash/__init__.py class DashIntegration ( Integration ): \"\"\"Definition of Dash integration for ZenML.\"\"\" NAME = DASH REQUIREMENTS = [ \"dash>=2.0.0\" , \"dash-cytoscape>=0.3.0\" , \"dash-bootstrap-components>=1.0.1\" , ] visualizers special pipeline_run_lineage_visualizer PipelineRunLineageVisualizer ( BasePipelineRunVisualizer ) Implementation of a lineage diagram via the dash and dash-cyctoscape library. Source code in zenml/integrations/dash/visualizers/pipeline_run_lineage_visualizer.py class PipelineRunLineageVisualizer ( BasePipelineRunVisualizer ): \"\"\"Implementation of a lineage diagram via the [dash]( https://plotly.com/dash/) and [dash-cyctoscape]( https://dash.plotly.com/cytoscape) library.\"\"\" ARTIFACT_PREFIX = \"artifact_\" STEP_PREFIX = \"step_\" STATUS_CLASS_MAPPING = { ExecutionStatus . CACHED : \"green\" , ExecutionStatus . FAILED : \"red\" , ExecutionStatus . RUNNING : \"yellow\" , ExecutionStatus . COMPLETED : \"blue\" , } def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> dash . Dash : \"\"\"Method to visualize pipeline runs via the Dash library. The layout puts every layer of the dag in a column. \"\"\" app = dash . Dash ( __name__ , external_stylesheets = [ dbc . themes . BOOTSTRAP , dbc . icons . BOOTSTRAP , ], ) nodes , edges , first_step_id = [], [], None first_step_id = None for step in object . steps : step_output_artifacts = list ( step . outputs . values ()) execution_id = ( step_output_artifacts [ 0 ] . producer_step . id if step_output_artifacts else step . id ) step_id = self . STEP_PREFIX + str ( step . id ) if first_step_id is None : first_step_id = step_id nodes . append ( { \"data\" : { \"id\" : step_id , \"execution_id\" : execution_id , \"label\" : f \" { execution_id } / { step . entrypoint_name } \" , \"entrypoint_name\" : step . entrypoint_name , # redundant for consistency \"name\" : step . name , # redundant for consistency \"type\" : \"step\" , \"parameters\" : step . parameters , \"inputs\" : { k : v . uri for k , v in step . inputs . items ()}, \"outputs\" : { k : v . uri for k , v in step . outputs . items ()}, }, \"classes\" : self . STATUS_CLASS_MAPPING [ step . status ], } ) for artifact_name , artifact in step . outputs . items (): nodes . append ( { \"data\" : { \"id\" : self . ARTIFACT_PREFIX + str ( artifact . id ), \"execution_id\" : artifact . id , \"label\" : f \" { artifact . id } / { artifact_name } (\" f \" { artifact . data_type } )\" , \"type\" : \"artifact\" , \"name\" : artifact_name , \"is_cached\" : artifact . is_cached , \"artifact_type\" : artifact . type , \"artifact_data_type\" : artifact . data_type , \"parent_step_id\" : artifact . parent_step_id , \"producer_step_id\" : artifact . producer_step . id , \"uri\" : artifact . uri , }, \"classes\" : f \"rectangle \" f \" { self . STATUS_CLASS_MAPPING [ step . status ] } \" , } ) edges . append ( { \"data\" : { \"source\" : self . STEP_PREFIX + str ( step . id ), \"target\" : self . ARTIFACT_PREFIX + str ( artifact . id ), }, \"classes\" : f \"edge-arrow \" f \" { self . STATUS_CLASS_MAPPING [ step . status ] } \" + ( \" dashed\" if artifact . is_cached else \" solid\" ), } ) for artifact_name , artifact in step . inputs . items (): edges . append ( { \"data\" : { \"source\" : self . ARTIFACT_PREFIX + str ( artifact . id ), \"target\" : self . STEP_PREFIX + str ( step . id ), }, \"classes\" : \"edge-arrow \" + ( f \" { self . STATUS_CLASS_MAPPING [ ExecutionStatus . CACHED ] } dashed\" if artifact . is_cached else f \" { self . STATUS_CLASS_MAPPING [ step . status ] } solid\" ), } ) app . layout = dbc . Row ( [ dbc . Container ( f \"Run: { object . name } \" , class_name = \"h1\" ), dbc . Row ( [ dbc . Col ( [ dbc . Row ( [ html . Span ( [ html . Span ( [ html . I ( className = \"bi bi-circle-fill me-1\" ), \"Step\" , ], className = \"me-2\" , ), html . Span ( [ html . I ( className = \"bi bi-square-fill me-1\" ), \"Artifact\" , ], className = \"me-4\" , ), dbc . Badge ( \"Completed\" , color = COLOR_BLUE , className = \"me-1\" , ), dbc . Badge ( \"Cached\" , color = COLOR_GREEN , className = \"me-1\" , ), dbc . Badge ( \"Running\" , color = COLOR_YELLOW , className = \"me-1\" , ), dbc . Badge ( \"Failed\" , color = COLOR_RED , className = \"me-1\" , ), ] ), ] ), dbc . Row ( [ cyto . Cytoscape ( id = \"cytoscape\" , layout = { \"name\" : \"breadthfirst\" , \"roots\" : f '[id = \" { first_step_id } \"]' , }, elements = edges + nodes , stylesheet = STYLESHEET , style = { \"width\" : \"100%\" , \"height\" : \"800px\" , }, zoom = 1 , ) ] ), dbc . Row ( [ dbc . Button ( \"Reset\" , id = \"bt-reset\" , color = \"primary\" , className = \"me-1\" , ) ] ), ] ), dbc . Col ( [ dcc . Markdown ( id = \"markdown-selected-node-data\" ), ] ), ] ), ], className = \"p-5\" , ) @app . callback ( # type: ignore[misc] Output ( \"markdown-selected-node-data\" , \"children\" ), Input ( \"cytoscape\" , \"selectedNodeData\" ), ) def display_data ( data_list : List [ Dict [ str , Any ]]) -> str : \"\"\"Callback for the text area below the graph\"\"\" if data_list is None : return \"Click on a node in the diagram.\" text = \"\" for data in data_list : text += f '## { data [ \"execution_id\" ] } / { data [ \"name\" ] } ' + \" \\n\\n \" if data [ \"type\" ] == \"artifact\" : for item in [ \"artifact_data_type\" , \"is_cached\" , \"producer_step_id\" , \"parent_step_id\" , \"uri\" , ]: text += f \"** { item } **: { data [ item ] } \" + \" \\n\\n \" elif data [ \"type\" ] == \"step\" : text += \"### Inputs:\" + \" \\n\\n \" for k , v in data [ \"inputs\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" text += \"### Outputs:\" + \" \\n\\n \" for k , v in data [ \"outputs\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" text += \"### Params:\" for k , v in data [ \"parameters\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" return text @app . callback ( # type: ignore[misc] [ Output ( \"cytoscape\" , \"zoom\" ), Output ( \"cytoscape\" , \"elements\" )], [ Input ( \"bt-reset\" , \"n_clicks\" )], ) def reset_layout ( n_clicks : int , ) -> List [ Union [ int , List [ Dict [ str , Collection [ str ]]]]]: \"\"\"Resets the layout\"\"\" logger . debug ( n_clicks , \"clicked in reset button.\" ) return [ 1 , edges + nodes ] app . run_server () return app visualize ( self , object , * args , ** kwargs ) Method to visualize pipeline runs via the Dash library. The layout puts every layer of the dag in a column. Source code in zenml/integrations/dash/visualizers/pipeline_run_lineage_visualizer.py def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> dash . Dash : \"\"\"Method to visualize pipeline runs via the Dash library. The layout puts every layer of the dag in a column. \"\"\" app = dash . Dash ( __name__ , external_stylesheets = [ dbc . themes . BOOTSTRAP , dbc . icons . BOOTSTRAP , ], ) nodes , edges , first_step_id = [], [], None first_step_id = None for step in object . steps : step_output_artifacts = list ( step . outputs . values ()) execution_id = ( step_output_artifacts [ 0 ] . producer_step . id if step_output_artifacts else step . id ) step_id = self . STEP_PREFIX + str ( step . id ) if first_step_id is None : first_step_id = step_id nodes . append ( { \"data\" : { \"id\" : step_id , \"execution_id\" : execution_id , \"label\" : f \" { execution_id } / { step . entrypoint_name } \" , \"entrypoint_name\" : step . entrypoint_name , # redundant for consistency \"name\" : step . name , # redundant for consistency \"type\" : \"step\" , \"parameters\" : step . parameters , \"inputs\" : { k : v . uri for k , v in step . inputs . items ()}, \"outputs\" : { k : v . uri for k , v in step . outputs . items ()}, }, \"classes\" : self . STATUS_CLASS_MAPPING [ step . status ], } ) for artifact_name , artifact in step . outputs . items (): nodes . append ( { \"data\" : { \"id\" : self . ARTIFACT_PREFIX + str ( artifact . id ), \"execution_id\" : artifact . id , \"label\" : f \" { artifact . id } / { artifact_name } (\" f \" { artifact . data_type } )\" , \"type\" : \"artifact\" , \"name\" : artifact_name , \"is_cached\" : artifact . is_cached , \"artifact_type\" : artifact . type , \"artifact_data_type\" : artifact . data_type , \"parent_step_id\" : artifact . parent_step_id , \"producer_step_id\" : artifact . producer_step . id , \"uri\" : artifact . uri , }, \"classes\" : f \"rectangle \" f \" { self . STATUS_CLASS_MAPPING [ step . status ] } \" , } ) edges . append ( { \"data\" : { \"source\" : self . STEP_PREFIX + str ( step . id ), \"target\" : self . ARTIFACT_PREFIX + str ( artifact . id ), }, \"classes\" : f \"edge-arrow \" f \" { self . STATUS_CLASS_MAPPING [ step . status ] } \" + ( \" dashed\" if artifact . is_cached else \" solid\" ), } ) for artifact_name , artifact in step . inputs . items (): edges . append ( { \"data\" : { \"source\" : self . ARTIFACT_PREFIX + str ( artifact . id ), \"target\" : self . STEP_PREFIX + str ( step . id ), }, \"classes\" : \"edge-arrow \" + ( f \" { self . STATUS_CLASS_MAPPING [ ExecutionStatus . CACHED ] } dashed\" if artifact . is_cached else f \" { self . STATUS_CLASS_MAPPING [ step . status ] } solid\" ), } ) app . layout = dbc . Row ( [ dbc . Container ( f \"Run: { object . name } \" , class_name = \"h1\" ), dbc . Row ( [ dbc . Col ( [ dbc . Row ( [ html . Span ( [ html . Span ( [ html . I ( className = \"bi bi-circle-fill me-1\" ), \"Step\" , ], className = \"me-2\" , ), html . Span ( [ html . I ( className = \"bi bi-square-fill me-1\" ), \"Artifact\" , ], className = \"me-4\" , ), dbc . Badge ( \"Completed\" , color = COLOR_BLUE , className = \"me-1\" , ), dbc . Badge ( \"Cached\" , color = COLOR_GREEN , className = \"me-1\" , ), dbc . Badge ( \"Running\" , color = COLOR_YELLOW , className = \"me-1\" , ), dbc . Badge ( \"Failed\" , color = COLOR_RED , className = \"me-1\" , ), ] ), ] ), dbc . Row ( [ cyto . Cytoscape ( id = \"cytoscape\" , layout = { \"name\" : \"breadthfirst\" , \"roots\" : f '[id = \" { first_step_id } \"]' , }, elements = edges + nodes , stylesheet = STYLESHEET , style = { \"width\" : \"100%\" , \"height\" : \"800px\" , }, zoom = 1 , ) ] ), dbc . Row ( [ dbc . Button ( \"Reset\" , id = \"bt-reset\" , color = \"primary\" , className = \"me-1\" , ) ] ), ] ), dbc . Col ( [ dcc . Markdown ( id = \"markdown-selected-node-data\" ), ] ), ] ), ], className = \"p-5\" , ) @app . callback ( # type: ignore[misc] Output ( \"markdown-selected-node-data\" , \"children\" ), Input ( \"cytoscape\" , \"selectedNodeData\" ), ) def display_data ( data_list : List [ Dict [ str , Any ]]) -> str : \"\"\"Callback for the text area below the graph\"\"\" if data_list is None : return \"Click on a node in the diagram.\" text = \"\" for data in data_list : text += f '## { data [ \"execution_id\" ] } / { data [ \"name\" ] } ' + \" \\n\\n \" if data [ \"type\" ] == \"artifact\" : for item in [ \"artifact_data_type\" , \"is_cached\" , \"producer_step_id\" , \"parent_step_id\" , \"uri\" , ]: text += f \"** { item } **: { data [ item ] } \" + \" \\n\\n \" elif data [ \"type\" ] == \"step\" : text += \"### Inputs:\" + \" \\n\\n \" for k , v in data [ \"inputs\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" text += \"### Outputs:\" + \" \\n\\n \" for k , v in data [ \"outputs\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" text += \"### Params:\" for k , v in data [ \"parameters\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" return text @app . callback ( # type: ignore[misc] [ Output ( \"cytoscape\" , \"zoom\" ), Output ( \"cytoscape\" , \"elements\" )], [ Input ( \"bt-reset\" , \"n_clicks\" )], ) def reset_layout ( n_clicks : int , ) -> List [ Union [ int , List [ Dict [ str , Collection [ str ]]]]]: \"\"\"Resets the layout\"\"\" logger . debug ( n_clicks , \"clicked in reset button.\" ) return [ 1 , edges + nodes ] app . run_server () return app evidently special The Evidently integration provides a way to monitor your models in production. It includes a way to detect data drift and different kinds of model performance issues. The results of Evidently calculations can either be exported as an interactive dashboard (visualized as an html file or in your Jupyter notebook), or as a JSON file. EvidentlyIntegration ( Integration ) Definition of Evidently integration for ZenML. Source code in zenml/integrations/evidently/__init__.py class EvidentlyIntegration ( Integration ): \"\"\"Definition of [Evidently](https://github.com/evidentlyai/evidently) integration for ZenML.\"\"\" NAME = EVIDENTLY REQUIREMENTS = [ \"evidently==v0.1.40.dev0\" ] steps special evidently_profile EvidentlyProfileConfig ( BaseDriftDetectionConfig ) pydantic-model Config class for Evidently profile steps. column_mapping: properties of the dataframe's columns used !!! profile_section \"a string that identifies the profile section to be used.\" The following are valid options supported by Evidently: - \"datadrift\" - \"categoricaltargetdrift\" - \"numericaltargetdrift\" - \"classificationmodelperformance\" - \"regressionmodelperformance\" - \"probabilisticmodelperformance\" Source code in zenml/integrations/evidently/steps/evidently_profile.py class EvidentlyProfileConfig ( BaseDriftDetectionConfig ): \"\"\"Config class for Evidently profile steps. column_mapping: properties of the dataframe's columns used profile_section: a string that identifies the profile section to be used. The following are valid options supported by Evidently: - \"datadrift\" - \"categoricaltargetdrift\" - \"numericaltargetdrift\" - \"classificationmodelperformance\" - \"regressionmodelperformance\" - \"probabilisticmodelperformance\" \"\"\" def get_profile_sections_and_tabs ( self , ) -> Tuple [ List [ ProfileSection ], List [ Tab ]]: try : return ( [ profile_mapper [ profile ]() for profile in self . profile_sections ], [ dashboard_mapper [ profile ]() for profile in self . profile_sections ], ) except KeyError : nl = \" \\n \" raise ValueError ( f \"Invalid profile section: { self . profile_sections } \\n\\n \" f \"Valid and supported options are: { nl } - \" f ' { f \" { nl } - \" . join ( list ( profile_mapper . keys ())) } ' ) column_mapping : Optional [ ColumnMapping ] profile_sections : Sequence [ str ] EvidentlyProfileStep ( BaseDriftDetectionStep ) Simple step implementation which implements Evidently's functionality for creating a profile. Source code in zenml/integrations/evidently/steps/evidently_profile.py class EvidentlyProfileStep ( BaseDriftDetectionStep ): \"\"\"Simple step implementation which implements Evidently's functionality for creating a profile.\"\"\" OUTPUT_SPEC = { \"profile\" : DataAnalysisArtifact , \"dashboard\" : DataAnalysisArtifact , } def entrypoint ( # type: ignore[override] self , reference_dataset : DataArtifact , comparison_dataset : DataArtifact , config : EvidentlyProfileConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] profile = dict , dashboard = str ): \"\"\"Main entrypoint for the Evidently categorical target drift detection step. Args: reference_dataset: a Pandas dataframe comparison_dataset: a Pandas dataframe of new data you wish to compare against the reference data config: the configuration for the step context: the context of the step Returns: profile: dictionary report extracted from an Evidently Profile generated for the data drift dashboard: HTML report extracted from an Evidently Dashboard generated for the data drift \"\"\" sections , tabs = config . get_profile_sections_and_tabs () data_drift_dashboard = Dashboard ( tabs = tabs ) data_drift_dashboard . calculate ( reference_dataset , comparison_dataset , column_mapping = config . column_mapping or None , ) data_drift_profile = Profile ( sections = sections ) data_drift_profile . calculate ( reference_dataset , comparison_dataset , column_mapping = config . column_mapping or None , ) return [ data_drift_profile . object (), data_drift_dashboard . html ()] CONFIG_CLASS ( BaseDriftDetectionConfig ) pydantic-model Config class for Evidently profile steps. column_mapping: properties of the dataframe's columns used !!! profile_section \"a string that identifies the profile section to be used.\" The following are valid options supported by Evidently: - \"datadrift\" - \"categoricaltargetdrift\" - \"numericaltargetdrift\" - \"classificationmodelperformance\" - \"regressionmodelperformance\" - \"probabilisticmodelperformance\" Source code in zenml/integrations/evidently/steps/evidently_profile.py class EvidentlyProfileConfig ( BaseDriftDetectionConfig ): \"\"\"Config class for Evidently profile steps. column_mapping: properties of the dataframe's columns used profile_section: a string that identifies the profile section to be used. The following are valid options supported by Evidently: - \"datadrift\" - \"categoricaltargetdrift\" - \"numericaltargetdrift\" - \"classificationmodelperformance\" - \"regressionmodelperformance\" - \"probabilisticmodelperformance\" \"\"\" def get_profile_sections_and_tabs ( self , ) -> Tuple [ List [ ProfileSection ], List [ Tab ]]: try : return ( [ profile_mapper [ profile ]() for profile in self . profile_sections ], [ dashboard_mapper [ profile ]() for profile in self . profile_sections ], ) except KeyError : nl = \" \\n \" raise ValueError ( f \"Invalid profile section: { self . profile_sections } \\n\\n \" f \"Valid and supported options are: { nl } - \" f ' { f \" { nl } - \" . join ( list ( profile_mapper . keys ())) } ' ) column_mapping : Optional [ ColumnMapping ] profile_sections : Sequence [ str ] entrypoint ( self , reference_dataset , comparison_dataset , config , context ) Main entrypoint for the Evidently categorical target drift detection step. Parameters: Name Type Description Default reference_dataset DataArtifact a Pandas dataframe required comparison_dataset DataArtifact a Pandas dataframe of new data you wish to compare against the reference data required config EvidentlyProfileConfig the configuration for the step required context StepContext the context of the step required Returns: Type Description profile dictionary report extracted from an Evidently Profile generated for the data drift dashboard: HTML report extracted from an Evidently Dashboard generated for the data drift Source code in zenml/integrations/evidently/steps/evidently_profile.py def entrypoint ( # type: ignore[override] self , reference_dataset : DataArtifact , comparison_dataset : DataArtifact , config : EvidentlyProfileConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] profile = dict , dashboard = str ): \"\"\"Main entrypoint for the Evidently categorical target drift detection step. Args: reference_dataset: a Pandas dataframe comparison_dataset: a Pandas dataframe of new data you wish to compare against the reference data config: the configuration for the step context: the context of the step Returns: profile: dictionary report extracted from an Evidently Profile generated for the data drift dashboard: HTML report extracted from an Evidently Dashboard generated for the data drift \"\"\" sections , tabs = config . get_profile_sections_and_tabs () data_drift_dashboard = Dashboard ( tabs = tabs ) data_drift_dashboard . calculate ( reference_dataset , comparison_dataset , column_mapping = config . column_mapping or None , ) data_drift_profile = Profile ( sections = sections ) data_drift_profile . calculate ( reference_dataset , comparison_dataset , column_mapping = config . column_mapping or None , ) return [ data_drift_profile . object (), data_drift_dashboard . html ()] visualizers special evidently_visualizer EvidentlyVisualizer ( BaseStepVisualizer ) The implementation of an Evidently Visualizer. Source code in zenml/integrations/evidently/visualizers/evidently_visualizer.py class EvidentlyVisualizer ( BaseStepVisualizer ): \"\"\"The implementation of an Evidently Visualizer.\"\"\" @abstractmethod def visualize ( self , object : StepView , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize components Args: object: StepView fetched from run.get_step(). \"\"\" for artifact_view in object . outputs . values (): # filter out anything but data analysis artifacts if ( artifact_view . type == DataAnalysisArtifact . __name__ and artifact_view . data_type == \"builtins.str\" ): artifact = artifact_view . read () self . generate_facet ( artifact ) def generate_facet ( self , html_ : str ) -> None : \"\"\"Generate a Facet Overview Args: h: HTML represented as a string. \"\"\" if self . running_in_notebook (): from IPython.core.display import HTML , display display ( HTML ( html_ )) else : logger . warn ( \"The magic functions are only usable in a Jupyter notebook.\" ) with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : zenml . io . utils . write_file_contents_as_string ( f . name , html_ ) url = f \"file:/// { f . name } \" logger . info ( \"Opening %s in a new browser..\" % f . name ) webbrowser . open ( url , new = 2 ) generate_facet ( self , html_ ) Generate a Facet Overview Parameters: Name Type Description Default h HTML represented as a string. required Source code in zenml/integrations/evidently/visualizers/evidently_visualizer.py def generate_facet ( self , html_ : str ) -> None : \"\"\"Generate a Facet Overview Args: h: HTML represented as a string. \"\"\" if self . running_in_notebook (): from IPython.core.display import HTML , display display ( HTML ( html_ )) else : logger . warn ( \"The magic functions are only usable in a Jupyter notebook.\" ) with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : zenml . io . utils . write_file_contents_as_string ( f . name , html_ ) url = f \"file:/// { f . name } \" logger . info ( \"Opening %s in a new browser..\" % f . name ) webbrowser . open ( url , new = 2 ) visualize ( self , object , * args , ** kwargs ) Method to visualize components Parameters: Name Type Description Default object StepView StepView fetched from run.get_step(). required Source code in zenml/integrations/evidently/visualizers/evidently_visualizer.py @abstractmethod def visualize ( self , object : StepView , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize components Args: object: StepView fetched from run.get_step(). \"\"\" for artifact_view in object . outputs . values (): # filter out anything but data analysis artifacts if ( artifact_view . type == DataAnalysisArtifact . __name__ and artifact_view . data_type == \"builtins.str\" ): artifact = artifact_view . read () self . generate_facet ( artifact ) facets special The Facets integration provides a simple way to visualize post-execution objects like PipelineView , PipelineRunView and StepView . These objects can be extended using the BaseVisualization class. This integration requires facets-overview be installed in your Python environment. FacetsIntegration ( Integration ) Definition of Facet integration for ZenML. Source code in zenml/integrations/facets/__init__.py class FacetsIntegration ( Integration ): \"\"\"Definition of [Facet](https://pair-code.github.io/facets/) integration for ZenML.\"\"\" NAME = FACETS REQUIREMENTS = [ \"facets-overview>=1.0.0\" , \"IPython\" ] visualizers special facet_statistics_visualizer FacetStatisticsVisualizer ( BaseStepVisualizer ) The base implementation of a ZenML Visualizer. Source code in zenml/integrations/facets/visualizers/facet_statistics_visualizer.py class FacetStatisticsVisualizer ( BaseStepVisualizer ): \"\"\"The base implementation of a ZenML Visualizer.\"\"\" @abstractmethod def visualize ( self , object : StepView , magic : bool = False , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize components Args: object: StepView fetched from run.get_step(). magic: Whether to render in a Jupyter notebook or not. \"\"\" datasets = [] for output_name , artifact_view in object . outputs . items (): df = artifact_view . read () if type ( df ) is not pd . DataFrame : logger . warning ( \"` %s ` is not a pd.DataFrame. You can only visualize \" \"statistics of steps that output pandas dataframes. \" \"Skipping this output..\" % output_name ) else : datasets . append ({ \"name\" : output_name , \"table\" : df }) h = self . generate_html ( datasets ) self . generate_facet ( h , magic ) def generate_html ( self , datasets : List [ Dict [ Text , pd . DataFrame ]]) -> str : \"\"\"Generates html for facet. Args: datasets: List of dicts of dataframes to be visualized as stats. Returns: HTML template with proto string embedded. \"\"\" proto = GenericFeatureStatisticsGenerator () . ProtoFromDataFrames ( datasets ) protostr = base64 . b64encode ( proto . SerializeToString ()) . decode ( \"utf-8\" ) template = os . path . join ( os . path . abspath ( os . path . dirname ( __file__ )), \"stats.html\" , ) html_template = zenml . io . utils . read_file_contents_as_string ( template ) html_ = html_template . replace ( \"protostr\" , protostr ) return html_ def generate_facet ( self , html_ : str , magic : bool = False ) -> None : \"\"\"Generate a Facet Overview Args: h: HTML represented as a string. magic: Whether to magically materialize facet in a notebook. \"\"\" if magic : if not self . running_in_notebook (): raise EnvironmentError ( \"The magic functions are only usable in a Jupyter notebook.\" ) display ( HTML ( html_ )) else : with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : zenml . io . utils . write_file_contents_as_string ( f . name , html_ ) url = f \"file:/// { f . name } \" logger . info ( \"Opening %s in a new browser..\" % f . name ) webbrowser . open ( url , new = 2 ) generate_facet ( self , html_ , magic = False ) Generate a Facet Overview Parameters: Name Type Description Default h HTML represented as a string. required magic bool Whether to magically materialize facet in a notebook. False Source code in zenml/integrations/facets/visualizers/facet_statistics_visualizer.py def generate_facet ( self , html_ : str , magic : bool = False ) -> None : \"\"\"Generate a Facet Overview Args: h: HTML represented as a string. magic: Whether to magically materialize facet in a notebook. \"\"\" if magic : if not self . running_in_notebook (): raise EnvironmentError ( \"The magic functions are only usable in a Jupyter notebook.\" ) display ( HTML ( html_ )) else : with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : zenml . io . utils . write_file_contents_as_string ( f . name , html_ ) url = f \"file:/// { f . name } \" logger . info ( \"Opening %s in a new browser..\" % f . name ) webbrowser . open ( url , new = 2 ) generate_html ( self , datasets ) Generates html for facet. Parameters: Name Type Description Default datasets List[Dict[str, pandas.core.frame.DataFrame]] List of dicts of dataframes to be visualized as stats. required Returns: Type Description str HTML template with proto string embedded. Source code in zenml/integrations/facets/visualizers/facet_statistics_visualizer.py def generate_html ( self , datasets : List [ Dict [ Text , pd . DataFrame ]]) -> str : \"\"\"Generates html for facet. Args: datasets: List of dicts of dataframes to be visualized as stats. Returns: HTML template with proto string embedded. \"\"\" proto = GenericFeatureStatisticsGenerator () . ProtoFromDataFrames ( datasets ) protostr = base64 . b64encode ( proto . SerializeToString ()) . decode ( \"utf-8\" ) template = os . path . join ( os . path . abspath ( os . path . dirname ( __file__ )), \"stats.html\" , ) html_template = zenml . io . utils . read_file_contents_as_string ( template ) html_ = html_template . replace ( \"protostr\" , protostr ) return html_ visualize ( self , object , magic = False , * args , ** kwargs ) Method to visualize components Parameters: Name Type Description Default object StepView StepView fetched from run.get_step(). required magic bool Whether to render in a Jupyter notebook or not. False Source code in zenml/integrations/facets/visualizers/facet_statistics_visualizer.py @abstractmethod def visualize ( self , object : StepView , magic : bool = False , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize components Args: object: StepView fetched from run.get_step(). magic: Whether to render in a Jupyter notebook or not. \"\"\" datasets = [] for output_name , artifact_view in object . outputs . items (): df = artifact_view . read () if type ( df ) is not pd . DataFrame : logger . warning ( \"` %s ` is not a pd.DataFrame. You can only visualize \" \"statistics of steps that output pandas dataframes. \" \"Skipping this output..\" % output_name ) else : datasets . append ({ \"name\" : output_name , \"table\" : df }) h = self . generate_html ( datasets ) self . generate_facet ( h , magic ) gcp special The GCP integration submodule provides a way to run ZenML pipelines in a cloud environment. Specifically, it allows the use of cloud artifact stores, metadata stores, and an io module to handle file operations on Google Cloud Storage (GCS). GcpIntegration ( Integration ) Definition of Google Cloud Platform integration for ZenML. Source code in zenml/integrations/gcp/__init__.py class GcpIntegration ( Integration ): \"\"\"Definition of Google Cloud Platform integration for ZenML.\"\"\" NAME = GCP REQUIREMENTS = [ \"gcsfs\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.gcp import artifact_stores # noqa from zenml.integrations.gcp import io # noqa activate () classmethod Activates the integration. Source code in zenml/integrations/gcp/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.gcp import artifact_stores # noqa from zenml.integrations.gcp import io # noqa artifact_stores special gcp_artifact_store GCPArtifactStore ( BaseArtifactStore ) pydantic-model Artifact Store for Google Cloud Storage based artifacts. Source code in zenml/integrations/gcp/artifact_stores/gcp_artifact_store.py class GCPArtifactStore ( BaseArtifactStore ): \"\"\"Artifact Store for Google Cloud Storage based artifacts.\"\"\" supports_local_execution = True supports_remote_execution = True @property def flavor ( self ) -> ArtifactStoreFlavor : \"\"\"The artifact store flavor.\"\"\" return ArtifactStoreFlavor . GCP @validator ( \"path\" ) def ensure_gcs_path ( cls , path : str ) -> str : \"\"\"Ensures that the path is a valid gcs path.\"\"\" if not path . startswith ( \"gs://\" ): raise ValueError ( f \"Path ' { path } ' specified for GCPArtifactStore is not a \" f \"valid gcs path, i.e., starting with `gs://`.\" ) return path flavor : ArtifactStoreFlavor property readonly The artifact store flavor. ensure_gcs_path ( path ) classmethod Ensures that the path is a valid gcs path. Source code in zenml/integrations/gcp/artifact_stores/gcp_artifact_store.py @validator ( \"path\" ) def ensure_gcs_path ( cls , path : str ) -> str : \"\"\"Ensures that the path is a valid gcs path.\"\"\" if not path . startswith ( \"gs://\" ): raise ValueError ( f \"Path ' { path } ' specified for GCPArtifactStore is not a \" f \"valid gcs path, i.e., starting with `gs://`.\" ) return path io special gcs_plugin Plugin which is created to add Google Cloud Store support to ZenML. It inherits from the base Filesystem created by TFX and overwrites the corresponding functions thanks to gcsfs. ZenGCS ( Filesystem ) Filesystem that delegates to Google Cloud Store using gcsfs. Note : To allow TFX to check for various error conditions, we need to raise their custom NotFoundError instead of the builtin python FileNotFoundError. Source code in zenml/integrations/gcp/io/gcs_plugin.py class ZenGCS ( Filesystem ): \"\"\"Filesystem that delegates to Google Cloud Store using gcsfs. **Note**: To allow TFX to check for various error conditions, we need to raise their custom `NotFoundError` instead of the builtin python FileNotFoundError.\"\"\" SUPPORTED_SCHEMES = [ \"gs://\" ] fs : gcsfs . GCSFileSystem = None @classmethod def _ensure_filesystem_set ( cls ) -> None : \"\"\"Ensures that the filesystem is set.\"\"\" if ZenGCS . fs is None : ZenGCS . fs = gcsfs . GCSFileSystem () @staticmethod def open ( path : PathType , mode : str = \"r\" ) -> Any : \"\"\"Open a file at the given path. Args: path: Path of the file to open. mode: Mode in which to open the file. Currently only 'rb' and 'wb' to read and write binary files are supported. \"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . open ( path = path , mode = mode ) except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def copy ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Copy a file. Args: src: The path to copy from. dst: The path to copy to. overwrite: If a file already exists at the destination, this method will overwrite it if overwrite=`True` and raise a FileExistsError otherwise. Raises: FileNotFoundError: If the source file does not exist. FileExistsError: If a file already exists at the destination and overwrite is not set to `True`. \"\"\" ZenGCS . _ensure_filesystem_set () if not overwrite and ZenGCS . fs . exists ( dst ): raise FileExistsError ( f \"Unable to copy to destination ' { convert_to_str ( dst ) } ', \" f \"file already exists. Set `overwrite=True` to copy anyway.\" ) # TODO [ENG-151]: Check if it works with overwrite=True or if we need to # manually remove it first try : ZenGCS . fs . copy ( path1 = src , path2 = dst ) except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def exists ( path : PathType ) -> bool : \"\"\"Check whether a path exists.\"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . exists ( path = path ) # type: ignore[no-any-return] @staticmethod def glob ( pattern : PathType ) -> List [ PathType ]: \"\"\"Return all paths that match the given glob pattern. The glob pattern may include: - '*' to match any number of characters - '?' to match a single character - '[...]' to match one of the characters inside the brackets - '**' as the full name of a path component to match to search in subdirectories of any depth (e.g. '/some_dir/**/some_file) Args: pattern: The glob pattern to match, see details above. Returns: A list of paths that match the given glob pattern. \"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . glob ( path = pattern ) # type: ignore[no-any-return] @staticmethod def isdir ( path : PathType ) -> bool : \"\"\"Check whether a path is a directory.\"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . isdir ( path = path ) # type: ignore[no-any-return] @staticmethod def listdir ( path : PathType ) -> List [ PathType ]: \"\"\"Return a list of files in a directory.\"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . listdir ( path = path ) # type: ignore[no-any-return] except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def makedirs ( path : PathType ) -> None : \"\"\"Create a directory at the given path. If needed also create missing parent directories.\"\"\" ZenGCS . _ensure_filesystem_set () ZenGCS . fs . makedirs ( path = path , exist_ok = True ) @staticmethod def mkdir ( path : PathType ) -> None : \"\"\"Create a directory at the given path.\"\"\" ZenGCS . _ensure_filesystem_set () ZenGCS . fs . makedir ( path = path ) @staticmethod def remove ( path : PathType ) -> None : \"\"\"Remove the file at the given path.\"\"\" ZenGCS . _ensure_filesystem_set () try : ZenGCS . fs . rm_file ( path = path ) except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def rename ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Rename source file to destination file. Args: src: The path of the file to rename. dst: The path to rename the source file to. overwrite: If a file already exists at the destination, this method will overwrite it if overwrite=`True` and raise a FileExistsError otherwise. Raises: FileNotFoundError: If the source file does not exist. FileExistsError: If a file already exists at the destination and overwrite is not set to `True`. \"\"\" ZenGCS . _ensure_filesystem_set () if not overwrite and ZenGCS . fs . exists ( dst ): raise FileExistsError ( f \"Unable to rename file to ' { convert_to_str ( dst ) } ', \" f \"file already exists. Set `overwrite=True` to rename anyway.\" ) # TODO [ENG-152]: Check if it works with overwrite=True or if we need # to manually remove it first try : ZenGCS . fs . rename ( path1 = src , path2 = dst ) except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def rmtree ( path : PathType ) -> None : \"\"\"Remove the given directory.\"\"\" ZenGCS . _ensure_filesystem_set () try : ZenGCS . fs . delete ( path = path , recursive = True ) except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def stat ( path : PathType ) -> Dict [ str , Any ]: \"\"\"Return stat info for the given path.\"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . stat ( path = path ) # type: ignore[no-any-return] except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def walk ( top : PathType , topdown : bool = True , onerror : Optional [ Callable [ ... , None ]] = None , ) -> Iterable [ Tuple [ PathType , List [ PathType ], List [ PathType ]]]: \"\"\"Return an iterator that walks the contents of the given directory. Args: top: Path of directory to walk. topdown: Unused argument to conform to interface. onerror: Unused argument to conform to interface. Returns: An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. \"\"\" ZenGCS . _ensure_filesystem_set () # TODO [ENG-153]: Additional params return ZenGCS . fs . walk ( path = top ) # type: ignore[no-any-return] copy ( src , dst , overwrite = False ) staticmethod Copy a file. Parameters: Name Type Description Default src Union[bytes, str] The path to copy from. required dst Union[bytes, str] The path to copy to. required overwrite bool If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. False Exceptions: Type Description FileNotFoundError If the source file does not exist. FileExistsError If a file already exists at the destination and overwrite is not set to True . Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def copy ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Copy a file. Args: src: The path to copy from. dst: The path to copy to. overwrite: If a file already exists at the destination, this method will overwrite it if overwrite=`True` and raise a FileExistsError otherwise. Raises: FileNotFoundError: If the source file does not exist. FileExistsError: If a file already exists at the destination and overwrite is not set to `True`. \"\"\" ZenGCS . _ensure_filesystem_set () if not overwrite and ZenGCS . fs . exists ( dst ): raise FileExistsError ( f \"Unable to copy to destination ' { convert_to_str ( dst ) } ', \" f \"file already exists. Set `overwrite=True` to copy anyway.\" ) # TODO [ENG-151]: Check if it works with overwrite=True or if we need to # manually remove it first try : ZenGCS . fs . copy ( path1 = src , path2 = dst ) except FileNotFoundError as e : raise NotFoundError () from e exists ( path ) staticmethod Check whether a path exists. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def exists ( path : PathType ) -> bool : \"\"\"Check whether a path exists.\"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . exists ( path = path ) # type: ignore[no-any-return] glob ( pattern ) staticmethod Return all paths that match the given glob pattern. The glob pattern may include: - ' ' to match any number of characters - '?' to match a single character - '[...]' to match one of the characters inside the brackets - ' ' as the full name of a path component to match to search in subdirectories of any depth (e.g. '/some_dir/ */some_file) Parameters: Name Type Description Default pattern Union[bytes, str] The glob pattern to match, see details above. required Returns: Type Description List[Union[bytes, str]] A list of paths that match the given glob pattern. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def glob ( pattern : PathType ) -> List [ PathType ]: \"\"\"Return all paths that match the given glob pattern. The glob pattern may include: - '*' to match any number of characters - '?' to match a single character - '[...]' to match one of the characters inside the brackets - '**' as the full name of a path component to match to search in subdirectories of any depth (e.g. '/some_dir/**/some_file) Args: pattern: The glob pattern to match, see details above. Returns: A list of paths that match the given glob pattern. \"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . glob ( path = pattern ) # type: ignore[no-any-return] isdir ( path ) staticmethod Check whether a path is a directory. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def isdir ( path : PathType ) -> bool : \"\"\"Check whether a path is a directory.\"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . isdir ( path = path ) # type: ignore[no-any-return] listdir ( path ) staticmethod Return a list of files in a directory. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def listdir ( path : PathType ) -> List [ PathType ]: \"\"\"Return a list of files in a directory.\"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . listdir ( path = path ) # type: ignore[no-any-return] except FileNotFoundError as e : raise NotFoundError () from e makedirs ( path ) staticmethod Create a directory at the given path. If needed also create missing parent directories. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def makedirs ( path : PathType ) -> None : \"\"\"Create a directory at the given path. If needed also create missing parent directories.\"\"\" ZenGCS . _ensure_filesystem_set () ZenGCS . fs . makedirs ( path = path , exist_ok = True ) mkdir ( path ) staticmethod Create a directory at the given path. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def mkdir ( path : PathType ) -> None : \"\"\"Create a directory at the given path.\"\"\" ZenGCS . _ensure_filesystem_set () ZenGCS . fs . makedir ( path = path ) open ( path , mode = 'r' ) staticmethod Open a file at the given path. Parameters: Name Type Description Default path Union[bytes, str] Path of the file to open. required mode str Mode in which to open the file. Currently only 'rb' and 'wb' to read and write binary files are supported. 'r' Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def open ( path : PathType , mode : str = \"r\" ) -> Any : \"\"\"Open a file at the given path. Args: path: Path of the file to open. mode: Mode in which to open the file. Currently only 'rb' and 'wb' to read and write binary files are supported. \"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . open ( path = path , mode = mode ) except FileNotFoundError as e : raise NotFoundError () from e remove ( path ) staticmethod Remove the file at the given path. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def remove ( path : PathType ) -> None : \"\"\"Remove the file at the given path.\"\"\" ZenGCS . _ensure_filesystem_set () try : ZenGCS . fs . rm_file ( path = path ) except FileNotFoundError as e : raise NotFoundError () from e rename ( src , dst , overwrite = False ) staticmethod Rename source file to destination file. Parameters: Name Type Description Default src Union[bytes, str] The path of the file to rename. required dst Union[bytes, str] The path to rename the source file to. required overwrite bool If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. False Exceptions: Type Description FileNotFoundError If the source file does not exist. FileExistsError If a file already exists at the destination and overwrite is not set to True . Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def rename ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Rename source file to destination file. Args: src: The path of the file to rename. dst: The path to rename the source file to. overwrite: If a file already exists at the destination, this method will overwrite it if overwrite=`True` and raise a FileExistsError otherwise. Raises: FileNotFoundError: If the source file does not exist. FileExistsError: If a file already exists at the destination and overwrite is not set to `True`. \"\"\" ZenGCS . _ensure_filesystem_set () if not overwrite and ZenGCS . fs . exists ( dst ): raise FileExistsError ( f \"Unable to rename file to ' { convert_to_str ( dst ) } ', \" f \"file already exists. Set `overwrite=True` to rename anyway.\" ) # TODO [ENG-152]: Check if it works with overwrite=True or if we need # to manually remove it first try : ZenGCS . fs . rename ( path1 = src , path2 = dst ) except FileNotFoundError as e : raise NotFoundError () from e rmtree ( path ) staticmethod Remove the given directory. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def rmtree ( path : PathType ) -> None : \"\"\"Remove the given directory.\"\"\" ZenGCS . _ensure_filesystem_set () try : ZenGCS . fs . delete ( path = path , recursive = True ) except FileNotFoundError as e : raise NotFoundError () from e stat ( path ) staticmethod Return stat info for the given path. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def stat ( path : PathType ) -> Dict [ str , Any ]: \"\"\"Return stat info for the given path.\"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . stat ( path = path ) # type: ignore[no-any-return] except FileNotFoundError as e : raise NotFoundError () from e walk ( top , topdown = True , onerror = None ) staticmethod Return an iterator that walks the contents of the given directory. Parameters: Name Type Description Default top Union[bytes, str] Path of directory to walk. required topdown bool Unused argument to conform to interface. True onerror Optional[Callable[..., NoneType]] Unused argument to conform to interface. None Returns: Type Description Iterable[Tuple[Union[bytes, str], List[Union[bytes, str]], List[Union[bytes, str]]]] An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def walk ( top : PathType , topdown : bool = True , onerror : Optional [ Callable [ ... , None ]] = None , ) -> Iterable [ Tuple [ PathType , List [ PathType ], List [ PathType ]]]: \"\"\"Return an iterator that walks the contents of the given directory. Args: top: Path of directory to walk. topdown: Unused argument to conform to interface. onerror: Unused argument to conform to interface. Returns: An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. \"\"\" ZenGCS . _ensure_filesystem_set () # TODO [ENG-153]: Additional params return ZenGCS . fs . walk ( path = top ) # type: ignore[no-any-return] graphviz special GraphvizIntegration ( Integration ) Definition of Graphviz integration for ZenML. Source code in zenml/integrations/graphviz/__init__.py class GraphvizIntegration ( Integration ): \"\"\"Definition of Graphviz integration for ZenML.\"\"\" NAME = GRAPHVIZ REQUIREMENTS = [ \"graphviz>=0.17\" ] SYSTEM_REQUIREMENTS = { \"graphviz\" : \"dot\" } visualizers special pipeline_run_dag_visualizer PipelineRunDagVisualizer ( BasePipelineRunVisualizer ) Visualize the lineage of runs in a pipeline. Source code in zenml/integrations/graphviz/visualizers/pipeline_run_dag_visualizer.py class PipelineRunDagVisualizer ( BasePipelineRunVisualizer ): \"\"\"Visualize the lineage of runs in a pipeline.\"\"\" ARTIFACT_DEFAULT_COLOR = \"blue\" ARTIFACT_CACHED_COLOR = \"green\" ARTIFACT_SHAPE = \"box\" ARTIFACT_PREFIX = \"artifact_\" STEP_COLOR = \"#431D93\" STEP_SHAPE = \"ellipse\" STEP_PREFIX = \"step_\" FONT = \"Roboto\" @abstractmethod def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> graphviz . Digraph : \"\"\"Creates a pipeline lineage diagram using graphviz.\"\"\" logger . warning ( \"This integration is not completed yet. Results might be unexpected.\" ) dot = graphviz . Digraph ( comment = object . name ) # link the steps together for step in object . steps : # add each step as a node dot . node ( self . STEP_PREFIX + str ( step . id ), step . entrypoint_name , shape = self . STEP_SHAPE , ) # for each parent of a step, add an edge for artifact_name , artifact in step . outputs . items (): dot . node ( self . ARTIFACT_PREFIX + str ( artifact . id ), f \" { artifact_name } \\n \" f \"( { artifact . _data_type } )\" , shape = self . ARTIFACT_SHAPE , ) dot . edge ( self . STEP_PREFIX + str ( step . id ), self . ARTIFACT_PREFIX + str ( artifact . id ), ) for artifact_name , artifact in step . inputs . items (): dot . edge ( self . ARTIFACT_PREFIX + str ( artifact . id ), self . STEP_PREFIX + str ( step . id ), ) with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : dot . render ( filename = f . name , format = \"png\" , view = True , cleanup = True ) return dot visualize ( self , object , * args , ** kwargs ) Creates a pipeline lineage diagram using graphviz. Source code in zenml/integrations/graphviz/visualizers/pipeline_run_dag_visualizer.py @abstractmethod def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> graphviz . Digraph : \"\"\"Creates a pipeline lineage diagram using graphviz.\"\"\" logger . warning ( \"This integration is not completed yet. Results might be unexpected.\" ) dot = graphviz . Digraph ( comment = object . name ) # link the steps together for step in object . steps : # add each step as a node dot . node ( self . STEP_PREFIX + str ( step . id ), step . entrypoint_name , shape = self . STEP_SHAPE , ) # for each parent of a step, add an edge for artifact_name , artifact in step . outputs . items (): dot . node ( self . ARTIFACT_PREFIX + str ( artifact . id ), f \" { artifact_name } \\n \" f \"( { artifact . _data_type } )\" , shape = self . ARTIFACT_SHAPE , ) dot . edge ( self . STEP_PREFIX + str ( step . id ), self . ARTIFACT_PREFIX + str ( artifact . id ), ) for artifact_name , artifact in step . inputs . items (): dot . edge ( self . ARTIFACT_PREFIX + str ( artifact . id ), self . STEP_PREFIX + str ( step . id ), ) with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : dot . render ( filename = f . name , format = \"png\" , view = True , cleanup = True ) return dot integration Integration Base class for integration in ZenML Source code in zenml/integrations/integration.py class Integration ( metaclass = IntegrationMeta ): \"\"\"Base class for integration in ZenML\"\"\" NAME = \"base_integration\" REQUIREMENTS : List [ str ] = [] SYSTEM_REQUIREMENTS : Dict [ str , str ] = {} @classmethod def check_installation ( cls ) -> bool : \"\"\"Method to check whether the required packages are installed\"\"\" try : for requirement , command in cls . SYSTEM_REQUIREMENTS . items (): result = shutil . which ( command ) if result is None : logger . debug ( \"Unable to find the required packages for %s on your \" \"system. Please install the packages on your system \" \"and try again.\" , requirement , ) return False for r in cls . REQUIREMENTS : pkg_resources . get_distribution ( r ) logger . debug ( f \"Integration { cls . NAME } is installed correctly with \" f \"requirements { cls . REQUIREMENTS } .\" ) return True except pkg_resources . DistributionNotFound as e : logger . debug ( f \"Unable to find required package ' { e . req } ' for \" f \"integration { cls . NAME } .\" ) return False except pkg_resources . VersionConflict as e : logger . debug ( f \"VersionConflict error when loading installation { cls . NAME } : \" f \" { str ( e ) } \" ) return False @staticmethod def activate () -> None : \"\"\"Abstract method to activate the integration\"\"\" activate () staticmethod Abstract method to activate the integration Source code in zenml/integrations/integration.py @staticmethod def activate () -> None : \"\"\"Abstract method to activate the integration\"\"\" check_installation () classmethod Method to check whether the required packages are installed Source code in zenml/integrations/integration.py @classmethod def check_installation ( cls ) -> bool : \"\"\"Method to check whether the required packages are installed\"\"\" try : for requirement , command in cls . SYSTEM_REQUIREMENTS . items (): result = shutil . which ( command ) if result is None : logger . debug ( \"Unable to find the required packages for %s on your \" \"system. Please install the packages on your system \" \"and try again.\" , requirement , ) return False for r in cls . REQUIREMENTS : pkg_resources . get_distribution ( r ) logger . debug ( f \"Integration { cls . NAME } is installed correctly with \" f \"requirements { cls . REQUIREMENTS } .\" ) return True except pkg_resources . DistributionNotFound as e : logger . debug ( f \"Unable to find required package ' { e . req } ' for \" f \"integration { cls . NAME } .\" ) return False except pkg_resources . VersionConflict as e : logger . debug ( f \"VersionConflict error when loading installation { cls . NAME } : \" f \" { str ( e ) } \" ) return False IntegrationMeta ( type ) Metaclass responsible for registering different Integration subclasses Source code in zenml/integrations/integration.py class IntegrationMeta ( type ): \"\"\"Metaclass responsible for registering different Integration subclasses\"\"\" def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"IntegrationMeta\" : \"\"\"Hook into creation of an Integration class.\"\"\" cls = cast ( Type [ \"Integration\" ], super () . __new__ ( mcs , name , bases , dct )) if name != \"Integration\" : integration_registry . register_integration ( cls . NAME , cls ) return cls __new__ ( mcs , name , bases , dct ) special staticmethod Hook into creation of an Integration class. Source code in zenml/integrations/integration.py def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"IntegrationMeta\" : \"\"\"Hook into creation of an Integration class.\"\"\" cls = cast ( Type [ \"Integration\" ], super () . __new__ ( mcs , name , bases , dct )) if name != \"Integration\" : integration_registry . register_integration ( cls . NAME , cls ) return cls kubeflow special The Kubeflow integration sub-module powers an alternative to the local orchestrator. You can enable it by registering the Kubeflow orchestrator with the CLI tool. KubeflowIntegration ( Integration ) Definition of Kubeflow Integration for ZenML. Source code in zenml/integrations/kubeflow/__init__.py class KubeflowIntegration ( Integration ): \"\"\"Definition of Kubeflow Integration for ZenML.\"\"\" NAME = KUBEFLOW REQUIREMENTS = [ \"kfp==1.8.9\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates all classes required for the airflow integration.\"\"\" from zenml.integrations.kubeflow import metadata_stores # noqa from zenml.integrations.kubeflow import orchestrators # noqa activate () classmethod Activates all classes required for the airflow integration. Source code in zenml/integrations/kubeflow/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates all classes required for the airflow integration.\"\"\" from zenml.integrations.kubeflow import metadata_stores # noqa from zenml.integrations.kubeflow import orchestrators # noqa container_entrypoint Main entrypoint for containers with Kubeflow TFX component executors. main () Runs a single step defined by the command line arguments. Source code in zenml/integrations/kubeflow/container_entrypoint.py def main () -> None : \"\"\"Runs a single step defined by the command line arguments.\"\"\" # Log to the container's stdout so Kubeflow Pipelines UI can display logs to # the user. logging . basicConfig ( stream = sys . stdout , level = logging . INFO ) logging . getLogger () . setLevel ( logging . INFO ) args = _parse_command_line_arguments () tfx_pipeline = pipeline_pb2 . Pipeline () json_format . Parse ( args . tfx_ir , tfx_pipeline ) _resolve_runtime_parameters ( tfx_pipeline , args . run_name , args . runtime_parameter ) node_id = args . node_id pipeline_node = _get_pipeline_node ( tfx_pipeline , node_id ) deployment_config = runner_utils . extract_local_deployment_config ( tfx_pipeline ) executor_spec = runner_utils . extract_executor_spec ( deployment_config , node_id ) custom_driver_spec = runner_utils . extract_custom_driver_spec ( deployment_config , node_id ) custom_executor_operators = { executable_spec_pb2 . ContainerExecutableSpec : kubernetes_executor_operator . KubernetesExecutorOperator } # make sure all integrations are activated so all materializers etc. are # available integration_registry . activate_integrations () metadata_store = Repository () . active_stack . metadata_store metadata_connection = metadata . Metadata ( metadata_store . get_tfx_metadata_config () ) # import the user main module to register all the materializers importlib . import_module ( args . main_module ) if hasattr ( executor_spec , \"class_path\" ): executor_module_parts = getattr ( executor_spec , \"class_path\" ) . split ( \".\" ) executor_class_target_module_name = \".\" . join ( executor_module_parts [: - 1 ]) _create_executor_class ( step_source_module_name = args . step_module , step_function_name = args . step_function_name , executor_class_target_module_name = executor_class_target_module_name , input_artifact_type_mapping = json . loads ( args . input_artifact_types ), ) else : raise RuntimeError ( f \"No class path found inside executor spec: { executor_spec } .\" ) component_launcher = launcher . Launcher ( pipeline_node = pipeline_node , mlmd_connection = metadata_connection , pipeline_info = tfx_pipeline . pipeline_info , pipeline_runtime_spec = tfx_pipeline . runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , custom_executor_operators = custom_executor_operators , ) execution_info = execute_step ( component_launcher ) if execution_info : _dump_ui_metadata ( pipeline_node , execution_info , args . metadata_ui_path ) docker_utils build_docker_image ( build_context_path , image_name , dockerfile_path = None , dockerignore_path = None , requirements = None , use_local_requirements = False , base_image = None ) Builds a docker image. Parameters: Name Type Description Default build_context_path str Path to a directory that will be sent to the docker daemon as build context. required image_name str The name to use for the created docker image. required dockerfile_path Optional[str] Optional path to a dockerfile. If no value is given, a temporary dockerfile will be created. None dockerignore_path Optional[str] Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside build_context_path are included in the build context. None requirements Optional[AbstractSet[str]] Optional list of pip requirements to install. This will only be used if no value is given for dockerfile_path . None use_local_requirements bool If True and no values are given for dockerfile_path and requirements , then the packages installed in the environment of the current python processed will be installed in the docker image. False base_image Optional[str] The image to use as base for the docker image. None Source code in zenml/integrations/kubeflow/docker_utils.py def build_docker_image ( build_context_path : str , image_name : str , dockerfile_path : Optional [ str ] = None , dockerignore_path : Optional [ str ] = None , requirements : Optional [ AbstractSet [ str ]] = None , use_local_requirements : bool = False , base_image : Optional [ str ] = None , ) -> None : \"\"\"Builds a docker image. Args: build_context_path: Path to a directory that will be sent to the docker daemon as build context. image_name: The name to use for the created docker image. dockerfile_path: Optional path to a dockerfile. If no value is given, a temporary dockerfile will be created. dockerignore_path: Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside `build_context_path` are included in the build context. requirements: Optional list of pip requirements to install. This will only be used if no value is given for `dockerfile_path`. use_local_requirements: If `True` and no values are given for `dockerfile_path` and `requirements`, then the packages installed in the environment of the current python processed will be installed in the docker image. base_image: The image to use as base for the docker image. \"\"\" if not requirements and use_local_requirements : local_requirements = get_current_environment_requirements () requirements = { f \" { package } == { version } \" for package , version in local_requirements . items () if package != \"zenml\" # exclude ZenML } logger . info ( \"Using requirements from local environment to build \" \"docker image: %s \" , requirements , ) if dockerfile_path : dockerfile_contents = zenml . io . utils . read_file_contents_as_string ( dockerfile_path ) else : dockerfile_contents = generate_dockerfile_contents ( requirements = requirements , base_image = base_image or DEFAULT_BASE_IMAGE , ) build_context = create_custom_build_context ( build_context_path = build_context_path , dockerfile_contents = dockerfile_contents , dockerignore_path = dockerignore_path , ) # If a custom base image is provided, make sure to always pull the # latest version of that image. If no base image is provided, we use # the static default ZenML image so there is no need to constantly pull always_pull_base_image = bool ( base_image ) logger . info ( \"Building docker image ' %s ', this might take a while...\" , image_name ) docker_client = DockerClient . from_env () # We use the client api directly here so we can stream the logs output_stream = docker_client . images . client . api . build ( fileobj = build_context , custom_context = True , tag = image_name , pull = always_pull_base_image , rm = False , # don't remove intermediate containers ) _process_stream ( output_stream ) logger . info ( \"Finished building docker image.\" ) create_custom_build_context ( build_context_path , dockerfile_contents , dockerignore_path = None ) Creates a docker build context. Parameters: Name Type Description Default build_context_path str Path to a directory that will be sent to the docker daemon as build context. required dockerfile_contents str File contents of the Dockerfile to use for the build. required dockerignore_path Optional[str] Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside build_context_path are included in the build context. None Returns: Type Description Any Docker build context that can be passed when building a docker image. Source code in zenml/integrations/kubeflow/docker_utils.py def create_custom_build_context ( build_context_path : str , dockerfile_contents : str , dockerignore_path : Optional [ str ] = None , ) -> Any : \"\"\"Creates a docker build context. Args: build_context_path: Path to a directory that will be sent to the docker daemon as build context. dockerfile_contents: File contents of the Dockerfile to use for the build. dockerignore_path: Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside `build_context_path` are included in the build context. Returns: Docker build context that can be passed when building a docker image. \"\"\" exclude_patterns = [] default_dockerignore_path = os . path . join ( build_context_path , \".dockerignore\" ) if dockerignore_path : exclude_patterns = _parse_dockerignore ( dockerignore_path ) elif fileio . file_exists ( default_dockerignore_path ): logger . info ( \"Using dockerignore found at path ' %s ' to create docker \" \"build context.\" , default_dockerignore_path , ) exclude_patterns = _parse_dockerignore ( default_dockerignore_path ) else : logger . info ( \"No explicit dockerignore specified and no file called \" \".dockerignore exists at the build context root ( %s ).\" \"Creating docker build context with all files inside the build \" \"context root directory.\" , build_context_path , ) logger . debug ( \"Exclude patterns for creating docker build context: %s \" , exclude_patterns , ) no_ignores_found = not exclude_patterns files = docker_build_utils . exclude_paths ( build_context_path , patterns = exclude_patterns ) extra_files = [( \"Dockerfile\" , dockerfile_contents )] context = docker_build_utils . create_archive ( root = build_context_path , files = sorted ( files ), gzip = False , extra_files = extra_files , ) build_context_size = os . path . getsize ( context . name ) if build_context_size > 50 * 1024 * 1024 and no_ignores_found : # The build context exceeds 50MiB and we didn't find any excludes # in dockerignore files -> remind to specify a .dockerignore file logger . warning ( \"Build context size for docker image: %s . If you believe this is \" \"unreasonably large, make sure to include a .dockerignore file at \" \"the root of your build context ( %s ) or specify a custom file \" \"when defining your pipeline.\" , string_utils . get_human_readable_filesize ( build_context_size ), default_dockerignore_path , ) return context generate_dockerfile_contents ( base_image , command = None , requirements = None ) Generates a Dockerfile. Parameters: Name Type Description Default base_image str The image to use as base for the dockerfile. required command Optional[str] The default command that gets executed when running a container of an image created by this dockerfile. None requirements Optional[AbstractSet[str]] Optional list of pip requirements to install. None Returns: Type Description str Content of a dockerfile. Source code in zenml/integrations/kubeflow/docker_utils.py def generate_dockerfile_contents ( base_image : str , command : Optional [ str ] = None , requirements : Optional [ AbstractSet [ str ]] = None , ) -> str : \"\"\"Generates a Dockerfile. Args: base_image: The image to use as base for the dockerfile. command: The default command that gets executed when running a container of an image created by this dockerfile. requirements: Optional list of pip requirements to install. Returns: Content of a dockerfile. \"\"\" lines = [ f \"FROM { base_image } \" , \"WORKDIR /app\" ] if requirements : lines . extend ( [ f \"RUN pip install --no-cache { ' ' . join ( requirements ) } \" , ] ) lines . append ( \"COPY . .\" ) if command : lines . append ( f \"CMD { command } \" ) return \" \\n \" . join ( lines ) get_current_environment_requirements () Returns a dict of package requirements for the environment that the current python process is running in. Source code in zenml/integrations/kubeflow/docker_utils.py def get_current_environment_requirements () -> Dict [ str , str ]: \"\"\"Returns a dict of package requirements for the environment that the current python process is running in.\"\"\" return { distribution . key : distribution . version for distribution in pkg_resources . working_set } get_image_digest ( image_name ) Gets the digest of a docker image. Parameters: Name Type Description Default image_name str Name of the image to get the digest for. required Returns: Type Description Optional[str] Returns the repo digest for the given image if there exists exactly one. If there are zero or multiple repo digests, returns None . Source code in zenml/integrations/kubeflow/docker_utils.py def get_image_digest ( image_name : str ) -> Optional [ str ]: \"\"\"Gets the digest of a docker image. Args: image_name: Name of the image to get the digest for. Returns: Returns the repo digest for the given image if there exists exactly one. If there are zero or multiple repo digests, returns `None`. \"\"\" docker_client = DockerClient . from_env () image = docker_client . images . get ( image_name ) repo_digests = image . attrs [ \"RepoDigests\" ] if len ( repo_digests ) == 1 : return cast ( str , repo_digests [ 0 ]) else : logger . debug ( \"Found zero or more repo digests for docker image ' %s ': %s \" , image_name , repo_digests , ) return None push_docker_image ( image_name ) Pushes a docker image to a container registry. Parameters: Name Type Description Default image_name str The full name (including a tag) of the image to push. required Source code in zenml/integrations/kubeflow/docker_utils.py def push_docker_image ( image_name : str ) -> None : \"\"\"Pushes a docker image to a container registry. Args: image_name: The full name (including a tag) of the image to push. \"\"\" logger . info ( \"Pushing docker image ' %s '.\" , image_name ) docker_client = DockerClient . from_env () output_stream = docker_client . images . push ( image_name , stream = True ) _process_stream ( output_stream ) logger . info ( \"Finished pushing docker image.\" ) metadata_stores special kubeflow_metadata_store KubeflowMetadataStore ( MySQLMetadataStore ) pydantic-model Kubeflow MySQL backend for ZenML metadata store. Source code in zenml/integrations/kubeflow/metadata_stores/kubeflow_metadata_store.py class KubeflowMetadataStore ( MySQLMetadataStore ): \"\"\"Kubeflow MySQL backend for ZenML metadata store.\"\"\" host : str = \"127.0.0.1\" port : int = 3306 database : str = \"metadb\" username : str = \"root\" password : str = \"\" @property def flavor ( self ) -> MetadataStoreFlavor : \"\"\"The metadata store flavor.\"\"\" return MetadataStoreFlavor . KUBEFLOW def get_tfx_metadata_config ( self , ) -> Union [ metadata_store_pb2 . ConnectionConfig , metadata_store_pb2 . MetadataStoreClientConfig , ]: \"\"\"Return tfx metadata config for the kubeflow metadata store.\"\"\" if inside_kfp_pod (): connection_config = metadata_store_pb2 . MetadataStoreClientConfig () connection_config . host = os . environ [ \"METADATA_GRPC_SERVICE_HOST\" ] connection_config . port = int ( os . environ [ \"METADATA_GRPC_SERVICE_PORT\" ] ) return connection_config else : return super () . get_tfx_metadata_config () flavor : MetadataStoreFlavor property readonly The metadata store flavor. get_tfx_metadata_config ( self ) Return tfx metadata config for the kubeflow metadata store. Source code in zenml/integrations/kubeflow/metadata_stores/kubeflow_metadata_store.py def get_tfx_metadata_config ( self , ) -> Union [ metadata_store_pb2 . ConnectionConfig , metadata_store_pb2 . MetadataStoreClientConfig , ]: \"\"\"Return tfx metadata config for the kubeflow metadata store.\"\"\" if inside_kfp_pod (): connection_config = metadata_store_pb2 . MetadataStoreClientConfig () connection_config . host = os . environ [ \"METADATA_GRPC_SERVICE_HOST\" ] connection_config . port = int ( os . environ [ \"METADATA_GRPC_SERVICE_PORT\" ] ) return connection_config else : return super () . get_tfx_metadata_config () inside_kfp_pod () Returns if the current python process is running inside a KFP Pod. Source code in zenml/integrations/kubeflow/metadata_stores/kubeflow_metadata_store.py def inside_kfp_pod () -> bool : \"\"\"Returns if the current python process is running inside a KFP Pod.\"\"\" if \"KFP_POD_NAME\" not in os . environ : return False try : k8s_config . load_incluster_config () return True except k8s_config . ConfigException : return False orchestrators special kubeflow_component Kubeflow Pipelines based implementation of TFX components. These components are lightweight wrappers around the KFP DSL's ContainerOp, and ensure that the container gets called with the right set of input arguments. It also ensures that each component exports named output attributes that are consistent with those provided by the native TFX components, thus ensuring that both types of pipeline definitions are compatible. Note: This requires Kubeflow Pipelines SDK to be installed. KubeflowComponent Base component for all Kubeflow pipelines TFX components. Returns a wrapper around a KFP DSL ContainerOp class, and adds named output attributes that match the output names for the corresponding native TFX components. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_component.py class KubeflowComponent : \"\"\"Base component for all Kubeflow pipelines TFX components. Returns a wrapper around a KFP DSL ContainerOp class, and adds named output attributes that match the output names for the corresponding native TFX components. \"\"\" def __init__ ( self , component : tfx_base_component . BaseComponent , depends_on : Set [ dsl . ContainerOp ], image : str , tfx_ir : pipeline_pb2 . Pipeline , # type: ignore[valid-type] pod_labels_to_attach : Dict [ str , str ], main_module : str , step_module : str , step_function_name : str , runtime_parameters : List [ data_types . RuntimeParameter ], metadata_ui_path : str = \"/tmp/mlpipeline-ui-metadata.json\" , ): \"\"\"Creates a new Kubeflow-based component. This class essentially wraps a dsl.ContainerOp construct in Kubeflow Pipelines. Args: component: The logical TFX component to wrap. depends_on: The set of upstream KFP ContainerOp components that this component will depend on. image: The container image to use for this component. tfx_ir: The TFX intermedia representation of the pipeline. pod_labels_to_attach: Dict of pod labels to attach to the GKE pod. runtime_parameters: Runtime parameters of the pipeline. metadata_ui_path: File location for metadata-ui-metadata.json file. \"\"\" utils . replace_placeholder ( component ) input_artifact_type_mapping = _get_input_artifact_type_mapping ( component ) arguments = [ \"--node_id\" , component . id , \"--tfx_ir\" , json_format . MessageToJson ( tfx_ir ), \"--metadata_ui_path\" , metadata_ui_path , \"--main_module\" , main_module , \"--step_module\" , step_module , \"--step_function_name\" , step_function_name , \"--input_artifact_types\" , json . dumps ( input_artifact_type_mapping ), \"--run_name\" , \"{{workflow.annotations.pipelines.kubeflow.org/run_name}}\" , ] for param in runtime_parameters : arguments . append ( \"--runtime_parameter\" ) arguments . append ( _encode_runtime_parameter ( param )) stack = Repository () . active_stack artifact_store = stack . artifact_store metadata_store = stack . metadata_store volumes : Dict [ str , k8s_client . V1Volume ] = {} has_local_repos = False if isinstance ( artifact_store , LocalArtifactStore ): has_local_repos = True host_path = k8s_client . V1HostPathVolumeSource ( path = artifact_store . path , type = \"Directory\" ) volumes [ artifact_store . path ] = k8s_client . V1Volume ( name = \"local-artifact-store\" , host_path = host_path ) logger . debug ( \"Adding host path volume for local artifact store (path: %s ) \" \"in kubeflow pipelines container.\" , artifact_store . path , ) if isinstance ( metadata_store , SQLiteMetadataStore ): has_local_repos = True metadata_store_dir = os . path . dirname ( metadata_store . uri ) host_path = k8s_client . V1HostPathVolumeSource ( path = metadata_store_dir , type = \"Directory\" ) volumes [ metadata_store_dir ] = k8s_client . V1Volume ( name = \"local-metadata-store\" , host_path = host_path ) logger . debug ( \"Adding host path volume for local metadata store (uri: %s ) \" \"in kubeflow pipelines container.\" , metadata_store . uri , ) self . container_op = dsl . ContainerOp ( name = component . id , command = CONTAINER_ENTRYPOINT_COMMAND , image = image , arguments = arguments , output_artifact_paths = { \"mlpipeline-ui-metadata\" : metadata_ui_path , }, pvolumes = volumes , ) if has_local_repos : if sys . platform == \"win32\" : # File permissions are not checked on Windows. This if clause # prevents mypy from complaining about unused 'type: ignore' # statements pass else : # Run KFP containers in the context of the local UID/GID # to ensure that the artifact and metadata stores can be shared # with the local pipeline runs. self . container_op . container . security_context = ( k8s_client . V1SecurityContext ( run_as_user = os . getuid (), run_as_group = os . getgid (), ) ) logger . debug ( \"Setting security context UID and GID to local user/group \" \"in kubeflow pipelines container.\" ) for op in depends_on : self . container_op . after ( op ) self . container_op . container . add_env_variable ( k8s_client . V1EnvVar ( name = ENV_ZENML_PREVENT_PIPELINE_EXECUTION , value = \"True\" ) ) for k , v in pod_labels_to_attach . items (): self . container_op . add_pod_label ( k , v ) __init__ ( self , component , depends_on , image , tfx_ir , pod_labels_to_attach , main_module , step_module , step_function_name , runtime_parameters , metadata_ui_path = '/tmp/mlpipeline-ui-metadata.json' ) special Creates a new Kubeflow-based component. This class essentially wraps a dsl.ContainerOp construct in Kubeflow Pipelines. Parameters: Name Type Description Default component BaseComponent The logical TFX component to wrap. required depends_on Set[kfp.dsl._container_op.ContainerOp] The set of upstream KFP ContainerOp components that this component will depend on. required image str The container image to use for this component. required tfx_ir Pipeline The TFX intermedia representation of the pipeline. required pod_labels_to_attach Dict[str, str] Dict of pod labels to attach to the GKE pod. required runtime_parameters List[tfx.orchestration.data_types.RuntimeParameter] Runtime parameters of the pipeline. required metadata_ui_path str File location for metadata-ui-metadata.json file. '/tmp/mlpipeline-ui-metadata.json' Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_component.py def __init__ ( self , component : tfx_base_component . BaseComponent , depends_on : Set [ dsl . ContainerOp ], image : str , tfx_ir : pipeline_pb2 . Pipeline , # type: ignore[valid-type] pod_labels_to_attach : Dict [ str , str ], main_module : str , step_module : str , step_function_name : str , runtime_parameters : List [ data_types . RuntimeParameter ], metadata_ui_path : str = \"/tmp/mlpipeline-ui-metadata.json\" , ): \"\"\"Creates a new Kubeflow-based component. This class essentially wraps a dsl.ContainerOp construct in Kubeflow Pipelines. Args: component: The logical TFX component to wrap. depends_on: The set of upstream KFP ContainerOp components that this component will depend on. image: The container image to use for this component. tfx_ir: The TFX intermedia representation of the pipeline. pod_labels_to_attach: Dict of pod labels to attach to the GKE pod. runtime_parameters: Runtime parameters of the pipeline. metadata_ui_path: File location for metadata-ui-metadata.json file. \"\"\" utils . replace_placeholder ( component ) input_artifact_type_mapping = _get_input_artifact_type_mapping ( component ) arguments = [ \"--node_id\" , component . id , \"--tfx_ir\" , json_format . MessageToJson ( tfx_ir ), \"--metadata_ui_path\" , metadata_ui_path , \"--main_module\" , main_module , \"--step_module\" , step_module , \"--step_function_name\" , step_function_name , \"--input_artifact_types\" , json . dumps ( input_artifact_type_mapping ), \"--run_name\" , \"{{workflow.annotations.pipelines.kubeflow.org/run_name}}\" , ] for param in runtime_parameters : arguments . append ( \"--runtime_parameter\" ) arguments . append ( _encode_runtime_parameter ( param )) stack = Repository () . active_stack artifact_store = stack . artifact_store metadata_store = stack . metadata_store volumes : Dict [ str , k8s_client . V1Volume ] = {} has_local_repos = False if isinstance ( artifact_store , LocalArtifactStore ): has_local_repos = True host_path = k8s_client . V1HostPathVolumeSource ( path = artifact_store . path , type = \"Directory\" ) volumes [ artifact_store . path ] = k8s_client . V1Volume ( name = \"local-artifact-store\" , host_path = host_path ) logger . debug ( \"Adding host path volume for local artifact store (path: %s ) \" \"in kubeflow pipelines container.\" , artifact_store . path , ) if isinstance ( metadata_store , SQLiteMetadataStore ): has_local_repos = True metadata_store_dir = os . path . dirname ( metadata_store . uri ) host_path = k8s_client . V1HostPathVolumeSource ( path = metadata_store_dir , type = \"Directory\" ) volumes [ metadata_store_dir ] = k8s_client . V1Volume ( name = \"local-metadata-store\" , host_path = host_path ) logger . debug ( \"Adding host path volume for local metadata store (uri: %s ) \" \"in kubeflow pipelines container.\" , metadata_store . uri , ) self . container_op = dsl . ContainerOp ( name = component . id , command = CONTAINER_ENTRYPOINT_COMMAND , image = image , arguments = arguments , output_artifact_paths = { \"mlpipeline-ui-metadata\" : metadata_ui_path , }, pvolumes = volumes , ) if has_local_repos : if sys . platform == \"win32\" : # File permissions are not checked on Windows. This if clause # prevents mypy from complaining about unused 'type: ignore' # statements pass else : # Run KFP containers in the context of the local UID/GID # to ensure that the artifact and metadata stores can be shared # with the local pipeline runs. self . container_op . container . security_context = ( k8s_client . V1SecurityContext ( run_as_user = os . getuid (), run_as_group = os . getgid (), ) ) logger . debug ( \"Setting security context UID and GID to local user/group \" \"in kubeflow pipelines container.\" ) for op in depends_on : self . container_op . after ( op ) self . container_op . container . add_env_variable ( k8s_client . V1EnvVar ( name = ENV_ZENML_PREVENT_PIPELINE_EXECUTION , value = \"True\" ) ) for k , v in pod_labels_to_attach . items (): self . container_op . add_pod_label ( k , v ) kubeflow_dag_runner The below code is copied from the TFX source repo with minor changes. All credits goes to the TFX team for the core implementation KubeflowDagRunner ( TfxRunner ) Kubeflow Pipelines runner. Constructs a pipeline definition YAML file based on the TFX logical pipeline. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py class KubeflowDagRunner ( tfx_runner . TfxRunner ): \"\"\"Kubeflow Pipelines runner. Constructs a pipeline definition YAML file based on the TFX logical pipeline. \"\"\" def __init__ ( self , config : KubeflowDagRunnerConfig , output_path : str , pod_labels_to_attach : Optional [ Dict [ str , str ]] = None , ): \"\"\"Initializes KubeflowDagRunner for compiling a Kubeflow Pipeline. Args: config: A KubeflowDagRunnerConfig object to specify runtime configuration when running the pipeline under Kubeflow. output_path: Path where the pipeline definition file will be stored. pod_labels_to_attach: Optional set of pod labels to attach to GKE pod spinned up for this pipeline. Default to the 3 labels: 1. add-pod-env: true, 2. pipeline SDK type, 3. pipeline unique ID, where 2 and 3 are instrumentation of usage tracking. \"\"\" super () . __init__ ( config ) self . _kubeflow_config = config self . _output_path = output_path self . _compiler = compiler . Compiler () self . _tfx_compiler = tfx_compiler . Compiler () self . _params : List [ dsl . PipelineParam ] = [] self . _params_by_component_id : Dict [ str , List [ data_types . RuntimeParameter ] ] = collections . defaultdict ( list ) self . _deduped_parameter_names : Set [ str ] = set () self . _pod_labels_to_attach = ( pod_labels_to_attach or get_default_pod_labels () ) def _parse_parameter_from_component ( self , component : tfx_base_component . BaseComponent ) -> None : \"\"\"Extract embedded RuntimeParameter placeholders from a component. Extract embedded RuntimeParameter placeholders from a component, then append the corresponding dsl.PipelineParam to KubeflowDagRunner. Args: component: a TFX component. \"\"\" deduped_parameter_names_for_component = set () for parameter in component . exec_properties . values (): if not isinstance ( parameter , data_types . RuntimeParameter ): continue # Ignore pipeline root because it will be added later. if parameter . name == tfx_pipeline . ROOT_PARAMETER . name : continue if parameter . name in deduped_parameter_names_for_component : continue deduped_parameter_names_for_component . add ( parameter . name ) self . _params_by_component_id [ component . id ] . append ( parameter ) if parameter . name not in self . _deduped_parameter_names : self . _deduped_parameter_names . add ( parameter . name ) dsl_parameter = dsl . PipelineParam ( name = parameter . name , value = str ( parameter . default ) ) self . _params . append ( dsl_parameter ) def _parse_parameter_from_pipeline ( self , pipeline : tfx_pipeline . Pipeline ) -> None : \"\"\"Extract all the RuntimeParameter placeholders from the pipeline.\"\"\" for component in pipeline . components : self . _parse_parameter_from_component ( component ) def _construct_pipeline_graph ( self , pipeline : tfx_pipeline . Pipeline ) -> None : \"\"\"Constructs a Kubeflow Pipeline graph. Args: pipeline: The logical TFX pipeline to base the construction on. pipeline_root: dsl.PipelineParam representing the pipeline root. \"\"\" component_to_kfp_op : Dict [ base_node . BaseNode , dsl . ContainerOp ] = {} tfx_ir = self . _generate_tfx_ir ( pipeline ) # Assumption: There is a partial ordering of components in the list, # i.e. if component A depends on component B and C, then A appears # after B and C in the list. for component in pipeline . components : # Keep track of the set of upstream dsl.ContainerOps for this # component. depends_on = set () for upstream_component in component . upstream_nodes : depends_on . add ( component_to_kfp_op [ upstream_component ]) # remove the extra pipeline node information tfx_node_ir = self . _dehydrate_tfx_ir ( tfx_ir , component . id ) from zenml.utils import source_utils main_module_file = sys . modules [ \"__main__\" ] . __file__ main_module = source_utils . get_module_source_from_file_path ( os . path . abspath ( main_module_file ) ) step_module = component . component_type . split ( \".\" )[: - 1 ] if step_module [ 0 ] == \"__main__\" : step_module = main_module else : step_module = \".\" . join ( step_module ) kfp_component = KubeflowComponent ( main_module = main_module , step_module = step_module , step_function_name = component . id , component = component , depends_on = depends_on , image = self . _kubeflow_config . image , pod_labels_to_attach = self . _pod_labels_to_attach , tfx_ir = tfx_node_ir , metadata_ui_path = self . _kubeflow_config . metadata_ui_path , runtime_parameters = self . _params_by_component_id [ component . id ], ) for operator in self . _kubeflow_config . pipeline_operator_funcs : kfp_component . container_op . apply ( operator ) component_to_kfp_op [ component ] = kfp_component . container_op def _del_unused_field ( self , node_id : str , message_dict : MutableMapping [ str , Any ] ) -> None : \"\"\"Remove fields that are not used by the pipeline.\"\"\" for item in list ( message_dict . keys ()): if item != node_id : del message_dict [ item ] def _dehydrate_tfx_ir ( self , original_pipeline : pipeline_pb2 . Pipeline , node_id : str # type: ignore[valid-type] # noqa ) -> pipeline_pb2 . Pipeline : # type: ignore[valid-type] \"\"\"Dehydrate the TFX IR to remove unused fields.\"\"\" pipeline = copy . deepcopy ( original_pipeline ) for node in pipeline . nodes : # type: ignore[attr-defined] if ( node . WhichOneof ( \"node\" ) == \"pipeline_node\" and node . pipeline_node . node_info . id == node_id ): del pipeline . nodes [:] # type: ignore[attr-defined] pipeline . nodes . extend ([ node ]) # type: ignore[attr-defined] break deployment_config = pipeline_pb2 . IntermediateDeploymentConfig () pipeline . deployment_config . Unpack ( deployment_config ) # type: ignore[attr-defined] # noqa self . _del_unused_field ( node_id , deployment_config . executor_specs ) self . _del_unused_field ( node_id , deployment_config . custom_driver_specs ) self . _del_unused_field ( node_id , deployment_config . node_level_platform_configs ) pipeline . deployment_config . Pack ( deployment_config ) # type: ignore[attr-defined] # noqa return pipeline def _generate_tfx_ir ( self , pipeline : tfx_pipeline . Pipeline ) -> Optional [ pipeline_pb2 . Pipeline ]: # type: ignore[valid-type] \"\"\"Generate the TFX IR from the logical TFX pipeline.\"\"\" result = self . _tfx_compiler . compile ( pipeline ) return result def run ( self , pipeline : tfx_pipeline . Pipeline ) -> None : \"\"\"Compiles and outputs a Kubeflow Pipeline YAML definition file. Args: pipeline: The logical TFX pipeline to use when building the Kubeflow pipeline. \"\"\" for component in pipeline . components : # TODO(b/187122662): Pass through pip dependencies as a first-class # component flag. if isinstance ( component , tfx_base_component . BaseComponent ): component . _resolve_pip_dependencies ( # pylint: disable=protected-access pipeline . pipeline_info . pipeline_root ) def _construct_pipeline () -> None : \"\"\"Creates Kubeflow ContainerOps for each TFX component encountered in the pipeline definition.\"\"\" self . _construct_pipeline_graph ( pipeline ) # Need to run this first to get self._params populated. Then KFP # compiler can correctly match default value with PipelineParam. self . _parse_parameter_from_pipeline ( pipeline ) # Create workflow spec and write out to package. self . _compiler . _create_and_write_workflow ( # pylint: disable=protected-access pipeline_func = _construct_pipeline , pipeline_name = pipeline . pipeline_info . pipeline_name , params_list = self . _params , package_path = self . _output_path , ) logger . info ( \"Finished writing kubeflow pipeline definition file ' %s '.\" , self . _output_path , ) __init__ ( self , config , output_path , pod_labels_to_attach = None ) special Initializes KubeflowDagRunner for compiling a Kubeflow Pipeline. Parameters: Name Type Description Default config KubeflowDagRunnerConfig A KubeflowDagRunnerConfig object to specify runtime configuration when running the pipeline under Kubeflow. required output_path str Path where the pipeline definition file will be stored. required pod_labels_to_attach Optional[Dict[str, str]] Optional set of pod labels to attach to GKE pod spinned up for this pipeline. Default to the 3 labels: 1. add-pod-env: true, 2. pipeline SDK type, 3. pipeline unique ID, where 2 and 3 are instrumentation of usage tracking. None Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py def __init__ ( self , config : KubeflowDagRunnerConfig , output_path : str , pod_labels_to_attach : Optional [ Dict [ str , str ]] = None , ): \"\"\"Initializes KubeflowDagRunner for compiling a Kubeflow Pipeline. Args: config: A KubeflowDagRunnerConfig object to specify runtime configuration when running the pipeline under Kubeflow. output_path: Path where the pipeline definition file will be stored. pod_labels_to_attach: Optional set of pod labels to attach to GKE pod spinned up for this pipeline. Default to the 3 labels: 1. add-pod-env: true, 2. pipeline SDK type, 3. pipeline unique ID, where 2 and 3 are instrumentation of usage tracking. \"\"\" super () . __init__ ( config ) self . _kubeflow_config = config self . _output_path = output_path self . _compiler = compiler . Compiler () self . _tfx_compiler = tfx_compiler . Compiler () self . _params : List [ dsl . PipelineParam ] = [] self . _params_by_component_id : Dict [ str , List [ data_types . RuntimeParameter ] ] = collections . defaultdict ( list ) self . _deduped_parameter_names : Set [ str ] = set () self . _pod_labels_to_attach = ( pod_labels_to_attach or get_default_pod_labels () ) run ( self , pipeline ) Compiles and outputs a Kubeflow Pipeline YAML definition file. Parameters: Name Type Description Default pipeline Pipeline The logical TFX pipeline to use when building the Kubeflow pipeline. required Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py def run ( self , pipeline : tfx_pipeline . Pipeline ) -> None : \"\"\"Compiles and outputs a Kubeflow Pipeline YAML definition file. Args: pipeline: The logical TFX pipeline to use when building the Kubeflow pipeline. \"\"\" for component in pipeline . components : # TODO(b/187122662): Pass through pip dependencies as a first-class # component flag. if isinstance ( component , tfx_base_component . BaseComponent ): component . _resolve_pip_dependencies ( # pylint: disable=protected-access pipeline . pipeline_info . pipeline_root ) def _construct_pipeline () -> None : \"\"\"Creates Kubeflow ContainerOps for each TFX component encountered in the pipeline definition.\"\"\" self . _construct_pipeline_graph ( pipeline ) # Need to run this first to get self._params populated. Then KFP # compiler can correctly match default value with PipelineParam. self . _parse_parameter_from_pipeline ( pipeline ) # Create workflow spec and write out to package. self . _compiler . _create_and_write_workflow ( # pylint: disable=protected-access pipeline_func = _construct_pipeline , pipeline_name = pipeline . pipeline_info . pipeline_name , params_list = self . _params , package_path = self . _output_path , ) logger . info ( \"Finished writing kubeflow pipeline definition file ' %s '.\" , self . _output_path , ) KubeflowDagRunnerConfig ( PipelineConfig ) Runtime configuration parameters specific to execution on Kubeflow. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py class KubeflowDagRunnerConfig ( pipeline_config . PipelineConfig ): \"\"\"Runtime configuration parameters specific to execution on Kubeflow.\"\"\" def __init__ ( self , image : str , pipeline_operator_funcs : Optional [ List [ OpFunc ]] = None , supported_launcher_classes : Optional [ List [ Type [ base_component_launcher . BaseComponentLauncher ]] ] = None , metadata_ui_path : str = \"/tmp/mlpipeline-ui-metadata.json\" , ** kwargs : Any ): \"\"\"Creates a KubeflowDagRunnerConfig object. The user can use pipeline_operator_funcs to apply modifications to ContainerOps used in the pipeline. For example, to ensure the pipeline steps mount a GCP secret, and a Persistent Volume, one can create config object like so: from kfp import gcp, onprem mount_secret_op = gcp.use_secret('my-secret-name) mount_volume_op = onprem.mount_pvc( \"my-persistent-volume-claim\", \"my-volume-name\", \"/mnt/volume-mount-path\") config = KubeflowDagRunnerConfig( pipeline_operator_funcs=[mount_secret_op, mount_volume_op] ) Args: image: The docker image to use in the pipeline. pipeline_operator_funcs: A list of ContainerOp modifying functions that will be applied to every container step in the pipeline. supported_launcher_classes: A list of component launcher classes that are supported by the current pipeline. List sequence determines the order in which launchers are chosen for each component being run. metadata_ui_path: File location for metadata-ui-metadata.json file. **kwargs: keyword args for PipelineConfig. \"\"\" supported_launcher_classes = supported_launcher_classes or [ in_process_component_launcher . InProcessComponentLauncher , kubernetes_component_launcher . KubernetesComponentLauncher , ] super () . __init__ ( supported_launcher_classes = supported_launcher_classes , ** kwargs ) self . pipeline_operator_funcs = ( pipeline_operator_funcs or get_default_pipeline_operator_funcs () ) self . image = image self . metadata_ui_path = metadata_ui_path __init__ ( self , image , pipeline_operator_funcs = None , supported_launcher_classes = None , metadata_ui_path = '/tmp/mlpipeline-ui-metadata.json' , ** kwargs ) special Creates a KubeflowDagRunnerConfig object. The user can use pipeline_operator_funcs to apply modifications to ContainerOps used in the pipeline. For example, to ensure the pipeline steps mount a GCP secret, and a Persistent Volume, one can create config object like so: from kfp import gcp, onprem mount_secret_op = gcp.use_secret('my-secret-name) mount_volume_op = onprem.mount_pvc( \"my-persistent-volume-claim\", \"my-volume-name\", \"/mnt/volume-mount-path\") config = KubeflowDagRunnerConfig( pipeline_operator_funcs=[mount_secret_op, mount_volume_op] ) Parameters: Name Type Description Default image str The docker image to use in the pipeline. required pipeline_operator_funcs Optional[List[Callable[[kfp.dsl._container_op.ContainerOp], Union[kfp.dsl._container_op.ContainerOp, NoneType]]]] A list of ContainerOp modifying functions that will be applied to every container step in the pipeline. None supported_launcher_classes Optional[List[Type[tfx.orchestration.launcher.base_component_launcher.BaseComponentLauncher]]] A list of component launcher classes that are supported by the current pipeline. List sequence determines the order in which launchers are chosen for each component being run. None metadata_ui_path str File location for metadata-ui-metadata.json file. '/tmp/mlpipeline-ui-metadata.json' **kwargs Any keyword args for PipelineConfig. {} Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py def __init__ ( self , image : str , pipeline_operator_funcs : Optional [ List [ OpFunc ]] = None , supported_launcher_classes : Optional [ List [ Type [ base_component_launcher . BaseComponentLauncher ]] ] = None , metadata_ui_path : str = \"/tmp/mlpipeline-ui-metadata.json\" , ** kwargs : Any ): \"\"\"Creates a KubeflowDagRunnerConfig object. The user can use pipeline_operator_funcs to apply modifications to ContainerOps used in the pipeline. For example, to ensure the pipeline steps mount a GCP secret, and a Persistent Volume, one can create config object like so: from kfp import gcp, onprem mount_secret_op = gcp.use_secret('my-secret-name) mount_volume_op = onprem.mount_pvc( \"my-persistent-volume-claim\", \"my-volume-name\", \"/mnt/volume-mount-path\") config = KubeflowDagRunnerConfig( pipeline_operator_funcs=[mount_secret_op, mount_volume_op] ) Args: image: The docker image to use in the pipeline. pipeline_operator_funcs: A list of ContainerOp modifying functions that will be applied to every container step in the pipeline. supported_launcher_classes: A list of component launcher classes that are supported by the current pipeline. List sequence determines the order in which launchers are chosen for each component being run. metadata_ui_path: File location for metadata-ui-metadata.json file. **kwargs: keyword args for PipelineConfig. \"\"\" supported_launcher_classes = supported_launcher_classes or [ in_process_component_launcher . InProcessComponentLauncher , kubernetes_component_launcher . KubernetesComponentLauncher , ] super () . __init__ ( supported_launcher_classes = supported_launcher_classes , ** kwargs ) self . pipeline_operator_funcs = ( pipeline_operator_funcs or get_default_pipeline_operator_funcs () ) self . image = image self . metadata_ui_path = metadata_ui_path get_default_pipeline_operator_funcs ( use_gcp_sa = False ) Returns a default list of pipeline operator functions. Parameters: Name Type Description Default use_gcp_sa bool If true, mount a GCP service account secret to each pod, with the name _KUBEFLOW_GCP_SECRET_NAME. False Returns: Type Description List[Callable[[kfp.dsl._container_op.ContainerOp], Optional[kfp.dsl._container_op.ContainerOp]]] A list of functions with type OpFunc. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py def get_default_pipeline_operator_funcs ( use_gcp_sa : bool = False , ) -> List [ OpFunc ]: \"\"\"Returns a default list of pipeline operator functions. Args: use_gcp_sa: If true, mount a GCP service account secret to each pod, with the name _KUBEFLOW_GCP_SECRET_NAME. Returns: A list of functions with type OpFunc. \"\"\" # Enables authentication for GCP services if needed. gcp_secret_op = gcp . use_gcp_secret ( _KUBEFLOW_GCP_SECRET_NAME ) # Mounts configmap containing Metadata gRPC server configuration. mount_config_map_op = _mount_config_map_op ( \"metadata-grpc-configmap\" ) if use_gcp_sa : return [ gcp_secret_op , mount_config_map_op ] else : return [ mount_config_map_op ] get_default_pod_labels () Returns the default pod label dict for Kubeflow. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py def get_default_pod_labels () -> Dict [ str , str ]: \"\"\"Returns the default pod label dict for Kubeflow.\"\"\" # KFP default transformers add pod env: # https://github.com/kubeflow/pipelines/blob/0.1.32/sdk/python/kfp/compiler/_default_transformers.py result = { \"add-pod-env\" : \"true\" , telemetry_utils . LABEL_KFP_SDK_ENV : \"tfx\" } return result kubeflow_orchestrator KubeflowOrchestrator ( BaseOrchestrator ) pydantic-model Orchestrator responsible for running pipelines using Kubeflow. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py class KubeflowOrchestrator ( BaseOrchestrator ): \"\"\"Orchestrator responsible for running pipelines using Kubeflow.\"\"\" custom_docker_base_image_name : Optional [ str ] = None kubeflow_pipelines_ui_port : int = DEFAULT_KFP_UI_PORT kubernetes_context : Optional [ str ] = None supports_local_execution = True supports_remote_execution = True @property def flavor ( self ) -> OrchestratorFlavor : \"\"\"The orchestrator flavor.\"\"\" return OrchestratorFlavor . KUBEFLOW @property def validator ( self ) -> Optional [ StackValidator ]: \"\"\"Validates that the stack contains a container registry.\"\"\" return StackValidator ( required_components = { StackComponentType . CONTAINER_REGISTRY } ) def get_docker_image_name ( self , pipeline_name : str ) -> str : \"\"\"Returns the full docker image name including registry and tag.\"\"\" base_image_name = f \"zenml-kubeflow: { pipeline_name } \" container_registry = Repository () . active_stack . container_registry if container_registry : registry_uri = container_registry . uri . rstrip ( \"/\" ) return f \" { registry_uri } / { base_image_name } \" else : return base_image_name @property def root_directory ( self ) -> str : \"\"\"Returns path to the root directory for all files concerning this orchestrator.\"\"\" return os . path . join ( zenml . io . utils . get_global_config_directory (), \"kubeflow\" , str ( self . uuid ), ) @property def pipeline_directory ( self ) -> str : \"\"\"Returns path to a directory in which the kubeflow pipeline files are stored.\"\"\" return os . path . join ( self . root_directory , \"pipelines\" ) def prepare_pipeline_deployment ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , runtime_configuration : \"RuntimeConfiguration\" , ) -> None : \"\"\"Builds a docker image for the current environment and uploads it to a container registry if configured. \"\"\" from zenml.integrations.kubeflow.docker_utils import ( build_docker_image , push_docker_image , ) image_name = self . get_docker_image_name ( pipeline . name ) requirements = { \"kubernetes\" , * stack . requirements ( exclude_components = { StackComponentType . ORCHESTRATOR } ), * self . _get_pipeline_requirements ( pipeline ), } logger . debug ( \"Kubeflow docker container requirements: %s \" , requirements ) build_docker_image ( build_context_path = str ( Repository () . root ), image_name = image_name , dockerignore_path = pipeline . dockerignore_file , requirements = requirements , base_image = self . custom_docker_base_image_name , ) if stack . container_registry : push_docker_image ( image_name ) def run_pipeline ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , run_name : str ) -> Any : \"\"\"Runs a pipeline on Kubeflow Pipelines.\"\"\" from zenml.integrations.kubeflow.docker_utils import get_image_digest image_name = self . get_docker_image_name ( pipeline . name ) image_name = get_image_digest ( image_name ) or image_name fileio . make_dirs ( self . pipeline_directory ) pipeline_file_path = os . path . join ( self . pipeline_directory , f \" { pipeline . name } .yaml\" ) runner_config = KubeflowDagRunnerConfig ( image = image_name ) runner = KubeflowDagRunner ( config = runner_config , output_path = pipeline_file_path ) tfx_pipeline = create_tfx_pipeline ( pipeline , stack = stack ) runner . run ( tfx_pipeline ) self . _upload_and_run_pipeline ( pipeline_file_path = pipeline_file_path , run_name = run_name , enable_cache = pipeline . enable_cache , ) def _upload_and_run_pipeline ( self , pipeline_file_path : str , run_name : str , enable_cache : bool ) -> None : \"\"\"Tries to upload and run a KFP pipeline. Args: pipeline_file_path: Path to the pipeline definition file. run_name: A name for the pipeline run that will be started. enable_cache: Whether caching is enabled for this pipeline run. \"\"\" try : if self . kubernetes_context : logger . info ( \"Running in kubernetes context ' %s '.\" , self . kubernetes_context , ) # load kubernetes config to authorize the KFP client config . load_kube_config ( context = self . kubernetes_context ) # upload the pipeline to Kubeflow and start it client = kfp . Client () result = client . create_run_from_pipeline_package ( pipeline_file_path , arguments = {}, run_name = run_name , enable_caching = enable_cache , ) logger . info ( \"Started pipeline run with ID ' %s '.\" , result . run_id ) except urllib3 . exceptions . HTTPError as error : logger . warning ( \"Failed to upload Kubeflow pipeline: %s . \" \"Please make sure your kube config is configured and the \" \"current context is set correctly.\" , error , ) def _get_pipeline_requirements ( self , pipeline : \"BasePipeline\" ) -> Set [ str ]: \"\"\"Gets list of requirements for a pipeline.\"\"\" if pipeline . requirements_file and fileio . file_exists ( pipeline . requirements_file ): logger . debug ( \"Using requirements from file %s .\" , pipeline . requirements_file ) with fileio . open ( pipeline . requirements_file , \"r\" ) as f : return { requirement . strip () for requirement in f . read () . split ( \" \\n \" ) } else : return set () @property def _pid_file_path ( self ) -> str : \"\"\"Returns path to the daemon PID file.\"\"\" return os . path . join ( self . root_directory , \"kubeflow_daemon.pid\" ) @property def log_file ( self ) -> str : \"\"\"Path of the daemon log file.\"\"\" return os . path . join ( self . root_directory , \"kubeflow_daemon.log\" ) @property def _k3d_cluster_name ( self ) -> str : \"\"\"Returns the K3D cluster name.\"\"\" # K3D only allows cluster names with up to 32 characters, use the # first 8 chars of the orchestrator UUID as identifier return f \"zenml-kubeflow- { str ( self . uuid )[: 8 ] } \" def _get_k3d_registry_name ( self , port : int ) -> str : \"\"\"Returns the K3D registry name.\"\"\" return f \"k3d-zenml-kubeflow-registry.localhost: { port } \" @property def _k3d_registry_config_path ( self ) -> str : \"\"\"Returns the path to the K3D registry config yaml.\"\"\" return os . path . join ( self . root_directory , \"k3d_registry.yaml\" ) def list_manual_setup_steps ( self , container_registry_name : str , container_registry_path : str ) -> None : \"\"\"Logs manual steps needed to setup the Kubeflow local orchestrator.\"\"\" global_config_dir_path = zenml . io . utils . get_global_config_directory () kubeflow_commands = [ f \"> k3d cluster create CLUSTER_NAME --registry-create { container_registry_name } --registry-config { container_registry_path } --volume { global_config_dir_path } : { global_config_dir_path } \\n \" , f \"> kubectl --context CLUSTER_NAME apply -k github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref= { KFP_VERSION } &timeout=1m\" , \"> kubectl --context CLUSTER_NAME wait --timeout=60s --for condition=established crd/applications.app.k8s.io\" , f \"> kubectl --context CLUSTER_NAME apply -k github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref= { KFP_VERSION } &timeout=1m\" , f \"> kubectl --namespace kubeflow port-forward svc/ml-pipeline-ui { self . kubeflow_pipelines_ui_port } :80\" , ] logger . error ( \"Unable to spin up local Kubeflow Pipelines deployment.\" ) logger . info ( \"If you wish to spin up this Kubeflow local orchestrator manually, \" \"please enter the following commands (substituting where appropriate): \\n \" ) logger . info ( \" \\n \" . join ( kubeflow_commands )) @property def is_provisioned ( self ) -> bool : \"\"\"Returns if a local k3d cluster for this orchestrator exists.\"\"\" if not local_deployment_utils . check_prerequisites (): # if any prerequisites are missing there is certainly no # local deployment running return False return local_deployment_utils . k3d_cluster_exists ( cluster_name = self . _k3d_cluster_name ) @property def is_running ( self ) -> bool : \"\"\"Returns if the local k3d cluster for this orchestrator is running.\"\"\" if not self . is_provisioned : return False return local_deployment_utils . k3d_cluster_running ( cluster_name = self . _k3d_cluster_name ) def provision ( self ) -> None : \"\"\"Provisions a local Kubeflow Pipelines deployment.\"\"\" if self . is_running : logger . info ( \"Found already existing local Kubeflow Pipelines deployment. \" \"If there are any issues with the existing deployment, please \" \"run 'zenml orchestrator down' to delete it.\" ) return if not local_deployment_utils . check_prerequisites (): logger . error ( \"Unable to provision local Kubeflow Pipelines deployment: \" \"Please install 'k3d' and 'kubectl' and try again.\" ) return container_registry = Repository () . active_stack . container_registry if not container_registry : logger . error ( \"Unable to provision local Kubeflow Pipelines deployment: \" \"Missing container registry in current stack.\" ) return logger . info ( \"Provisioning local Kubeflow Pipelines deployment...\" ) fileio . make_dirs ( self . root_directory ) container_registry_port = int ( container_registry . uri . split ( \":\" )[ - 1 ]) container_registry_name = self . _get_k3d_registry_name ( port = container_registry_port ) local_deployment_utils . write_local_registry_yaml ( yaml_path = self . _k3d_registry_config_path , registry_name = container_registry_name , registry_uri = container_registry . uri , ) try : local_deployment_utils . create_k3d_cluster ( cluster_name = self . _k3d_cluster_name , registry_name = container_registry_name , registry_config_path = self . _k3d_registry_config_path , ) kubernetes_context = f \"k3d- { self . _k3d_cluster_name } \" local_deployment_utils . deploy_kubeflow_pipelines ( kubernetes_context = kubernetes_context ) port = self . kubeflow_pipelines_ui_port if ( port == DEFAULT_KFP_UI_PORT and not networking_utils . port_available ( port ) ): # if the user didn't specify a specific port and the default # port is occupied, fallback to a random open port port = networking_utils . find_available_port () local_deployment_utils . start_kfp_ui_daemon ( pid_file_path = self . _pid_file_path , log_file_path = self . log_file , port = port , ) except Exception as e : logger . error ( e ) self . list_manual_setup_steps ( container_registry_name , self . _k3d_registry_config_path ) self . deprovision () def deprovision ( self ) -> None : \"\"\"Deprovisions a local Kubeflow Pipelines deployment.\"\"\" if self . is_running : local_deployment_utils . delete_k3d_cluster ( cluster_name = self . _k3d_cluster_name ) if fileio . file_exists ( self . _pid_file_path ): if sys . platform == \"win32\" : # Daemon functionality is not supported on Windows, so the PID # file won't exist. This if clause exists just for mypy to not # complain about missing functions pass else : from zenml.utils import daemon daemon . stop_daemon ( self . _pid_file_path , kill_children = True ) fileio . remove ( self . _pid_file_path ) if fileio . file_exists ( self . log_file ): fileio . remove ( self . log_file ) logger . info ( \"Local kubeflow pipelines deployment deprovisioned.\" ) def resume ( self ) -> None : \"\"\"Resumes the local k3d cluster.\"\"\" if self . is_running : logger . info ( \"Local kubeflow pipelines deployment already running.\" ) return if not self . is_provisioned : raise ProvisioningError ( \"Unable to resume local kubeflow pipelines deployment: No \" \"resources provisioned for local deployment.\" ) local_deployment_utils . start_k3d_cluster ( cluster_name = self . _k3d_cluster_name ) def suspend ( self ) -> None : \"\"\"Suspends the local k3d cluster.\"\"\" if not self . is_running : logger . info ( \"Local kubeflow pipelines deployment not running.\" ) return local_deployment_utils . stop_k3d_cluster ( cluster_name = self . _k3d_cluster_name ) flavor : OrchestratorFlavor property readonly The orchestrator flavor. is_provisioned : bool property readonly Returns if a local k3d cluster for this orchestrator exists. is_running : bool property readonly Returns if the local k3d cluster for this orchestrator is running. log_file : str property readonly Path of the daemon log file. pipeline_directory : str property readonly Returns path to a directory in which the kubeflow pipeline files are stored. root_directory : str property readonly Returns path to the root directory for all files concerning this orchestrator. validator : Optional [ zenml . stack . stack_validator . StackValidator ] property readonly Validates that the stack contains a container registry. deprovision ( self ) Deprovisions a local Kubeflow Pipelines deployment. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def deprovision ( self ) -> None : \"\"\"Deprovisions a local Kubeflow Pipelines deployment.\"\"\" if self . is_running : local_deployment_utils . delete_k3d_cluster ( cluster_name = self . _k3d_cluster_name ) if fileio . file_exists ( self . _pid_file_path ): if sys . platform == \"win32\" : # Daemon functionality is not supported on Windows, so the PID # file won't exist. This if clause exists just for mypy to not # complain about missing functions pass else : from zenml.utils import daemon daemon . stop_daemon ( self . _pid_file_path , kill_children = True ) fileio . remove ( self . _pid_file_path ) if fileio . file_exists ( self . log_file ): fileio . remove ( self . log_file ) logger . info ( \"Local kubeflow pipelines deployment deprovisioned.\" ) get_docker_image_name ( self , pipeline_name ) Returns the full docker image name including registry and tag. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def get_docker_image_name ( self , pipeline_name : str ) -> str : \"\"\"Returns the full docker image name including registry and tag.\"\"\" base_image_name = f \"zenml-kubeflow: { pipeline_name } \" container_registry = Repository () . active_stack . container_registry if container_registry : registry_uri = container_registry . uri . rstrip ( \"/\" ) return f \" { registry_uri } / { base_image_name } \" else : return base_image_name list_manual_setup_steps ( self , container_registry_name , container_registry_path ) Logs manual steps needed to setup the Kubeflow local orchestrator. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def list_manual_setup_steps ( self , container_registry_name : str , container_registry_path : str ) -> None : \"\"\"Logs manual steps needed to setup the Kubeflow local orchestrator.\"\"\" global_config_dir_path = zenml . io . utils . get_global_config_directory () kubeflow_commands = [ f \"> k3d cluster create CLUSTER_NAME --registry-create { container_registry_name } --registry-config { container_registry_path } --volume { global_config_dir_path } : { global_config_dir_path } \\n \" , f \"> kubectl --context CLUSTER_NAME apply -k github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref= { KFP_VERSION } &timeout=1m\" , \"> kubectl --context CLUSTER_NAME wait --timeout=60s --for condition=established crd/applications.app.k8s.io\" , f \"> kubectl --context CLUSTER_NAME apply -k github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref= { KFP_VERSION } &timeout=1m\" , f \"> kubectl --namespace kubeflow port-forward svc/ml-pipeline-ui { self . kubeflow_pipelines_ui_port } :80\" , ] logger . error ( \"Unable to spin up local Kubeflow Pipelines deployment.\" ) logger . info ( \"If you wish to spin up this Kubeflow local orchestrator manually, \" \"please enter the following commands (substituting where appropriate): \\n \" ) logger . info ( \" \\n \" . join ( kubeflow_commands )) prepare_pipeline_deployment ( self , pipeline , stack , runtime_configuration ) Builds a docker image for the current environment and uploads it to a container registry if configured. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def prepare_pipeline_deployment ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , runtime_configuration : \"RuntimeConfiguration\" , ) -> None : \"\"\"Builds a docker image for the current environment and uploads it to a container registry if configured. \"\"\" from zenml.integrations.kubeflow.docker_utils import ( build_docker_image , push_docker_image , ) image_name = self . get_docker_image_name ( pipeline . name ) requirements = { \"kubernetes\" , * stack . requirements ( exclude_components = { StackComponentType . ORCHESTRATOR } ), * self . _get_pipeline_requirements ( pipeline ), } logger . debug ( \"Kubeflow docker container requirements: %s \" , requirements ) build_docker_image ( build_context_path = str ( Repository () . root ), image_name = image_name , dockerignore_path = pipeline . dockerignore_file , requirements = requirements , base_image = self . custom_docker_base_image_name , ) if stack . container_registry : push_docker_image ( image_name ) provision ( self ) Provisions a local Kubeflow Pipelines deployment. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def provision ( self ) -> None : \"\"\"Provisions a local Kubeflow Pipelines deployment.\"\"\" if self . is_running : logger . info ( \"Found already existing local Kubeflow Pipelines deployment. \" \"If there are any issues with the existing deployment, please \" \"run 'zenml orchestrator down' to delete it.\" ) return if not local_deployment_utils . check_prerequisites (): logger . error ( \"Unable to provision local Kubeflow Pipelines deployment: \" \"Please install 'k3d' and 'kubectl' and try again.\" ) return container_registry = Repository () . active_stack . container_registry if not container_registry : logger . error ( \"Unable to provision local Kubeflow Pipelines deployment: \" \"Missing container registry in current stack.\" ) return logger . info ( \"Provisioning local Kubeflow Pipelines deployment...\" ) fileio . make_dirs ( self . root_directory ) container_registry_port = int ( container_registry . uri . split ( \":\" )[ - 1 ]) container_registry_name = self . _get_k3d_registry_name ( port = container_registry_port ) local_deployment_utils . write_local_registry_yaml ( yaml_path = self . _k3d_registry_config_path , registry_name = container_registry_name , registry_uri = container_registry . uri , ) try : local_deployment_utils . create_k3d_cluster ( cluster_name = self . _k3d_cluster_name , registry_name = container_registry_name , registry_config_path = self . _k3d_registry_config_path , ) kubernetes_context = f \"k3d- { self . _k3d_cluster_name } \" local_deployment_utils . deploy_kubeflow_pipelines ( kubernetes_context = kubernetes_context ) port = self . kubeflow_pipelines_ui_port if ( port == DEFAULT_KFP_UI_PORT and not networking_utils . port_available ( port ) ): # if the user didn't specify a specific port and the default # port is occupied, fallback to a random open port port = networking_utils . find_available_port () local_deployment_utils . start_kfp_ui_daemon ( pid_file_path = self . _pid_file_path , log_file_path = self . log_file , port = port , ) except Exception as e : logger . error ( e ) self . list_manual_setup_steps ( container_registry_name , self . _k3d_registry_config_path ) self . deprovision () resume ( self ) Resumes the local k3d cluster. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def resume ( self ) -> None : \"\"\"Resumes the local k3d cluster.\"\"\" if self . is_running : logger . info ( \"Local kubeflow pipelines deployment already running.\" ) return if not self . is_provisioned : raise ProvisioningError ( \"Unable to resume local kubeflow pipelines deployment: No \" \"resources provisioned for local deployment.\" ) local_deployment_utils . start_k3d_cluster ( cluster_name = self . _k3d_cluster_name ) run_pipeline ( self , pipeline , stack , run_name ) Runs a pipeline on Kubeflow Pipelines. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def run_pipeline ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , run_name : str ) -> Any : \"\"\"Runs a pipeline on Kubeflow Pipelines.\"\"\" from zenml.integrations.kubeflow.docker_utils import get_image_digest image_name = self . get_docker_image_name ( pipeline . name ) image_name = get_image_digest ( image_name ) or image_name fileio . make_dirs ( self . pipeline_directory ) pipeline_file_path = os . path . join ( self . pipeline_directory , f \" { pipeline . name } .yaml\" ) runner_config = KubeflowDagRunnerConfig ( image = image_name ) runner = KubeflowDagRunner ( config = runner_config , output_path = pipeline_file_path ) tfx_pipeline = create_tfx_pipeline ( pipeline , stack = stack ) runner . run ( tfx_pipeline ) self . _upload_and_run_pipeline ( pipeline_file_path = pipeline_file_path , run_name = run_name , enable_cache = pipeline . enable_cache , ) suspend ( self ) Suspends the local k3d cluster. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def suspend ( self ) -> None : \"\"\"Suspends the local k3d cluster.\"\"\" if not self . is_running : logger . info ( \"Local kubeflow pipelines deployment not running.\" ) return local_deployment_utils . stop_k3d_cluster ( cluster_name = self . _k3d_cluster_name ) kubeflow_utils Common utility for Kubeflow-based orchestrator. replace_placeholder ( component ) Replaces the RuntimeParameter placeholders with kfp.dsl.PipelineParam. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_utils.py def replace_placeholder ( component : base_node . BaseNode ) -> None : \"\"\"Replaces the RuntimeParameter placeholders with kfp.dsl.PipelineParam.\"\"\" keys = list ( component . exec_properties . keys ()) for key in keys : exec_property = component . exec_properties [ key ] if not isinstance ( exec_property , data_types . RuntimeParameter ): continue component . exec_properties [ key ] = str ( dsl . PipelineParam ( name = exec_property . name ) ) local_deployment_utils check_prerequisites () Checks whether all prerequisites for a local kubeflow pipelines deployment are installed. Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def check_prerequisites () -> bool : \"\"\"Checks whether all prerequisites for a local kubeflow pipelines deployment are installed.\"\"\" k3d_installed = shutil . which ( \"k3d\" ) is not None kubectl_installed = shutil . which ( \"kubectl\" ) is not None logger . debug ( \"Local kubeflow deployment prerequisites: K3D - %s , Kubectl - %s \" , k3d_installed , kubectl_installed , ) return k3d_installed and kubectl_installed create_k3d_cluster ( cluster_name , registry_name , registry_config_path ) Creates a K3D cluster. Parameters: Name Type Description Default cluster_name str Name of the cluster to create. required registry_name str Name of the registry to create for this cluster. required registry_config_path str Path to the registry config file. required Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def create_k3d_cluster ( cluster_name : str , registry_name : str , registry_config_path : str ) -> None : \"\"\"Creates a K3D cluster. Args: cluster_name: Name of the cluster to create. registry_name: Name of the registry to create for this cluster. registry_config_path: Path to the registry config file. \"\"\" logger . info ( \"Creating local K3D cluster ' %s '.\" , cluster_name ) global_config_dir_path = zenml . io . utils . get_global_config_directory () subprocess . check_call ( [ \"k3d\" , \"cluster\" , \"create\" , cluster_name , \"--registry-create\" , registry_name , \"--registry-config\" , registry_config_path , \"--volume\" , f \" { global_config_dir_path } : { global_config_dir_path } \" , ] ) logger . info ( \"Finished K3D cluster creation.\" ) delete_k3d_cluster ( cluster_name ) Deletes a K3D cluster with the given name. Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def delete_k3d_cluster ( cluster_name : str ) -> None : \"\"\"Deletes a K3D cluster with the given name.\"\"\" subprocess . check_call ([ \"k3d\" , \"cluster\" , \"delete\" , cluster_name ]) logger . info ( \"Deleted local k3d cluster ' %s '.\" , cluster_name ) deploy_kubeflow_pipelines ( kubernetes_context ) Deploys Kubeflow Pipelines. Parameters: Name Type Description Default kubernetes_context str The kubernetes context on which Kubeflow Pipelines should be deployed. required Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def deploy_kubeflow_pipelines ( kubernetes_context : str ) -> None : \"\"\"Deploys Kubeflow Pipelines. Args: kubernetes_context: The kubernetes context on which Kubeflow Pipelines should be deployed. \"\"\" logger . info ( \"Deploying Kubeflow Pipelines.\" ) subprocess . check_call ( [ \"kubectl\" , \"--context\" , kubernetes_context , \"apply\" , \"-k\" , f \"github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref= { KFP_VERSION } &timeout=1m\" , ] ) subprocess . check_call ( [ \"kubectl\" , \"--context\" , kubernetes_context , \"wait\" , \"--timeout=60s\" , \"--for\" , \"condition=established\" , \"crd/applications.app.k8s.io\" , ] ) subprocess . check_call ( [ \"kubectl\" , \"--context\" , kubernetes_context , \"apply\" , \"-k\" , f \"github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref= { KFP_VERSION } &timeout=1m\" , ] ) logger . info ( \"Waiting for all Kubeflow Pipelines pods to be ready (this might \" \"take a few minutes).\" ) while True : logger . info ( \"Current pod status:\" ) subprocess . check_call ( [ \"kubectl\" , \"--context\" , kubernetes_context , \"--namespace\" , \"kubeflow\" , \"get\" , \"pods\" , ] ) if kubeflow_pipelines_ready ( kubernetes_context = kubernetes_context ): break logger . info ( \"One or more pods not ready yet, waiting for 30 seconds...\" ) time . sleep ( 30 ) logger . info ( \"Finished Kubeflow Pipelines setup.\" ) k3d_cluster_exists ( cluster_name ) Checks whether there exists a K3D cluster with the given name. Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def k3d_cluster_exists ( cluster_name : str ) -> bool : \"\"\"Checks whether there exists a K3D cluster with the given name.\"\"\" output = subprocess . check_output ( [ \"k3d\" , \"cluster\" , \"list\" , \"--output\" , \"json\" ] ) clusters = json . loads ( output ) for cluster in clusters : if cluster [ \"name\" ] == cluster_name : return True return False k3d_cluster_running ( cluster_name ) Checks whether the K3D cluster with the given name is running. Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def k3d_cluster_running ( cluster_name : str ) -> bool : \"\"\"Checks whether the K3D cluster with the given name is running.\"\"\" output = subprocess . check_output ( [ \"k3d\" , \"cluster\" , \"list\" , \"--output\" , \"json\" ] ) clusters = json . loads ( output ) for cluster in clusters : if cluster [ \"name\" ] == cluster_name : server_count : int = cluster [ \"serversCount\" ] servers_running : int = cluster [ \"serversRunning\" ] return servers_running == server_count return False kubeflow_pipelines_ready ( kubernetes_context ) Returns whether all Kubeflow Pipelines pods are ready. Parameters: Name Type Description Default kubernetes_context str The kubernetes context in which the pods should be checked. required Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def kubeflow_pipelines_ready ( kubernetes_context : str ) -> bool : \"\"\"Returns whether all Kubeflow Pipelines pods are ready. Args: kubernetes_context: The kubernetes context in which the pods should be checked. \"\"\" try : subprocess . check_call ( [ \"kubectl\" , \"--context\" , kubernetes_context , \"--namespace\" , \"kubeflow\" , \"wait\" , \"--for\" , \"condition=ready\" , \"--timeout=0s\" , \"pods\" , \"--all\" , ], stdout = subprocess . DEVNULL , stderr = subprocess . DEVNULL , ) return True except subprocess . CalledProcessError : return False start_k3d_cluster ( cluster_name ) Starts a K3D cluster with the given name. Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def start_k3d_cluster ( cluster_name : str ) -> None : \"\"\"Starts a K3D cluster with the given name.\"\"\" subprocess . check_call ([ \"k3d\" , \"cluster\" , \"start\" , cluster_name ]) logger . info ( \"Started local k3d cluster ' %s '.\" , cluster_name ) start_kfp_ui_daemon ( pid_file_path , log_file_path , port ) Starts a daemon process that forwards ports so the Kubeflow Pipelines UI is accessible in the browser. Parameters: Name Type Description Default pid_file_path str Path where the file with the daemons process ID should be written. required log_file_path str Path to a file where the daemon logs should be written. required port int Port on which the UI should be accessible. required Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def start_kfp_ui_daemon ( pid_file_path : str , log_file_path : str , port : int ) -> None : \"\"\"Starts a daemon process that forwards ports so the Kubeflow Pipelines UI is accessible in the browser. Args: pid_file_path: Path where the file with the daemons process ID should be written. log_file_path: Path to a file where the daemon logs should be written. port: Port on which the UI should be accessible. \"\"\" command = [ \"kubectl\" , \"--namespace\" , \"kubeflow\" , \"port-forward\" , \"svc/ml-pipeline-ui\" , f \" { port } :80\" , ] if not networking_utils . port_available ( port ): modified_command = command . copy () modified_command [ - 1 ] = \"PORT:80\" logger . warning ( \"Unable to port-forward Kubeflow Pipelines UI to local port %d \" \"because the port is occupied. In order to access the Kubeflow \" \"Pipelines UI at http://localhost:PORT/, please run ' %s ' in a \" \"separate command line shell (replace PORT with a free port of \" \"your choice).\" , port , \" \" . join ( modified_command ), ) elif sys . platform == \"win32\" : logger . warning ( \"Daemon functionality not supported on Windows. \" \"In order to access the Kubeflow Pipelines UI at \" \"http://localhost: %d /, please run ' %s ' in a separate command \" \"line shell.\" , port , \" \" . join ( command ), ) else : from zenml.utils import daemon def _daemon_function () -> None : \"\"\"Port-forwards the Kubeflow Pipelines UI pod.\"\"\" subprocess . check_call ( command ) daemon . run_as_daemon ( _daemon_function , pid_file = pid_file_path , log_file = log_file_path ) logger . info ( \"Started Kubeflow Pipelines UI daemon (check the daemon logs at %s \" \"in case you're not able to view the UI). The Kubeflow Pipelines \" \"UI should now be accessible at http://localhost: %d /.\" , log_file_path , port , ) stop_k3d_cluster ( cluster_name ) Stops a K3D cluster with the given name. Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def stop_k3d_cluster ( cluster_name : str ) -> None : \"\"\"Stops a K3D cluster with the given name.\"\"\" subprocess . check_call ([ \"k3d\" , \"cluster\" , \"stop\" , cluster_name ]) logger . info ( \"Stopped local k3d cluster ' %s '.\" , cluster_name ) write_local_registry_yaml ( yaml_path , registry_name , registry_uri ) Writes a K3D registry config file. Parameters: Name Type Description Default yaml_path str Path where the config file should be written to. required registry_name str Name of the registry. required registry_uri str URI of the registry. required Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def write_local_registry_yaml ( yaml_path : str , registry_name : str , registry_uri : str ) -> None : \"\"\"Writes a K3D registry config file. Args: yaml_path: Path where the config file should be written to. registry_name: Name of the registry. registry_uri: URI of the registry. \"\"\" yaml_content = { \"mirrors\" : { registry_uri : { \"endpoint\" : [ f \"http:// { registry_name } \" ]}} } yaml_utils . write_yaml ( yaml_path , yaml_content ) mlflow special The mlflow integrations currently enables you to use mlflow tracking as a convenient way to visualize your experiment runs within the mlflow ui MlflowIntegration ( Integration ) Definition of Plotly integration for ZenML. Source code in zenml/integrations/mlflow/__init__.py class MlflowIntegration ( Integration ): \"\"\"Definition of Plotly integration for ZenML.\"\"\" NAME = MLFLOW REQUIREMENTS = [ \"mlflow>=1.2.0\" ] mlflow_utils enable_mlflow ( _pipeline , experiment_name = None ) Outer decorator function for the creation of a ZenML pipeline with mlflow tracking enabled. In order for a pipeline to run within the context of mlflow, the mlflow experiment should be associated with the pipeline directly. Each separate pipeline run needs to be associated directly with a mlflow experiment. For this, the init and run methods need to be extended accordingly. Parameters: Name Type Description Default _pipeline Type[zenml.pipelines.base_pipeline.BasePipeline] The decorated pipeline required experiment_name Optional[str] Experiment name to use for mlflow None Returns: Type Description Type[zenml.pipelines.base_pipeline.BasePipeline] the inner decorator which has a pipeline with the two methods extended Source code in zenml/integrations/mlflow/mlflow_utils.py def enable_mlflow ( _pipeline : Type [ BasePipeline ], experiment_name : Optional [ str ] = None ) -> Type [ BasePipeline ]: \"\"\"Outer decorator function for the creation of a ZenML pipeline with mlflow tracking enabled. In order for a pipeline to run within the context of mlflow, the mlflow experiment should be associated with the pipeline directly. Each separate pipeline run needs to be associated directly with a mlflow experiment. For this, the __init__ and run methods need to be extended accordingly. Args: _pipeline: The decorated pipeline experiment_name: Experiment name to use for mlflow Returns: the inner decorator which has a pipeline with the two methods extended \"\"\" def inner_decorator ( pipeline : Type [ BasePipeline ]) -> Type [ BasePipeline ]: \"\"\"Inner decorator function for the creation of a ZenML Pipeline with mlflow The __init__ and run method are both extended. Args: pipeline: BasePipeline which will be extended Returns: the class of a newly generated ZenML Pipeline with mlflow \"\"\" # TODO [ENG-369]: Do we need to create a new class here or can we simply # extend the methods of the original pipeline class? return type ( # noqa pipeline . __name__ , ( pipeline ,), { \"__init__\" : enable_mlflow_init ( pipeline . __init__ , experiment_name ), \"run\" : enable_mlflow_run ( pipeline . run ), }, ) return inner_decorator ( _pipeline ) enable_mlflow_init ( original_init , experiment = None ) Outer decorator function for extending the init method for pipelines that should be run using mlflow Parameters: Name Type Description Default original_init Callable[[zenml.pipelines.base_pipeline.BasePipeline, zenml.steps.base_step.BaseStep, Any], NoneType] The init method that should be extended required experiment Optional[str] The users chosen experiment name to use for mlflow None Returns: Type Description Callable[..., NoneType] the inner decorator which extends the init method Source code in zenml/integrations/mlflow/mlflow_utils.py def enable_mlflow_init ( original_init : Callable [[ BasePipeline , BaseStep , Any ], None ], experiment : Optional [ str ] = None , ) -> Callable [ ... , None ]: \"\"\"Outer decorator function for extending the __init__ method for pipelines that should be run using mlflow Args: original_init: The __init__ method that should be extended experiment: The users chosen experiment name to use for mlflow Returns: the inner decorator which extends the __init__ method \"\"\" def inner_decorator ( self : BasePipeline , * args : BaseStep , ** kwargs : Any ) -> None : \"\"\"Inner decorator overwriting the pipeline __init__ Makes sure mlflow is properly set up and all mlflow logging takes place within one mlflow experiment that is associated with the pipeline \"\"\" original_init ( self , * args , ** kwargs ) setup_mlflow ( backend_store_uri = local_mlflow_backend (), experiment_name = experiment if experiment else self . name , ) return inner_decorator enable_mlflow_run ( run ) Outer decorator function for extending the run method for pipelines that should be run using mlflow Parameters: Name Type Description Default run Callable[..., Any] The run method that should be extended required Returns: Type Description Callable[..., Any] the inner decorator which extends the run method Source code in zenml/integrations/mlflow/mlflow_utils.py def enable_mlflow_run ( run : Callable [ ... , Any ]) -> Callable [ ... , Any ]: \"\"\"Outer decorator function for extending the run method for pipelines that should be run using mlflow Args: run: The run method that should be extended Returns: the inner decorator which extends the run method \"\"\" def inner_decorator ( self : BasePipeline , run_name : Optional [ str ] = None ) -> Any : \"\"\"Inner decorator used to extend the run method of a pipeline. This ensures each pipeline run is run within a different mlflow context. Args: self: self of the original pipeline class run_name: Optional name for the run. \"\"\" with mlflow . start_run ( run_name = run_name ): run ( self , run_name ) return inner_decorator local_mlflow_backend () Returns the local mlflow backend inside the global zenml directory Source code in zenml/integrations/mlflow/mlflow_utils.py def local_mlflow_backend () -> str : \"\"\"Returns the local mlflow backend inside the global zenml directory\"\"\" local_mlflow_backend_uri = os . path . join ( get_global_config_directory (), \"local_stores\" , \"mlruns\" ) if not os . path . exists ( local_mlflow_backend_uri ): os . makedirs ( local_mlflow_backend_uri ) # TODO [medium]: safely access (possibly non-existent) artifact stores return \"file:\" + local_mlflow_backend_uri setup_mlflow ( backend_store_uri = None , experiment_name = 'default' ) Setup all mlflow related configurations. This includes specifying which mlflow tracking uri should b e used and which experiment the tracking will be associated with. Parameters: Name Type Description Default backend_store_uri Optional[str] The mlflow backend to log to None experiment_name str The experiment name under which all runs will be tracked 'default' Source code in zenml/integrations/mlflow/mlflow_utils.py def setup_mlflow ( backend_store_uri : Optional [ str ] = None , experiment_name : str = \"default\" ) -> None : \"\"\"Setup all mlflow related configurations. This includes specifying which mlflow tracking uri should b e used and which experiment the tracking will be associated with. Args: backend_store_uri: The mlflow backend to log to experiment_name: The experiment name under which all runs will be tracked \"\"\" # TODO [ENG-316]: Implement a way to get the mlflow token and set # it as env variable at MLFLOW_TRACKING_TOKEN if not backend_store_uri : backend_store_uri = local_mlflow_backend () set_tracking_uri ( backend_store_uri ) # Set which experiment is used within mlflow set_experiment ( experiment_name ) plotly special PlotlyIntegration ( Integration ) Definition of Plotly integration for ZenML. Source code in zenml/integrations/plotly/__init__.py class PlotlyIntegration ( Integration ): \"\"\"Definition of Plotly integration for ZenML.\"\"\" NAME = PLOTLY REQUIREMENTS = [ \"plotly>=5.4.0\" ] visualizers special pipeline_lineage_visualizer PipelineLineageVisualizer ( BasePipelineVisualizer ) Visualize the lineage of runs in a pipeline using plotly. Source code in zenml/integrations/plotly/visualizers/pipeline_lineage_visualizer.py class PipelineLineageVisualizer ( BasePipelineVisualizer ): \"\"\"Visualize the lineage of runs in a pipeline using plotly.\"\"\" @abstractmethod def visualize ( self , object : PipelineView , * args : Any , ** kwargs : Any ) -> Figure : \"\"\"Creates a pipeline lineage diagram using plotly.\"\"\" logger . warning ( \"This integration is not completed yet. Results might be unexpected.\" ) category_dict = {} dimensions = [ \"run\" ] for run in object . runs : category_dict [ run . name ] = { \"run\" : run . name } for step in run . steps : category_dict [ run . name ] . update ( { step . entrypoint_name : str ( step . id ), } ) if step . entrypoint_name not in dimensions : dimensions . append ( f \" { step . entrypoint_name } \" ) category_df = pd . DataFrame . from_dict ( category_dict , orient = \"index\" ) category_df = category_df . reset_index () fig = px . parallel_categories ( category_df , dimensions , color = None , labels = \"status\" , ) fig . show () return fig visualize ( self , object , * args , ** kwargs ) Creates a pipeline lineage diagram using plotly. Source code in zenml/integrations/plotly/visualizers/pipeline_lineage_visualizer.py @abstractmethod def visualize ( self , object : PipelineView , * args : Any , ** kwargs : Any ) -> Figure : \"\"\"Creates a pipeline lineage diagram using plotly.\"\"\" logger . warning ( \"This integration is not completed yet. Results might be unexpected.\" ) category_dict = {} dimensions = [ \"run\" ] for run in object . runs : category_dict [ run . name ] = { \"run\" : run . name } for step in run . steps : category_dict [ run . name ] . update ( { step . entrypoint_name : str ( step . id ), } ) if step . entrypoint_name not in dimensions : dimensions . append ( f \" { step . entrypoint_name } \" ) category_df = pd . DataFrame . from_dict ( category_dict , orient = \"index\" ) category_df = category_df . reset_index () fig = px . parallel_categories ( category_df , dimensions , color = None , labels = \"status\" , ) fig . show () return fig pytorch special PytorchIntegration ( Integration ) Definition of PyTorch integration for ZenML. Source code in zenml/integrations/pytorch/__init__.py class PytorchIntegration ( Integration ): \"\"\"Definition of PyTorch integration for ZenML.\"\"\" NAME = PYTORCH REQUIREMENTS = [ \"torch\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.pytorch import materializers # noqa activate () classmethod Activates the integration. Source code in zenml/integrations/pytorch/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.pytorch import materializers # noqa materializers special pytorch_materializer PyTorchMaterializer ( BaseMaterializer ) Materializer to read/write Pytorch models. Source code in zenml/integrations/pytorch/materializers/pytorch_materializer.py class PyTorchMaterializer ( BaseMaterializer ): \"\"\"Materializer to read/write Pytorch models.\"\"\" ASSOCIATED_TYPES = [ Module , TorchDict ] ASSOCIATED_ARTIFACT_TYPES = [ ModelArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> Union [ Module , TorchDict ]: \"\"\"Reads and returns a PyTorch model. Returns: A loaded pytorch model. \"\"\" super () . handle_input ( data_type ) return torch . load ( os . path . join ( self . artifact . uri , DEFAULT_FILENAME )) # type: ignore[no-untyped-call] # noqa def handle_return ( self , model : Union [ Module , TorchDict ]) -> None : \"\"\"Writes a PyTorch model. Args: model: A torch.nn.Module or a dict to pass into model.save \"\"\" super () . handle_return ( model ) torch . save ( model , os . path . join ( self . artifact . uri , DEFAULT_FILENAME )) handle_input ( self , data_type ) Reads and returns a PyTorch model. Returns: Type Description Union[torch.nn.modules.module.Module, zenml.integrations.pytorch.materializers.pytorch_types.TorchDict] A loaded pytorch model. Source code in zenml/integrations/pytorch/materializers/pytorch_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Union [ Module , TorchDict ]: \"\"\"Reads and returns a PyTorch model. Returns: A loaded pytorch model. \"\"\" super () . handle_input ( data_type ) return torch . load ( os . path . join ( self . artifact . uri , DEFAULT_FILENAME )) # type: ignore[no-untyped-call] # noqa handle_return ( self , model ) Writes a PyTorch model. Parameters: Name Type Description Default model Union[torch.nn.modules.module.Module, zenml.integrations.pytorch.materializers.pytorch_types.TorchDict] A torch.nn.Module or a dict to pass into model.save required Source code in zenml/integrations/pytorch/materializers/pytorch_materializer.py def handle_return ( self , model : Union [ Module , TorchDict ]) -> None : \"\"\"Writes a PyTorch model. Args: model: A torch.nn.Module or a dict to pass into model.save \"\"\" super () . handle_return ( model ) torch . save ( model , os . path . join ( self . artifact . uri , DEFAULT_FILENAME )) pytorch_types TorchDict ( dict , Generic ) A type of dict that represents saving a model. Source code in zenml/integrations/pytorch/materializers/pytorch_types.py class TorchDict ( Dict [ str , Any ]): \"\"\"A type of dict that represents saving a model.\"\"\" pytorch_lightning special PytorchLightningIntegration ( Integration ) Definition of PyTorch Lightning integration for ZenML. Source code in zenml/integrations/pytorch_lightning/__init__.py class PytorchLightningIntegration ( Integration ): \"\"\"Definition of PyTorch Lightning integration for ZenML.\"\"\" NAME = PYTORCH_L REQUIREMENTS = [ \"pytorch_lightning\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.pytorch_lightning import materializers # noqa activate () classmethod Activates the integration. Source code in zenml/integrations/pytorch_lightning/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.pytorch_lightning import materializers # noqa materializers special pytorch_lightning_materializer PyTorchLightningMaterializer ( BaseMaterializer ) Materializer to read/write Pytorch models. Source code in zenml/integrations/pytorch_lightning/materializers/pytorch_lightning_materializer.py class PyTorchLightningMaterializer ( BaseMaterializer ): \"\"\"Materializer to read/write Pytorch models.\"\"\" ASSOCIATED_TYPES = [ Trainer ] ASSOCIATED_ARTIFACT_TYPES = [ ModelArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> Trainer : \"\"\"Reads and returns a PyTorch Lightning trainer. Returns: A PyTorch Lightning trainer object. \"\"\" super () . handle_input ( data_type ) return Trainer ( resume_from_checkpoint = os . path . join ( self . artifact . uri , CHECKPOINT_NAME ) ) def handle_return ( self , trainer : Trainer ) -> None : \"\"\"Writes a PyTorch Lightning trainer. Args: trainer: A PyTorch Lightning trainer object. \"\"\" super () . handle_return ( trainer ) trainer . save_checkpoint ( os . path . join ( self . artifact . uri , CHECKPOINT_NAME ) ) handle_input ( self , data_type ) Reads and returns a PyTorch Lightning trainer. Returns: Type Description Trainer A PyTorch Lightning trainer object. Source code in zenml/integrations/pytorch_lightning/materializers/pytorch_lightning_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Trainer : \"\"\"Reads and returns a PyTorch Lightning trainer. Returns: A PyTorch Lightning trainer object. \"\"\" super () . handle_input ( data_type ) return Trainer ( resume_from_checkpoint = os . path . join ( self . artifact . uri , CHECKPOINT_NAME ) ) handle_return ( self , trainer ) Writes a PyTorch Lightning trainer. Parameters: Name Type Description Default trainer Trainer A PyTorch Lightning trainer object. required Source code in zenml/integrations/pytorch_lightning/materializers/pytorch_lightning_materializer.py def handle_return ( self , trainer : Trainer ) -> None : \"\"\"Writes a PyTorch Lightning trainer. Args: trainer: A PyTorch Lightning trainer object. \"\"\" super () . handle_return ( trainer ) trainer . save_checkpoint ( os . path . join ( self . artifact . uri , CHECKPOINT_NAME ) ) registry IntegrationRegistry Registry to keep track of ZenML Integrations Source code in zenml/integrations/registry.py class IntegrationRegistry ( object ): \"\"\"Registry to keep track of ZenML Integrations\"\"\" def __init__ ( self ) -> None : \"\"\"Initializing the integration registry\"\"\" self . _integrations : Dict [ str , Type [ \"Integration\" ]] = {} @property def integrations ( self ) -> Dict [ str , Type [ \"Integration\" ]]: \"\"\"Method to get integrations dictionary. Returns: A dict of integration key to type of `Integration`. \"\"\" return self . _integrations @integrations . setter def integrations ( self , i : Any ) -> None : \"\"\"Setter method for the integrations property\"\"\" raise IntegrationError ( \"Please do not manually change the integrations within the \" \"registry. If you would like to register a new integration \" \"manually, please use \" \"`integration_registry.register_integration()`.\" ) def register_integration ( self , key : str , type_ : Type [ \"Integration\" ] ) -> None : \"\"\"Method to register an integration with a given name\"\"\" self . _integrations [ key ] = type_ def activate_integrations ( self ) -> None : \"\"\"Method to activate the integrations with are registered in the registry\"\"\" for name , integration in self . _integrations . items (): if integration . check_installation (): integration . activate () logger . debug ( f \"Integration ` { name } ` is activated.\" ) else : logger . debug ( f \"Integration ` { name } ` could not be activated.\" ) @property def list_integration_names ( self ) -> List [ str ]: \"\"\"Get a list of all possible integrations\"\"\" return [ name for name in self . _integrations ] def select_integration_requirements ( self , integration_name : Optional [ str ] = None ) -> List [ str ]: \"\"\"Select the requirements for a given integration or all integrations\"\"\" if integration_name : if integration_name in self . list_integration_names : return self . _integrations [ integration_name ] . REQUIREMENTS else : raise KeyError ( f \"Version { integration_name } does not exist. \" f \"Currently the following integrations are implemented. \" f \" { self . list_integration_names } \" ) else : return [ requirement for name in self . list_integration_names for requirement in self . _integrations [ name ] . REQUIREMENTS ] def is_installed ( self , integration_name : Optional [ str ] = None ) -> bool : \"\"\"Checks if all requirements for an integration are installed\"\"\" if integration_name in self . list_integration_names : return self . _integrations [ integration_name ] . check_installation () elif not integration_name : all_installed = [ self . _integrations [ item ] . check_installation () for item in self . list_integration_names ] return any ( all_installed ) else : raise KeyError ( f \"Integration ' { integration_name } ' not found. \" f \"Currently the following integrations are available: \" f \" { self . list_integration_names } \" ) integrations : Dict [ str , Type [ Integration ]] property writable Method to get integrations dictionary. Returns: Type Description Dict[str, Type[Integration]] A dict of integration key to type of Integration . list_integration_names : List [ str ] property readonly Get a list of all possible integrations __init__ ( self ) special Initializing the integration registry Source code in zenml/integrations/registry.py def __init__ ( self ) -> None : \"\"\"Initializing the integration registry\"\"\" self . _integrations : Dict [ str , Type [ \"Integration\" ]] = {} activate_integrations ( self ) Method to activate the integrations with are registered in the registry Source code in zenml/integrations/registry.py def activate_integrations ( self ) -> None : \"\"\"Method to activate the integrations with are registered in the registry\"\"\" for name , integration in self . _integrations . items (): if integration . check_installation (): integration . activate () logger . debug ( f \"Integration ` { name } ` is activated.\" ) else : logger . debug ( f \"Integration ` { name } ` could not be activated.\" ) is_installed ( self , integration_name = None ) Checks if all requirements for an integration are installed Source code in zenml/integrations/registry.py def is_installed ( self , integration_name : Optional [ str ] = None ) -> bool : \"\"\"Checks if all requirements for an integration are installed\"\"\" if integration_name in self . list_integration_names : return self . _integrations [ integration_name ] . check_installation () elif not integration_name : all_installed = [ self . _integrations [ item ] . check_installation () for item in self . list_integration_names ] return any ( all_installed ) else : raise KeyError ( f \"Integration ' { integration_name } ' not found. \" f \"Currently the following integrations are available: \" f \" { self . list_integration_names } \" ) register_integration ( self , key , type_ ) Method to register an integration with a given name Source code in zenml/integrations/registry.py def register_integration ( self , key : str , type_ : Type [ \"Integration\" ] ) -> None : \"\"\"Method to register an integration with a given name\"\"\" self . _integrations [ key ] = type_ select_integration_requirements ( self , integration_name = None ) Select the requirements for a given integration or all integrations Source code in zenml/integrations/registry.py def select_integration_requirements ( self , integration_name : Optional [ str ] = None ) -> List [ str ]: \"\"\"Select the requirements for a given integration or all integrations\"\"\" if integration_name : if integration_name in self . list_integration_names : return self . _integrations [ integration_name ] . REQUIREMENTS else : raise KeyError ( f \"Version { integration_name } does not exist. \" f \"Currently the following integrations are implemented. \" f \" { self . list_integration_names } \" ) else : return [ requirement for name in self . list_integration_names for requirement in self . _integrations [ name ] . REQUIREMENTS ] sklearn special SklearnIntegration ( Integration ) Definition of sklearn integration for ZenML. Source code in zenml/integrations/sklearn/__init__.py class SklearnIntegration ( Integration ): \"\"\"Definition of sklearn integration for ZenML.\"\"\" NAME = SKLEARN REQUIREMENTS = [ \"scikit-learn\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.sklearn import materializers # noqa activate () classmethod Activates the integration. Source code in zenml/integrations/sklearn/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.sklearn import materializers # noqa helpers special digits get_digits () Returns the digits dataset in the form of a tuple of numpy arrays. Source code in zenml/integrations/sklearn/helpers/digits.py def get_digits () -> Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]: \"\"\"Returns the digits dataset in the form of a tuple of numpy arrays.\"\"\" digits = load_digits () # flatten the images n_samples = len ( digits . images ) data = digits . images . reshape (( n_samples , - 1 )) # Split data into 50% train and 50% test subsets X_train , X_test , y_train , y_test = train_test_split ( data , digits . target , test_size = 0.5 , shuffle = False ) return X_train , X_test , y_train , y_test get_digits_model () Creates a support vector classifier for digits dataset. Source code in zenml/integrations/sklearn/helpers/digits.py def get_digits_model () -> ClassifierMixin : \"\"\"Creates a support vector classifier for digits dataset.\"\"\" return SVC ( gamma = 0.001 ) materializers special sklearn_materializer SklearnMaterializer ( BaseMaterializer ) Materializer to read data to and from sklearn. Source code in zenml/integrations/sklearn/materializers/sklearn_materializer.py class SklearnMaterializer ( BaseMaterializer ): \"\"\"Materializer to read data to and from sklearn.\"\"\" ASSOCIATED_TYPES = [ BaseEstimator , ClassifierMixin , ClusterMixin , BiclusterMixin , OutlierMixin , RegressorMixin , MetaEstimatorMixin , MultiOutputMixin , DensityMixin , TransformerMixin , ] ASSOCIATED_ARTIFACT_TYPES = [ ModelArtifact ] def handle_input ( self , data_type : Type [ Any ] ) -> Union [ BaseEstimator , ClassifierMixin , ClusterMixin , BiclusterMixin , OutlierMixin , RegressorMixin , MetaEstimatorMixin , MultiOutputMixin , DensityMixin , TransformerMixin , ]: \"\"\"Reads a base sklearn model from a pickle file.\"\"\" super () . handle_input ( data_type ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) with fileio . open ( filepath , \"rb\" ) as fid : clf = pickle . load ( fid ) return clf def handle_return ( self , clf : Union [ BaseEstimator , ClassifierMixin , ClusterMixin , BiclusterMixin , OutlierMixin , RegressorMixin , MetaEstimatorMixin , MultiOutputMixin , DensityMixin , TransformerMixin , ], ) -> None : \"\"\"Creates a pickle for a sklearn model. Args: clf: A sklearn model. \"\"\" super () . handle_return ( clf ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) with fileio . open ( filepath , \"wb\" ) as fid : pickle . dump ( clf , fid ) handle_input ( self , data_type ) Reads a base sklearn model from a pickle file. Source code in zenml/integrations/sklearn/materializers/sklearn_materializer.py def handle_input ( self , data_type : Type [ Any ] ) -> Union [ BaseEstimator , ClassifierMixin , ClusterMixin , BiclusterMixin , OutlierMixin , RegressorMixin , MetaEstimatorMixin , MultiOutputMixin , DensityMixin , TransformerMixin , ]: \"\"\"Reads a base sklearn model from a pickle file.\"\"\" super () . handle_input ( data_type ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) with fileio . open ( filepath , \"rb\" ) as fid : clf = pickle . load ( fid ) return clf handle_return ( self , clf ) Creates a pickle for a sklearn model. Parameters: Name Type Description Default clf Union[sklearn.base.BaseEstimator, sklearn.base.ClassifierMixin, sklearn.base.ClusterMixin, sklearn.base.BiclusterMixin, sklearn.base.OutlierMixin, sklearn.base.RegressorMixin, sklearn.base.MetaEstimatorMixin, sklearn.base.MultiOutputMixin, sklearn.base.DensityMixin, sklearn.base.TransformerMixin] A sklearn model. required Source code in zenml/integrations/sklearn/materializers/sklearn_materializer.py def handle_return ( self , clf : Union [ BaseEstimator , ClassifierMixin , ClusterMixin , BiclusterMixin , OutlierMixin , RegressorMixin , MetaEstimatorMixin , MultiOutputMixin , DensityMixin , TransformerMixin , ], ) -> None : \"\"\"Creates a pickle for a sklearn model. Args: clf: A sklearn model. \"\"\" super () . handle_return ( clf ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) with fileio . open ( filepath , \"wb\" ) as fid : pickle . dump ( clf , fid ) steps special sklearn_evaluator SklearnEvaluator ( BaseEvaluatorStep ) A simple step implementation which utilizes sklearn to evaluate the performance of a given model on a given test dataset Source code in zenml/integrations/sklearn/steps/sklearn_evaluator.py class SklearnEvaluator ( BaseEvaluatorStep ): \"\"\"A simple step implementation which utilizes sklearn to evaluate the performance of a given model on a given test dataset\"\"\" def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , model : tf . keras . Model , config : SklearnEvaluatorConfig , ) -> dict : # type: ignore[type-arg] \"\"\"Method which is responsible for the computation of the evaluation Args: dataset: a pandas Dataframe which represents the test dataset model: a trained tensorflow Keras model config: the configuration for the step Returns: a dictionary which has the evaluation report \"\"\" labels = dataset . pop ( config . label_class_column ) predictions = model . predict ( dataset ) predicted_classes = [ 1 if v > 0.5 else 0 for v in predictions ] report = classification_report ( labels , predicted_classes , output_dict = True ) return report # type: ignore[no-any-return] CONFIG_CLASS ( BaseEvaluatorConfig ) pydantic-model Config class for the sklearn evaluator Source code in zenml/integrations/sklearn/steps/sklearn_evaluator.py class SklearnEvaluatorConfig ( BaseEvaluatorConfig ): \"\"\"Config class for the sklearn evaluator\"\"\" label_class_column : str entrypoint ( self , dataset , model , config ) Method which is responsible for the computation of the evaluation Parameters: Name Type Description Default dataset DataFrame a pandas Dataframe which represents the test dataset required model Model a trained tensorflow Keras model required config SklearnEvaluatorConfig the configuration for the step required Returns: Type Description dict a dictionary which has the evaluation report Source code in zenml/integrations/sklearn/steps/sklearn_evaluator.py def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , model : tf . keras . Model , config : SklearnEvaluatorConfig , ) -> dict : # type: ignore[type-arg] \"\"\"Method which is responsible for the computation of the evaluation Args: dataset: a pandas Dataframe which represents the test dataset model: a trained tensorflow Keras model config: the configuration for the step Returns: a dictionary which has the evaluation report \"\"\" labels = dataset . pop ( config . label_class_column ) predictions = model . predict ( dataset ) predicted_classes = [ 1 if v > 0.5 else 0 for v in predictions ] report = classification_report ( labels , predicted_classes , output_dict = True ) return report # type: ignore[no-any-return] SklearnEvaluatorConfig ( BaseEvaluatorConfig ) pydantic-model Config class for the sklearn evaluator Source code in zenml/integrations/sklearn/steps/sklearn_evaluator.py class SklearnEvaluatorConfig ( BaseEvaluatorConfig ): \"\"\"Config class for the sklearn evaluator\"\"\" label_class_column : str sklearn_splitter SklearnSplitter ( BaseSplitStep ) A simple step implementation which utilizes sklearn to split a given dataset into train, test and validation splits Source code in zenml/integrations/sklearn/steps/sklearn_splitter.py class SklearnSplitter ( BaseSplitStep ): \"\"\"A simple step implementation which utilizes sklearn to split a given dataset into train, test and validation splits\"\"\" def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , config : SklearnSplitterConfig , ) -> Output ( # type:ignore[valid-type] train = pd . DataFrame , test = pd . DataFrame , validation = pd . DataFrame ): \"\"\"Method which is responsible for the splitting logic Args: dataset: a pandas Dataframe which entire dataset config: the configuration for the step Returns: three dataframes representing the splits \"\"\" if ( any ( [ split not in config . ratios for split in [ \"train\" , \"test\" , \"validation\" ] ] ) or len ( config . ratios ) != 3 ): raise KeyError ( f \"Make sure that you only use 'train', 'test' and \" f \"'validation' as keys in the ratios dict. Current keys: \" f \" { config . ratios . keys () } \" ) if sum ( config . ratios . values ()) != 1 : raise ValueError ( f \"Make sure that the ratios sum up to 1. Current \" f \"ratios: { config . ratios } \" ) train_dataset , test_dataset = train_test_split ( dataset , test_size = config . ratios [ \"test\" ] ) train_dataset , val_dataset = train_test_split ( train_dataset , test_size = ( config . ratios [ \"validation\" ] / ( config . ratios [ \"validation\" ] + config . ratios [ \"train\" ]) ), ) return train_dataset , test_dataset , val_dataset CONFIG_CLASS ( BaseSplitStepConfig ) pydantic-model Config class for the sklearn splitter Source code in zenml/integrations/sklearn/steps/sklearn_splitter.py class SklearnSplitterConfig ( BaseSplitStepConfig ): \"\"\"Config class for the sklearn splitter\"\"\" ratios : Dict [ str , float ] entrypoint ( self , dataset , config ) Method which is responsible for the splitting logic Parameters: Name Type Description Default dataset DataFrame a pandas Dataframe which entire dataset required config SklearnSplitterConfig the configuration for the step required Returns: Type Description <zenml.steps.step_output.Output object at 0x7f2d9364ca60> three dataframes representing the splits Source code in zenml/integrations/sklearn/steps/sklearn_splitter.py def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , config : SklearnSplitterConfig , ) -> Output ( # type:ignore[valid-type] train = pd . DataFrame , test = pd . DataFrame , validation = pd . DataFrame ): \"\"\"Method which is responsible for the splitting logic Args: dataset: a pandas Dataframe which entire dataset config: the configuration for the step Returns: three dataframes representing the splits \"\"\" if ( any ( [ split not in config . ratios for split in [ \"train\" , \"test\" , \"validation\" ] ] ) or len ( config . ratios ) != 3 ): raise KeyError ( f \"Make sure that you only use 'train', 'test' and \" f \"'validation' as keys in the ratios dict. Current keys: \" f \" { config . ratios . keys () } \" ) if sum ( config . ratios . values ()) != 1 : raise ValueError ( f \"Make sure that the ratios sum up to 1. Current \" f \"ratios: { config . ratios } \" ) train_dataset , test_dataset = train_test_split ( dataset , test_size = config . ratios [ \"test\" ] ) train_dataset , val_dataset = train_test_split ( train_dataset , test_size = ( config . ratios [ \"validation\" ] / ( config . ratios [ \"validation\" ] + config . ratios [ \"train\" ]) ), ) return train_dataset , test_dataset , val_dataset SklearnSplitterConfig ( BaseSplitStepConfig ) pydantic-model Config class for the sklearn splitter Source code in zenml/integrations/sklearn/steps/sklearn_splitter.py class SklearnSplitterConfig ( BaseSplitStepConfig ): \"\"\"Config class for the sklearn splitter\"\"\" ratios : Dict [ str , float ] sklearn_standard_scaler SklearnStandardScaler ( BasePreprocessorStep ) Simple step implementation which utilizes the StandardScaler from sklearn to transform the numeric columns of a pd.DataFrame Source code in zenml/integrations/sklearn/steps/sklearn_standard_scaler.py class SklearnStandardScaler ( BasePreprocessorStep ): \"\"\"Simple step implementation which utilizes the StandardScaler from sklearn to transform the numeric columns of a pd.DataFrame\"\"\" def entrypoint ( # type: ignore[override] self , train_dataset : pd . DataFrame , test_dataset : pd . DataFrame , validation_dataset : pd . DataFrame , statistics : pd . DataFrame , schema : pd . DataFrame , config : SklearnStandardScalerConfig , ) -> Output ( # type:ignore[valid-type] train_transformed = pd . DataFrame , test_transformed = pd . DataFrame , validation_transformed = pd . DataFrame , ): \"\"\"Main entrypoint function for the StandardScaler Args: train_dataset: pd.DataFrame, the training dataset test_dataset: pd.DataFrame, the test dataset validation_dataset: pd.DataFrame, the validation dataset statistics: pd.DataFrame, the statistics over the train dataset schema: pd.DataFrame, the detected schema of the dataset config: the configuration for the step Returns: the transformed train, test and validation datasets as pd.DataFrames \"\"\" schema_dict = { k : v [ 0 ] for k , v in schema . to_dict () . items ()} # Exclude columns feature_set = set ( train_dataset . columns ) - set ( config . exclude_columns ) for feature , feature_type in schema_dict . items (): if feature_type != \"int64\" and feature_type != \"float64\" : feature_set . remove ( feature ) logger . warning ( f \" { feature } column is a not numeric, thus it is excluded \" f \"from the standard scaling.\" ) transform_feature_set = feature_set - set ( config . ignore_columns ) # Transform the datasets scaler = StandardScaler () scaler . mean_ = statistics [ \"mean\" ][ transform_feature_set ] scaler . scale_ = statistics [ \"std\" ][ transform_feature_set ] train_dataset [ list ( transform_feature_set )] = scaler . transform ( train_dataset [ transform_feature_set ] ) test_dataset [ list ( transform_feature_set )] = scaler . transform ( test_dataset [ transform_feature_set ] ) validation_dataset [ list ( transform_feature_set )] = scaler . transform ( validation_dataset [ transform_feature_set ] ) return train_dataset , test_dataset , validation_dataset CONFIG_CLASS ( BasePreprocessorConfig ) pydantic-model Config class for the sklearn standard scaler ignore_columns: a list of column names which should not be scaled exclude_columns: a list of column names to be excluded from the dataset Source code in zenml/integrations/sklearn/steps/sklearn_standard_scaler.py class SklearnStandardScalerConfig ( BasePreprocessorConfig ): \"\"\"Config class for the sklearn standard scaler ignore_columns: a list of column names which should not be scaled exclude_columns: a list of column names to be excluded from the dataset \"\"\" ignore_columns : List [ str ] = [] exclude_columns : List [ str ] = [] entrypoint ( self , train_dataset , test_dataset , validation_dataset , statistics , schema , config ) Main entrypoint function for the StandardScaler Parameters: Name Type Description Default train_dataset DataFrame pd.DataFrame, the training dataset required test_dataset DataFrame pd.DataFrame, the test dataset required validation_dataset DataFrame pd.DataFrame, the validation dataset required statistics DataFrame pd.DataFrame, the statistics over the train dataset required schema DataFrame pd.DataFrame, the detected schema of the dataset required config SklearnStandardScalerConfig the configuration for the step required Returns: Type Description <zenml.steps.step_output.Output object at 0x7f2d93663d00> the transformed train, test and validation datasets as pd.DataFrames Source code in zenml/integrations/sklearn/steps/sklearn_standard_scaler.py def entrypoint ( # type: ignore[override] self , train_dataset : pd . DataFrame , test_dataset : pd . DataFrame , validation_dataset : pd . DataFrame , statistics : pd . DataFrame , schema : pd . DataFrame , config : SklearnStandardScalerConfig , ) -> Output ( # type:ignore[valid-type] train_transformed = pd . DataFrame , test_transformed = pd . DataFrame , validation_transformed = pd . DataFrame , ): \"\"\"Main entrypoint function for the StandardScaler Args: train_dataset: pd.DataFrame, the training dataset test_dataset: pd.DataFrame, the test dataset validation_dataset: pd.DataFrame, the validation dataset statistics: pd.DataFrame, the statistics over the train dataset schema: pd.DataFrame, the detected schema of the dataset config: the configuration for the step Returns: the transformed train, test and validation datasets as pd.DataFrames \"\"\" schema_dict = { k : v [ 0 ] for k , v in schema . to_dict () . items ()} # Exclude columns feature_set = set ( train_dataset . columns ) - set ( config . exclude_columns ) for feature , feature_type in schema_dict . items (): if feature_type != \"int64\" and feature_type != \"float64\" : feature_set . remove ( feature ) logger . warning ( f \" { feature } column is a not numeric, thus it is excluded \" f \"from the standard scaling.\" ) transform_feature_set = feature_set - set ( config . ignore_columns ) # Transform the datasets scaler = StandardScaler () scaler . mean_ = statistics [ \"mean\" ][ transform_feature_set ] scaler . scale_ = statistics [ \"std\" ][ transform_feature_set ] train_dataset [ list ( transform_feature_set )] = scaler . transform ( train_dataset [ transform_feature_set ] ) test_dataset [ list ( transform_feature_set )] = scaler . transform ( test_dataset [ transform_feature_set ] ) validation_dataset [ list ( transform_feature_set )] = scaler . transform ( validation_dataset [ transform_feature_set ] ) return train_dataset , test_dataset , validation_dataset SklearnStandardScalerConfig ( BasePreprocessorConfig ) pydantic-model Config class for the sklearn standard scaler ignore_columns: a list of column names which should not be scaled exclude_columns: a list of column names to be excluded from the dataset Source code in zenml/integrations/sklearn/steps/sklearn_standard_scaler.py class SklearnStandardScalerConfig ( BasePreprocessorConfig ): \"\"\"Config class for the sklearn standard scaler ignore_columns: a list of column names which should not be scaled exclude_columns: a list of column names to be excluded from the dataset \"\"\" ignore_columns : List [ str ] = [] exclude_columns : List [ str ] = [] tensorflow special TensorflowIntegration ( Integration ) Definition of Tensorflow integration for ZenML. Source code in zenml/integrations/tensorflow/__init__.py class TensorflowIntegration ( Integration ): \"\"\"Definition of Tensorflow integration for ZenML.\"\"\" NAME = TENSORFLOW REQUIREMENTS = [ \"tensorflow\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.tensorflow import materializers # noqa activate () classmethod Activates the integration. Source code in zenml/integrations/tensorflow/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.tensorflow import materializers # noqa materializers special keras_materializer KerasMaterializer ( BaseMaterializer ) Materializer to read/write Keras models. Source code in zenml/integrations/tensorflow/materializers/keras_materializer.py class KerasMaterializer ( BaseMaterializer ): \"\"\"Materializer to read/write Keras models.\"\"\" ASSOCIATED_TYPES = [ keras . Model ] ASSOCIATED_ARTIFACT_TYPES = [ ModelArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> keras . Model : \"\"\"Reads and returns a Keras model. Returns: A tf.keras.Model model. \"\"\" super () . handle_input ( data_type ) return keras . models . load_model ( self . artifact . uri ) def handle_return ( self , model : keras . Model ) -> None : \"\"\"Writes a keras model. Args: model: A tf.keras.Model model. \"\"\" super () . handle_return ( model ) model . save ( self . artifact . uri ) handle_input ( self , data_type ) Reads and returns a Keras model. Returns: Type Description Model A tf.keras.Model model. Source code in zenml/integrations/tensorflow/materializers/keras_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> keras . Model : \"\"\"Reads and returns a Keras model. Returns: A tf.keras.Model model. \"\"\" super () . handle_input ( data_type ) return keras . models . load_model ( self . artifact . uri ) handle_return ( self , model ) Writes a keras model. Parameters: Name Type Description Default model Model A tf.keras.Model model. required Source code in zenml/integrations/tensorflow/materializers/keras_materializer.py def handle_return ( self , model : keras . Model ) -> None : \"\"\"Writes a keras model. Args: model: A tf.keras.Model model. \"\"\" super () . handle_return ( model ) model . save ( self . artifact . uri ) tf_dataset_materializer TensorflowDatasetMaterializer ( BaseMaterializer ) Materializer to read data to and from tf.data.Dataset. Source code in zenml/integrations/tensorflow/materializers/tf_dataset_materializer.py class TensorflowDatasetMaterializer ( BaseMaterializer ): \"\"\"Materializer to read data to and from tf.data.Dataset.\"\"\" ASSOCIATED_TYPES = [ tf . data . Dataset ] ASSOCIATED_ARTIFACT_TYPES = [ DataArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads data into tf.data.Dataset\"\"\" super () . handle_input ( data_type ) path = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) return tf . data . experimental . load ( path ) def handle_return ( self , dataset : tf . data . Dataset ) -> None : \"\"\"Persists a tf.data.Dataset object.\"\"\" super () . handle_return ( dataset ) path = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) tf . data . experimental . save ( dataset , path , compression = None , shard_func = None ) handle_input ( self , data_type ) Reads data into tf.data.Dataset Source code in zenml/integrations/tensorflow/materializers/tf_dataset_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads data into tf.data.Dataset\"\"\" super () . handle_input ( data_type ) path = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) return tf . data . experimental . load ( path ) handle_return ( self , dataset ) Persists a tf.data.Dataset object. Source code in zenml/integrations/tensorflow/materializers/tf_dataset_materializer.py def handle_return ( self , dataset : tf . data . Dataset ) -> None : \"\"\"Persists a tf.data.Dataset object.\"\"\" super () . handle_return ( dataset ) path = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) tf . data . experimental . save ( dataset , path , compression = None , shard_func = None ) steps special tensorflow_trainer TensorflowBinaryClassifier ( BaseTrainerStep ) Simple step implementation which creates a simple tensorflow feedforward neural network and trains it on a given pd.DataFrame dataset Source code in zenml/integrations/tensorflow/steps/tensorflow_trainer.py class TensorflowBinaryClassifier ( BaseTrainerStep ): \"\"\"Simple step implementation which creates a simple tensorflow feedforward neural network and trains it on a given pd.DataFrame dataset \"\"\" def entrypoint ( # type: ignore[override] self , train_dataset : pd . DataFrame , validation_dataset : pd . DataFrame , config : TensorflowBinaryClassifierConfig , ) -> tf . keras . Model : \"\"\"Main entrypoint for the tensorflow trainer Args: train_dataset: pd.DataFrame, the training dataset validation_dataset: pd.DataFrame, the validation dataset config: the configuration of the step Returns: the trained tf.keras.Model \"\"\" model = tf . keras . Sequential () model . add ( tf . keras . layers . InputLayer ( input_shape = config . input_shape )) model . add ( tf . keras . layers . Flatten ()) last_layer = config . layers . pop () for i , layer in enumerate ( config . layers ): model . add ( tf . keras . layers . Dense ( layer , activation = \"relu\" )) model . add ( tf . keras . layers . Dense ( last_layer , activation = \"sigmoid\" )) model . compile ( optimizer = tf . keras . optimizers . Adam ( config . learning_rate ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = config . metrics , ) train_target = train_dataset . pop ( config . target_column ) validation_target = validation_dataset . pop ( config . target_column ) model . fit ( x = train_dataset , y = train_target , validation_data = ( validation_dataset , validation_target ), batch_size = config . batch_size , epochs = config . epochs , ) model . summary () return model CONFIG_CLASS ( BaseTrainerConfig ) pydantic-model Config class for the tensorflow trainer target_column: the name of the label column layers: the number of units in the fully connected layers input_shape: the shape of the input learning_rate: the learning rate metrics: the list of metrics to be computed epochs: the number of epochs batch_size: the size of the batch Source code in zenml/integrations/tensorflow/steps/tensorflow_trainer.py class TensorflowBinaryClassifierConfig ( BaseTrainerConfig ): \"\"\"Config class for the tensorflow trainer target_column: the name of the label column layers: the number of units in the fully connected layers input_shape: the shape of the input learning_rate: the learning rate metrics: the list of metrics to be computed epochs: the number of epochs batch_size: the size of the batch \"\"\" target_column : str layers : List [ int ] = [ 256 , 64 , 1 ] input_shape : Tuple [ int ] = ( 8 ,) learning_rate : float = 0.001 metrics : List [ str ] = [ \"accuracy\" ] epochs : int = 50 batch_size : int = 8 entrypoint ( self , train_dataset , validation_dataset , config ) Main entrypoint for the tensorflow trainer Parameters: Name Type Description Default train_dataset DataFrame pd.DataFrame, the training dataset required validation_dataset DataFrame pd.DataFrame, the validation dataset required config TensorflowBinaryClassifierConfig the configuration of the step required Returns: Type Description Model the trained tf.keras.Model Source code in zenml/integrations/tensorflow/steps/tensorflow_trainer.py def entrypoint ( # type: ignore[override] self , train_dataset : pd . DataFrame , validation_dataset : pd . DataFrame , config : TensorflowBinaryClassifierConfig , ) -> tf . keras . Model : \"\"\"Main entrypoint for the tensorflow trainer Args: train_dataset: pd.DataFrame, the training dataset validation_dataset: pd.DataFrame, the validation dataset config: the configuration of the step Returns: the trained tf.keras.Model \"\"\" model = tf . keras . Sequential () model . add ( tf . keras . layers . InputLayer ( input_shape = config . input_shape )) model . add ( tf . keras . layers . Flatten ()) last_layer = config . layers . pop () for i , layer in enumerate ( config . layers ): model . add ( tf . keras . layers . Dense ( layer , activation = \"relu\" )) model . add ( tf . keras . layers . Dense ( last_layer , activation = \"sigmoid\" )) model . compile ( optimizer = tf . keras . optimizers . Adam ( config . learning_rate ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = config . metrics , ) train_target = train_dataset . pop ( config . target_column ) validation_target = validation_dataset . pop ( config . target_column ) model . fit ( x = train_dataset , y = train_target , validation_data = ( validation_dataset , validation_target ), batch_size = config . batch_size , epochs = config . epochs , ) model . summary () return model TensorflowBinaryClassifierConfig ( BaseTrainerConfig ) pydantic-model Config class for the tensorflow trainer target_column: the name of the label column layers: the number of units in the fully connected layers input_shape: the shape of the input learning_rate: the learning rate metrics: the list of metrics to be computed epochs: the number of epochs batch_size: the size of the batch Source code in zenml/integrations/tensorflow/steps/tensorflow_trainer.py class TensorflowBinaryClassifierConfig ( BaseTrainerConfig ): \"\"\"Config class for the tensorflow trainer target_column: the name of the label column layers: the number of units in the fully connected layers input_shape: the shape of the input learning_rate: the learning rate metrics: the list of metrics to be computed epochs: the number of epochs batch_size: the size of the batch \"\"\" target_column : str layers : List [ int ] = [ 256 , 64 , 1 ] input_shape : Tuple [ int ] = ( 8 ,) learning_rate : float = 0.001 metrics : List [ str ] = [ \"accuracy\" ] epochs : int = 50 batch_size : int = 8 utils get_integration_for_module ( module_name ) Gets the integration class for a module inside an integration. If the module given by module_name is not part of a ZenML integration, this method will return None . If it is part of a ZenML integration, it will return the integration class found inside the integration init file. Source code in zenml/integrations/utils.py def get_integration_for_module ( module_name : str ) -> Optional [ Type [ Integration ]]: \"\"\"Gets the integration class for a module inside an integration. If the module given by `module_name` is not part of a ZenML integration, this method will return `None`. If it is part of a ZenML integration, it will return the integration class found inside the integration __init__ file. \"\"\" integration_prefix = \"zenml.integrations.\" if not module_name . startswith ( integration_prefix ): return None integration_module_name = \".\" . join ( module_name . split ( \".\" , 3 )[: 3 ]) try : integration_module = sys . modules [ integration_module_name ] except KeyError : integration_module = importlib . import_module ( integration_module_name ) for name , member in inspect . getmembers ( integration_module ): if ( member is not Integration and isinstance ( member , IntegrationMeta ) and issubclass ( member , Integration ) ): return cast ( Type [ Integration ], member ) return None get_requirements_for_module ( module_name ) Gets requirements for a module inside an integration. If the module given by module_name is not part of a ZenML integration, this method will return an empty list. If it is part of a ZenML integration, it will return the list of requirements specified inside the integration class found inside the integration init file. Source code in zenml/integrations/utils.py def get_requirements_for_module ( module_name : str ) -> List [ str ]: \"\"\"Gets requirements for a module inside an integration. If the module given by `module_name` is not part of a ZenML integration, this method will return an empty list. If it is part of a ZenML integration, it will return the list of requirements specified inside the integration class found inside the integration __init__ file. \"\"\" integration = get_integration_for_module ( module_name ) return integration . REQUIREMENTS if integration else []","title":"Integrations"},{"location":"api_docs/integrations/#integrations","text":"","title":"Integrations"},{"location":"api_docs/integrations/#zenml.integrations","text":"The ZenML integrations module contains sub-modules for each integration that we support. This includes orchestrators like Apache Airflow, visualization tools like the facets library, as well as deep learning libraries like PyTorch.","title":"integrations"},{"location":"api_docs/integrations/#zenml.integrations.airflow","text":"The Airflow integration sub-module powers an alternative to the local orchestrator. You can enable it by registering the Airflow orchestrator with the CLI tool, then bootstrap using the zenml orchestrator up command.","title":"airflow"},{"location":"api_docs/integrations/#zenml.integrations.airflow.AirflowIntegration","text":"Definition of Airflow Integration for ZenML. Source code in zenml/integrations/airflow/__init__.py class AirflowIntegration ( Integration ): \"\"\"Definition of Airflow Integration for ZenML.\"\"\" NAME = AIRFLOW REQUIREMENTS = [ \"apache-airflow==2.2.0\" ] @classmethod def activate ( cls ): \"\"\"Activates all classes required for the airflow integration.\"\"\" from zenml.integrations.airflow import orchestrators # noqa","title":"AirflowIntegration"},{"location":"api_docs/integrations/#zenml.integrations.airflow.AirflowIntegration.activate","text":"Activates all classes required for the airflow integration. Source code in zenml/integrations/airflow/__init__.py @classmethod def activate ( cls ): \"\"\"Activates all classes required for the airflow integration.\"\"\" from zenml.integrations.airflow import orchestrators # noqa","title":"activate()"},{"location":"api_docs/integrations/#zenml.integrations.airflow.orchestrators","text":"","title":"orchestrators"},{"location":"api_docs/integrations/#zenml.integrations.airflow.orchestrators.airflow_component","text":"Definition for Airflow component for TFX.","title":"airflow_component"},{"location":"api_docs/integrations/#zenml.integrations.airflow.orchestrators.airflow_component.AirflowComponent","text":"Airflow-specific TFX Component. This class wrap a component run into its own PythonOperator in Airflow. Source code in zenml/integrations/airflow/orchestrators/airflow_component.py class AirflowComponent ( python . PythonOperator ): \"\"\"Airflow-specific TFX Component. This class wrap a component run into its own PythonOperator in Airflow. \"\"\" def __init__ ( self , * , parent_dag : airflow . DAG , pipeline_node : pipeline_pb2 . PipelineNode , mlmd_connection : metadata . Metadata , pipeline_info : pipeline_pb2 . PipelineInfo , pipeline_runtime_spec : pipeline_pb2 . PipelineRuntimeSpec , executor_spec : Optional [ message . Message ] = None , custom_driver_spec : Optional [ message . Message ] = None ) -> None : \"\"\"Constructs an Airflow implementation of TFX component. Args: parent_dag: The airflow DAG that this component is contained in. pipeline_node: The specification of the node to launch. mlmd_connection: ML metadata connection info. pipeline_info: The information of the pipeline that this node runs in. pipeline_runtime_spec: The runtime information of the pipeline that this node runs in. executor_spec: Specification for the executor of the node. custom_driver_spec: Specification for custom driver. \"\"\" launcher_callable = functools . partial ( _airflow_component_launcher , pipeline_node = pipeline_node , mlmd_connection = mlmd_connection , pipeline_info = pipeline_info , pipeline_runtime_spec = pipeline_runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) super () . __init__ ( task_id = pipeline_node . node_info . id , provide_context = True , python_callable = launcher_callable , dag = parent_dag , ) __init__ ( self , * , parent_dag , pipeline_node , mlmd_connection , pipeline_info , pipeline_runtime_spec , executor_spec = None , custom_driver_spec = None ) special Constructs an Airflow implementation of TFX component. Parameters: Name Type Description Default parent_dag DAG The airflow DAG that this component is contained in. required pipeline_node PipelineNode The specification of the node to launch. required mlmd_connection Metadata ML metadata connection info. required pipeline_info PipelineInfo The information of the pipeline that this node runs in. required pipeline_runtime_spec PipelineRuntimeSpec The runtime information of the pipeline that this node runs in. required executor_spec Optional[google.protobuf.message.Message] Specification for the executor of the node. None custom_driver_spec Optional[google.protobuf.message.Message] Specification for custom driver. None Source code in zenml/integrations/airflow/orchestrators/airflow_component.py def __init__ ( self , * , parent_dag : airflow . DAG , pipeline_node : pipeline_pb2 . PipelineNode , mlmd_connection : metadata . Metadata , pipeline_info : pipeline_pb2 . PipelineInfo , pipeline_runtime_spec : pipeline_pb2 . PipelineRuntimeSpec , executor_spec : Optional [ message . Message ] = None , custom_driver_spec : Optional [ message . Message ] = None ) -> None : \"\"\"Constructs an Airflow implementation of TFX component. Args: parent_dag: The airflow DAG that this component is contained in. pipeline_node: The specification of the node to launch. mlmd_connection: ML metadata connection info. pipeline_info: The information of the pipeline that this node runs in. pipeline_runtime_spec: The runtime information of the pipeline that this node runs in. executor_spec: Specification for the executor of the node. custom_driver_spec: Specification for custom driver. \"\"\" launcher_callable = functools . partial ( _airflow_component_launcher , pipeline_node = pipeline_node , mlmd_connection = mlmd_connection , pipeline_info = pipeline_info , pipeline_runtime_spec = pipeline_runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) super () . __init__ ( task_id = pipeline_node . node_info . id , provide_context = True , python_callable = launcher_callable , dag = parent_dag , )","title":"AirflowComponent"},{"location":"api_docs/integrations/#zenml.integrations.airflow.orchestrators.airflow_dag_runner","text":"Definition of Airflow TFX runner. This is an unmodified copy from the TFX source code (outside of superficial, stylistic changes)","title":"airflow_dag_runner"},{"location":"api_docs/integrations/#zenml.integrations.airflow.orchestrators.airflow_dag_runner.AirflowDagRunner","text":"Tfx runner on Airflow. Source code in zenml/integrations/airflow/orchestrators/airflow_dag_runner.py class AirflowDagRunner ( tfx_runner . TfxRunner ): \"\"\"Tfx runner on Airflow.\"\"\" def __init__ ( self , config : Optional [ Union [ Dict [ str , Any ], AirflowPipelineConfig ]] = None , ): \"\"\"Creates an instance of AirflowDagRunner. Args: config: Optional Airflow pipeline config for customizing the launching of each component. \"\"\" if isinstance ( config , dict ): warnings . warn ( \"Pass config as a dict type is going to deprecated in 0.1.16. \" \"Use AirflowPipelineConfig type instead.\" , PendingDeprecationWarning , ) config = AirflowPipelineConfig ( airflow_dag_config = config ) super () . __init__ ( config ) def run ( self , pipeline : tfx_pipeline . Pipeline , run_name : str = \"\" ) -> \"airflow.DAG\" : \"\"\"Deploys given logical pipeline on Airflow. Args: pipeline: Logical pipeline containing pipeline args and comps. run_name: Optional name for the run. Returns: An Airflow DAG. \"\"\" # Only import these when needed. import airflow # noqa from zenml.integrations.airflow.orchestrators import airflow_component # Merge airflow-specific configs with pipeline args airflow_dag = airflow . DAG ( dag_id = pipeline . pipeline_info . pipeline_name , ** ( typing . cast ( AirflowPipelineConfig , self . _config ) . airflow_dag_config ), is_paused_upon_creation = False , catchup = False , # no backfill ) if \"tmp_dir\" not in pipeline . additional_pipeline_args : tmp_dir = os . path . join ( pipeline . pipeline_info . pipeline_root , \".temp\" , \"\" ) pipeline . additional_pipeline_args [ \"tmp_dir\" ] = tmp_dir for component in pipeline . components : if isinstance ( component , base_component . BaseComponent ): component . _resolve_pip_dependencies ( pipeline . pipeline_info . pipeline_root ) self . _replace_runtime_params ( component ) c = compiler . Compiler () pipeline = c . compile ( pipeline ) # Substitute the runtime parameter to be a concrete run_id runtime_parameter_utils . substitute_runtime_parameter ( pipeline , { \"pipeline-run-id\" : run_name , }, ) deployment_config = runner_utils . extract_local_deployment_config ( pipeline ) connection_config = ( Repository () . active_stack . metadata_store . get_tfx_metadata_config () ) component_impl_map = {} for node in pipeline . nodes : pipeline_node = node . pipeline_node node_id = pipeline_node . node_info . id executor_spec = runner_utils . extract_executor_spec ( deployment_config , node_id ) custom_driver_spec = runner_utils . extract_custom_driver_spec ( deployment_config , node_id ) current_airflow_component = airflow_component . AirflowComponent ( parent_dag = airflow_dag , pipeline_node = pipeline_node , mlmd_connection = connection_config , pipeline_info = pipeline . pipeline_info , pipeline_runtime_spec = pipeline . runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) component_impl_map [ node_id ] = current_airflow_component for upstream_node in node . pipeline_node . upstream_nodes : assert ( upstream_node in component_impl_map ), \"Components is not in topological order\" current_airflow_component . set_upstream ( component_impl_map [ upstream_node ] ) return airflow_dag def _replace_runtime_params ( self , comp : base_node . BaseNode ) -> base_node . BaseNode : \"\"\"Replaces runtime params for dynamic Airflow parameter execution. Args: comp: TFX component to be parsed. Returns: Returns edited component. \"\"\" for k , prop in comp . exec_properties . copy () . items (): if isinstance ( prop , RuntimeParameter ): # Airflow only supports string parameters. if prop . ptype != str : raise RuntimeError ( f \"RuntimeParameter in Airflow does not support \" f \" { prop . ptype } . The only ptype supported is string.\" ) # If the default is a template, drop the template markers # when inserting it into the .get() default argument below. # Otherwise, provide the default as a quoted string. default = cast ( str , prop . default ) if default . startswith ( \"{{\" ) and default . endswith ( \"}}\" ): default = default [ 2 : - 2 ] else : default = json . dumps ( default ) template_field = '{{ dag_run.conf.get(\" %s \", %s ) }}' % ( prop . name , default , ) comp . exec_properties [ k ] = template_field return comp __init__ ( self , config = None ) special Creates an instance of AirflowDagRunner. Parameters: Name Type Description Default config Union[Dict[str, Any], zenml.integrations.airflow.orchestrators.airflow_dag_runner.AirflowPipelineConfig] Optional Airflow pipeline config for customizing the None Source code in zenml/integrations/airflow/orchestrators/airflow_dag_runner.py def __init__ ( self , config : Optional [ Union [ Dict [ str , Any ], AirflowPipelineConfig ]] = None , ): \"\"\"Creates an instance of AirflowDagRunner. Args: config: Optional Airflow pipeline config for customizing the launching of each component. \"\"\" if isinstance ( config , dict ): warnings . warn ( \"Pass config as a dict type is going to deprecated in 0.1.16. \" \"Use AirflowPipelineConfig type instead.\" , PendingDeprecationWarning , ) config = AirflowPipelineConfig ( airflow_dag_config = config ) super () . __init__ ( config ) run ( self , pipeline , run_name = '' ) Deploys given logical pipeline on Airflow. Parameters: Name Type Description Default pipeline Pipeline Logical pipeline containing pipeline args and comps. required run_name str Optional name for the run. '' Returns: Type Description airflow.DAG An Airflow DAG. Source code in zenml/integrations/airflow/orchestrators/airflow_dag_runner.py def run ( self , pipeline : tfx_pipeline . Pipeline , run_name : str = \"\" ) -> \"airflow.DAG\" : \"\"\"Deploys given logical pipeline on Airflow. Args: pipeline: Logical pipeline containing pipeline args and comps. run_name: Optional name for the run. Returns: An Airflow DAG. \"\"\" # Only import these when needed. import airflow # noqa from zenml.integrations.airflow.orchestrators import airflow_component # Merge airflow-specific configs with pipeline args airflow_dag = airflow . DAG ( dag_id = pipeline . pipeline_info . pipeline_name , ** ( typing . cast ( AirflowPipelineConfig , self . _config ) . airflow_dag_config ), is_paused_upon_creation = False , catchup = False , # no backfill ) if \"tmp_dir\" not in pipeline . additional_pipeline_args : tmp_dir = os . path . join ( pipeline . pipeline_info . pipeline_root , \".temp\" , \"\" ) pipeline . additional_pipeline_args [ \"tmp_dir\" ] = tmp_dir for component in pipeline . components : if isinstance ( component , base_component . BaseComponent ): component . _resolve_pip_dependencies ( pipeline . pipeline_info . pipeline_root ) self . _replace_runtime_params ( component ) c = compiler . Compiler () pipeline = c . compile ( pipeline ) # Substitute the runtime parameter to be a concrete run_id runtime_parameter_utils . substitute_runtime_parameter ( pipeline , { \"pipeline-run-id\" : run_name , }, ) deployment_config = runner_utils . extract_local_deployment_config ( pipeline ) connection_config = ( Repository () . active_stack . metadata_store . get_tfx_metadata_config () ) component_impl_map = {} for node in pipeline . nodes : pipeline_node = node . pipeline_node node_id = pipeline_node . node_info . id executor_spec = runner_utils . extract_executor_spec ( deployment_config , node_id ) custom_driver_spec = runner_utils . extract_custom_driver_spec ( deployment_config , node_id ) current_airflow_component = airflow_component . AirflowComponent ( parent_dag = airflow_dag , pipeline_node = pipeline_node , mlmd_connection = connection_config , pipeline_info = pipeline . pipeline_info , pipeline_runtime_spec = pipeline . runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) component_impl_map [ node_id ] = current_airflow_component for upstream_node in node . pipeline_node . upstream_nodes : assert ( upstream_node in component_impl_map ), \"Components is not in topological order\" current_airflow_component . set_upstream ( component_impl_map [ upstream_node ] ) return airflow_dag","title":"AirflowDagRunner"},{"location":"api_docs/integrations/#zenml.integrations.airflow.orchestrators.airflow_dag_runner.AirflowPipelineConfig","text":"Pipeline config for AirflowDagRunner. Source code in zenml/integrations/airflow/orchestrators/airflow_dag_runner.py class AirflowPipelineConfig ( pipeline_config . PipelineConfig ): \"\"\"Pipeline config for AirflowDagRunner.\"\"\" def __init__ ( self , airflow_dag_config : Optional [ Dict [ str , Any ]] = None , ** kwargs : Any ): \"\"\"Creates an instance of AirflowPipelineConfig. Args: airflow_dag_config: Configs of Airflow DAG model. See https://airflow.apache.org/_api/airflow/models/dag/index.html#airflow.models.dag.DAG for the full spec. **kwargs: keyword args for PipelineConfig. \"\"\" super () . __init__ ( ** kwargs ) self . airflow_dag_config = airflow_dag_config or {} __init__ ( self , airflow_dag_config = None , ** kwargs ) special Creates an instance of AirflowPipelineConfig. Parameters: Name Type Description Default airflow_dag_config Optional[Dict[str, Any]] Configs of Airflow DAG model. See https://airflow.apache.org/_api/airflow/models/dag/index.html#airflow.models.dag.DAG for the full spec. None **kwargs Any keyword args for PipelineConfig. {} Source code in zenml/integrations/airflow/orchestrators/airflow_dag_runner.py def __init__ ( self , airflow_dag_config : Optional [ Dict [ str , Any ]] = None , ** kwargs : Any ): \"\"\"Creates an instance of AirflowPipelineConfig. Args: airflow_dag_config: Configs of Airflow DAG model. See https://airflow.apache.org/_api/airflow/models/dag/index.html#airflow.models.dag.DAG for the full spec. **kwargs: keyword args for PipelineConfig. \"\"\" super () . __init__ ( ** kwargs ) self . airflow_dag_config = airflow_dag_config or {}","title":"AirflowPipelineConfig"},{"location":"api_docs/integrations/#zenml.integrations.airflow.orchestrators.airflow_orchestrator","text":"","title":"airflow_orchestrator"},{"location":"api_docs/integrations/#zenml.integrations.airflow.orchestrators.airflow_orchestrator.AirflowOrchestrator","text":"Orchestrator responsible for running pipelines using Airflow. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py class AirflowOrchestrator ( BaseOrchestrator ): \"\"\"Orchestrator responsible for running pipelines using Airflow.\"\"\" airflow_home : str = \"\" airflow_config : Optional [ Dict [ str , Any ]] = {} schedule_interval_minutes : int = 1 supports_local_execution = True supports_remote_execution = False def __init__ ( self , ** values : Any ): \"\"\"Sets environment variables to configure airflow.\"\"\" super () . __init__ ( ** values ) self . _set_env () @property def flavor ( self ) -> OrchestratorFlavor : \"\"\"The orchestrator flavor.\"\"\" return OrchestratorFlavor . AIRFLOW @root_validator def set_airflow_home ( cls , values : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Sets airflow home according to orchestrator UUID.\"\"\" if \"uuid\" not in values : raise ValueError ( \"`uuid` needs to exist for AirflowOrchestrator.\" ) values [ \"airflow_home\" ] = os . path . join ( zenml . io . utils . get_global_config_directory (), AIRFLOW_ROOT_DIR , str ( values [ \"uuid\" ]), ) return values @property def dags_directory ( self ) -> str : \"\"\"Returns path to the airflow dags directory.\"\"\" return os . path . join ( self . airflow_home , \"dags\" ) @property def pid_file ( self ) -> str : \"\"\"Returns path to the daemon PID file.\"\"\" return os . path . join ( self . airflow_home , \"airflow_daemon.pid\" ) @property def log_file ( self ) -> str : \"\"\"Returns path to the airflow log file.\"\"\" return os . path . join ( self . airflow_home , \"airflow_orchestrator.log\" ) @property def password_file ( self ) -> str : \"\"\"Returns path to the webserver password file.\"\"\" return os . path . join ( self . airflow_home , \"standalone_admin_password.txt\" ) def _set_env ( self ) -> None : \"\"\"Sets environment variables to configure airflow.\"\"\" os . environ [ \"AIRFLOW_HOME\" ] = self . airflow_home os . environ [ \"AIRFLOW__CORE__DAGS_FOLDER\" ] = self . dags_directory os . environ [ \"AIRFLOW__CORE__DAG_DISCOVERY_SAFE_MODE\" ] = \"false\" os . environ [ \"AIRFLOW__CORE__LOAD_EXAMPLES\" ] = \"false\" # check the DAG folder every 10 seconds for new files os . environ [ \"AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL\" ] = \"10\" def _copy_to_dag_directory_if_necessary ( self , dag_filepath : str ): \"\"\"Copies the DAG module to the airflow DAGs directory if it's not already located there. Args: dag_filepath: Path to the file in which the DAG is defined. \"\"\" dags_directory = fileio . resolve_relative_path ( self . dags_directory ) if dags_directory == os . path . dirname ( dag_filepath ): logger . debug ( \"File is already in airflow DAGs directory.\" ) else : logger . debug ( \"Copying dag file ' %s ' to DAGs directory.\" , dag_filepath ) destination_path = os . path . join ( dags_directory , os . path . basename ( dag_filepath ) ) if fileio . file_exists ( destination_path ): logger . info ( \"File ' %s ' already exists, overwriting with new DAG file\" , destination_path , ) fileio . copy ( dag_filepath , destination_path , overwrite = True ) def _log_webserver_credentials ( self ): \"\"\"Logs URL and credentials to login to the airflow webserver. Raises: FileNotFoundError: If the password file does not exist. \"\"\" if fileio . file_exists ( self . password_file ): with open ( self . password_file ) as file : password = file . read () . strip () else : raise FileNotFoundError ( f \"Can't find password file ' { self . password_file } '\" ) logger . info ( \"To inspect your DAGs, login to http://0.0.0.0:8080 \" \"with username: admin password: %s \" , password , ) def runtime_options ( self ) -> Dict [ str , Any ]: \"\"\"Runtime options for the airflow orchestrator.\"\"\" return { DAG_FILEPATH_OPTION_KEY : None } def prepare_pipeline_deployment ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , runtime_configuration : \"RuntimeConfiguration\" , ) -> None : \"\"\"Checks whether airflow is running and copies the DAG file to the airflow DAGs directory. Raises: RuntimeError: If airflow is not running or no DAG filepath runtime option is provided. \"\"\" if not self . is_running : raise RuntimeError ( \"Airflow orchestrator is currently not running. \" \"Run `zenml orchestrator up` to start the \" \"orchestrator of the active stack.\" ) try : dag_filepath = runtime_configuration [ DAG_FILEPATH_OPTION_KEY ] except KeyError : raise RuntimeError ( f \"No DAG filepath found in runtime configuration. Make sure \" f \"to add the filepath to your airflow DAG file as a runtime \" f \"option (key: ' { DAG_FILEPATH_OPTION_KEY } ').\" ) self . _copy_to_dag_directory_if_necessary ( dag_filepath = dag_filepath ) @property def is_running ( self ) -> bool : \"\"\"Returns whether the airflow daemon is currently running.\"\"\" from airflow.cli.commands.standalone_command import StandaloneCommand from airflow.jobs.triggerer_job import TriggererJob daemon_running = daemon . check_if_daemon_is_running ( self . pid_file ) command = StandaloneCommand () webserver_port_open = command . port_open ( 8080 ) if not daemon_running : if webserver_port_open : raise RuntimeError ( \"The airflow daemon does not seem to be running but \" \"local port 8080 is occupied. Make sure the port is \" \"available and try again.\" ) # exit early so we don't check non-existing airflow databases return False # we can't use StandaloneCommand().is_ready() here as the # Airflow SequentialExecutor apparently does not send a heartbeat # while running a task which would result in this returning `False` # even if Airflow is running. airflow_running = webserver_port_open and command . job_running ( TriggererJob ) return airflow_running @property def is_provisioned ( self ) -> bool : \"\"\"Returns whether the airflow daemon is currently running.\"\"\" return self . is_running def provision ( self ) -> None : \"\"\"Ensures that Airflow is running.\"\"\" if self . is_running : logger . info ( \"Airflow is already running.\" ) self . _log_webserver_credentials () return if not fileio . file_exists ( self . dags_directory ): fileio . create_dir_recursive_if_not_exists ( self . dags_directory ) from airflow.cli.commands.standalone_command import StandaloneCommand try : command = StandaloneCommand () # Run the daemon with a working directory inside the current # zenml repo so the same repo will be used to run the DAGs daemon . run_as_daemon ( command . run , pid_file = self . pid_file , log_file = self . log_file , working_directory = zenml . io . utils . get_zenml_dir (), ) while not self . is_running : # Wait until the daemon started all the relevant airflow # processes time . sleep ( 0.1 ) self . _log_webserver_credentials () except Exception as e : logger . error ( e ) logger . error ( \"An error occurred while starting the Airflow daemon. If you \" \"want to start it manually, use the commands described in the \" \"official Airflow quickstart guide for running Airflow locally.\" ) self . down () def deprovision ( self ) -> None : \"\"\"Stops the airflow daemon if necessary and tears down resources.\"\"\" if self . is_running : daemon . stop_daemon ( self . pid_file , kill_children = True ) fileio . rm_dir ( self . airflow_home ) logger . info ( \"Airflow spun down.\" ) def run_pipeline ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , run_name : str ) -> Any : \"\"\"Schedules a pipeline to be run on Airflow. Returns: An Airflow DAG object that corresponds to the ZenML pipeline. \"\"\" self . airflow_config = { \"schedule_interval\" : datetime . timedelta ( minutes = self . schedule_interval_minutes ), # We set this in the past and turn catchup off and then it works \"start_date\" : datetime . datetime ( 2019 , 1 , 1 ), } runner = AirflowDagRunner ( AirflowPipelineConfig ( self . airflow_config )) tfx_pipeline = create_tfx_pipeline ( pipeline , stack = stack ) return runner . run ( tfx_pipeline , run_name = run_name ) dags_directory : str property readonly Returns path to the airflow dags directory. flavor : OrchestratorFlavor property readonly The orchestrator flavor. is_provisioned : bool property readonly Returns whether the airflow daemon is currently running. is_running : bool property readonly Returns whether the airflow daemon is currently running. log_file : str property readonly Returns path to the airflow log file. password_file : str property readonly Returns path to the webserver password file. pid_file : str property readonly Returns path to the daemon PID file. __init__ ( self , ** values ) special Sets environment variables to configure airflow. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py def __init__ ( self , ** values : Any ): \"\"\"Sets environment variables to configure airflow.\"\"\" super () . __init__ ( ** values ) self . _set_env () deprovision ( self ) Stops the airflow daemon if necessary and tears down resources. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py def deprovision ( self ) -> None : \"\"\"Stops the airflow daemon if necessary and tears down resources.\"\"\" if self . is_running : daemon . stop_daemon ( self . pid_file , kill_children = True ) fileio . rm_dir ( self . airflow_home ) logger . info ( \"Airflow spun down.\" ) prepare_pipeline_deployment ( self , pipeline , stack , runtime_configuration ) Checks whether airflow is running and copies the DAG file to the airflow DAGs directory. Exceptions: Type Description RuntimeError If airflow is not running or no DAG filepath runtime Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py def prepare_pipeline_deployment ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , runtime_configuration : \"RuntimeConfiguration\" , ) -> None : \"\"\"Checks whether airflow is running and copies the DAG file to the airflow DAGs directory. Raises: RuntimeError: If airflow is not running or no DAG filepath runtime option is provided. \"\"\" if not self . is_running : raise RuntimeError ( \"Airflow orchestrator is currently not running. \" \"Run `zenml orchestrator up` to start the \" \"orchestrator of the active stack.\" ) try : dag_filepath = runtime_configuration [ DAG_FILEPATH_OPTION_KEY ] except KeyError : raise RuntimeError ( f \"No DAG filepath found in runtime configuration. Make sure \" f \"to add the filepath to your airflow DAG file as a runtime \" f \"option (key: ' { DAG_FILEPATH_OPTION_KEY } ').\" ) self . _copy_to_dag_directory_if_necessary ( dag_filepath = dag_filepath ) provision ( self ) Ensures that Airflow is running. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py def provision ( self ) -> None : \"\"\"Ensures that Airflow is running.\"\"\" if self . is_running : logger . info ( \"Airflow is already running.\" ) self . _log_webserver_credentials () return if not fileio . file_exists ( self . dags_directory ): fileio . create_dir_recursive_if_not_exists ( self . dags_directory ) from airflow.cli.commands.standalone_command import StandaloneCommand try : command = StandaloneCommand () # Run the daemon with a working directory inside the current # zenml repo so the same repo will be used to run the DAGs daemon . run_as_daemon ( command . run , pid_file = self . pid_file , log_file = self . log_file , working_directory = zenml . io . utils . get_zenml_dir (), ) while not self . is_running : # Wait until the daemon started all the relevant airflow # processes time . sleep ( 0.1 ) self . _log_webserver_credentials () except Exception as e : logger . error ( e ) logger . error ( \"An error occurred while starting the Airflow daemon. If you \" \"want to start it manually, use the commands described in the \" \"official Airflow quickstart guide for running Airflow locally.\" ) self . down () run_pipeline ( self , pipeline , stack , run_name ) Schedules a pipeline to be run on Airflow. Returns: Type Description Any An Airflow DAG object that corresponds to the ZenML pipeline. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py def run_pipeline ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , run_name : str ) -> Any : \"\"\"Schedules a pipeline to be run on Airflow. Returns: An Airflow DAG object that corresponds to the ZenML pipeline. \"\"\" self . airflow_config = { \"schedule_interval\" : datetime . timedelta ( minutes = self . schedule_interval_minutes ), # We set this in the past and turn catchup off and then it works \"start_date\" : datetime . datetime ( 2019 , 1 , 1 ), } runner = AirflowDagRunner ( AirflowPipelineConfig ( self . airflow_config )) tfx_pipeline = create_tfx_pipeline ( pipeline , stack = stack ) return runner . run ( tfx_pipeline , run_name = run_name ) runtime_options ( self ) Runtime options for the airflow orchestrator. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py def runtime_options ( self ) -> Dict [ str , Any ]: \"\"\"Runtime options for the airflow orchestrator.\"\"\" return { DAG_FILEPATH_OPTION_KEY : None } set_airflow_home ( values ) classmethod Sets airflow home according to orchestrator UUID. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py @root_validator def set_airflow_home ( cls , values : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Sets airflow home according to orchestrator UUID.\"\"\" if \"uuid\" not in values : raise ValueError ( \"`uuid` needs to exist for AirflowOrchestrator.\" ) values [ \"airflow_home\" ] = os . path . join ( zenml . io . utils . get_global_config_directory (), AIRFLOW_ROOT_DIR , str ( values [ \"uuid\" ]), ) return values","title":"AirflowOrchestrator"},{"location":"api_docs/integrations/#zenml.integrations.dash","text":"","title":"dash"},{"location":"api_docs/integrations/#zenml.integrations.dash.DashIntegration","text":"Definition of Dash integration for ZenML. Source code in zenml/integrations/dash/__init__.py class DashIntegration ( Integration ): \"\"\"Definition of Dash integration for ZenML.\"\"\" NAME = DASH REQUIREMENTS = [ \"dash>=2.0.0\" , \"dash-cytoscape>=0.3.0\" , \"dash-bootstrap-components>=1.0.1\" , ]","title":"DashIntegration"},{"location":"api_docs/integrations/#zenml.integrations.dash.visualizers","text":"","title":"visualizers"},{"location":"api_docs/integrations/#zenml.integrations.dash.visualizers.pipeline_run_lineage_visualizer","text":"","title":"pipeline_run_lineage_visualizer"},{"location":"api_docs/integrations/#zenml.integrations.dash.visualizers.pipeline_run_lineage_visualizer.PipelineRunLineageVisualizer","text":"Implementation of a lineage diagram via the dash and dash-cyctoscape library. Source code in zenml/integrations/dash/visualizers/pipeline_run_lineage_visualizer.py class PipelineRunLineageVisualizer ( BasePipelineRunVisualizer ): \"\"\"Implementation of a lineage diagram via the [dash]( https://plotly.com/dash/) and [dash-cyctoscape]( https://dash.plotly.com/cytoscape) library.\"\"\" ARTIFACT_PREFIX = \"artifact_\" STEP_PREFIX = \"step_\" STATUS_CLASS_MAPPING = { ExecutionStatus . CACHED : \"green\" , ExecutionStatus . FAILED : \"red\" , ExecutionStatus . RUNNING : \"yellow\" , ExecutionStatus . COMPLETED : \"blue\" , } def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> dash . Dash : \"\"\"Method to visualize pipeline runs via the Dash library. The layout puts every layer of the dag in a column. \"\"\" app = dash . Dash ( __name__ , external_stylesheets = [ dbc . themes . BOOTSTRAP , dbc . icons . BOOTSTRAP , ], ) nodes , edges , first_step_id = [], [], None first_step_id = None for step in object . steps : step_output_artifacts = list ( step . outputs . values ()) execution_id = ( step_output_artifacts [ 0 ] . producer_step . id if step_output_artifacts else step . id ) step_id = self . STEP_PREFIX + str ( step . id ) if first_step_id is None : first_step_id = step_id nodes . append ( { \"data\" : { \"id\" : step_id , \"execution_id\" : execution_id , \"label\" : f \" { execution_id } / { step . entrypoint_name } \" , \"entrypoint_name\" : step . entrypoint_name , # redundant for consistency \"name\" : step . name , # redundant for consistency \"type\" : \"step\" , \"parameters\" : step . parameters , \"inputs\" : { k : v . uri for k , v in step . inputs . items ()}, \"outputs\" : { k : v . uri for k , v in step . outputs . items ()}, }, \"classes\" : self . STATUS_CLASS_MAPPING [ step . status ], } ) for artifact_name , artifact in step . outputs . items (): nodes . append ( { \"data\" : { \"id\" : self . ARTIFACT_PREFIX + str ( artifact . id ), \"execution_id\" : artifact . id , \"label\" : f \" { artifact . id } / { artifact_name } (\" f \" { artifact . data_type } )\" , \"type\" : \"artifact\" , \"name\" : artifact_name , \"is_cached\" : artifact . is_cached , \"artifact_type\" : artifact . type , \"artifact_data_type\" : artifact . data_type , \"parent_step_id\" : artifact . parent_step_id , \"producer_step_id\" : artifact . producer_step . id , \"uri\" : artifact . uri , }, \"classes\" : f \"rectangle \" f \" { self . STATUS_CLASS_MAPPING [ step . status ] } \" , } ) edges . append ( { \"data\" : { \"source\" : self . STEP_PREFIX + str ( step . id ), \"target\" : self . ARTIFACT_PREFIX + str ( artifact . id ), }, \"classes\" : f \"edge-arrow \" f \" { self . STATUS_CLASS_MAPPING [ step . status ] } \" + ( \" dashed\" if artifact . is_cached else \" solid\" ), } ) for artifact_name , artifact in step . inputs . items (): edges . append ( { \"data\" : { \"source\" : self . ARTIFACT_PREFIX + str ( artifact . id ), \"target\" : self . STEP_PREFIX + str ( step . id ), }, \"classes\" : \"edge-arrow \" + ( f \" { self . STATUS_CLASS_MAPPING [ ExecutionStatus . CACHED ] } dashed\" if artifact . is_cached else f \" { self . STATUS_CLASS_MAPPING [ step . status ] } solid\" ), } ) app . layout = dbc . Row ( [ dbc . Container ( f \"Run: { object . name } \" , class_name = \"h1\" ), dbc . Row ( [ dbc . Col ( [ dbc . Row ( [ html . Span ( [ html . Span ( [ html . I ( className = \"bi bi-circle-fill me-1\" ), \"Step\" , ], className = \"me-2\" , ), html . Span ( [ html . I ( className = \"bi bi-square-fill me-1\" ), \"Artifact\" , ], className = \"me-4\" , ), dbc . Badge ( \"Completed\" , color = COLOR_BLUE , className = \"me-1\" , ), dbc . Badge ( \"Cached\" , color = COLOR_GREEN , className = \"me-1\" , ), dbc . Badge ( \"Running\" , color = COLOR_YELLOW , className = \"me-1\" , ), dbc . Badge ( \"Failed\" , color = COLOR_RED , className = \"me-1\" , ), ] ), ] ), dbc . Row ( [ cyto . Cytoscape ( id = \"cytoscape\" , layout = { \"name\" : \"breadthfirst\" , \"roots\" : f '[id = \" { first_step_id } \"]' , }, elements = edges + nodes , stylesheet = STYLESHEET , style = { \"width\" : \"100%\" , \"height\" : \"800px\" , }, zoom = 1 , ) ] ), dbc . Row ( [ dbc . Button ( \"Reset\" , id = \"bt-reset\" , color = \"primary\" , className = \"me-1\" , ) ] ), ] ), dbc . Col ( [ dcc . Markdown ( id = \"markdown-selected-node-data\" ), ] ), ] ), ], className = \"p-5\" , ) @app . callback ( # type: ignore[misc] Output ( \"markdown-selected-node-data\" , \"children\" ), Input ( \"cytoscape\" , \"selectedNodeData\" ), ) def display_data ( data_list : List [ Dict [ str , Any ]]) -> str : \"\"\"Callback for the text area below the graph\"\"\" if data_list is None : return \"Click on a node in the diagram.\" text = \"\" for data in data_list : text += f '## { data [ \"execution_id\" ] } / { data [ \"name\" ] } ' + \" \\n\\n \" if data [ \"type\" ] == \"artifact\" : for item in [ \"artifact_data_type\" , \"is_cached\" , \"producer_step_id\" , \"parent_step_id\" , \"uri\" , ]: text += f \"** { item } **: { data [ item ] } \" + \" \\n\\n \" elif data [ \"type\" ] == \"step\" : text += \"### Inputs:\" + \" \\n\\n \" for k , v in data [ \"inputs\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" text += \"### Outputs:\" + \" \\n\\n \" for k , v in data [ \"outputs\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" text += \"### Params:\" for k , v in data [ \"parameters\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" return text @app . callback ( # type: ignore[misc] [ Output ( \"cytoscape\" , \"zoom\" ), Output ( \"cytoscape\" , \"elements\" )], [ Input ( \"bt-reset\" , \"n_clicks\" )], ) def reset_layout ( n_clicks : int , ) -> List [ Union [ int , List [ Dict [ str , Collection [ str ]]]]]: \"\"\"Resets the layout\"\"\" logger . debug ( n_clicks , \"clicked in reset button.\" ) return [ 1 , edges + nodes ] app . run_server () return app visualize ( self , object , * args , ** kwargs ) Method to visualize pipeline runs via the Dash library. The layout puts every layer of the dag in a column. Source code in zenml/integrations/dash/visualizers/pipeline_run_lineage_visualizer.py def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> dash . Dash : \"\"\"Method to visualize pipeline runs via the Dash library. The layout puts every layer of the dag in a column. \"\"\" app = dash . Dash ( __name__ , external_stylesheets = [ dbc . themes . BOOTSTRAP , dbc . icons . BOOTSTRAP , ], ) nodes , edges , first_step_id = [], [], None first_step_id = None for step in object . steps : step_output_artifacts = list ( step . outputs . values ()) execution_id = ( step_output_artifacts [ 0 ] . producer_step . id if step_output_artifacts else step . id ) step_id = self . STEP_PREFIX + str ( step . id ) if first_step_id is None : first_step_id = step_id nodes . append ( { \"data\" : { \"id\" : step_id , \"execution_id\" : execution_id , \"label\" : f \" { execution_id } / { step . entrypoint_name } \" , \"entrypoint_name\" : step . entrypoint_name , # redundant for consistency \"name\" : step . name , # redundant for consistency \"type\" : \"step\" , \"parameters\" : step . parameters , \"inputs\" : { k : v . uri for k , v in step . inputs . items ()}, \"outputs\" : { k : v . uri for k , v in step . outputs . items ()}, }, \"classes\" : self . STATUS_CLASS_MAPPING [ step . status ], } ) for artifact_name , artifact in step . outputs . items (): nodes . append ( { \"data\" : { \"id\" : self . ARTIFACT_PREFIX + str ( artifact . id ), \"execution_id\" : artifact . id , \"label\" : f \" { artifact . id } / { artifact_name } (\" f \" { artifact . data_type } )\" , \"type\" : \"artifact\" , \"name\" : artifact_name , \"is_cached\" : artifact . is_cached , \"artifact_type\" : artifact . type , \"artifact_data_type\" : artifact . data_type , \"parent_step_id\" : artifact . parent_step_id , \"producer_step_id\" : artifact . producer_step . id , \"uri\" : artifact . uri , }, \"classes\" : f \"rectangle \" f \" { self . STATUS_CLASS_MAPPING [ step . status ] } \" , } ) edges . append ( { \"data\" : { \"source\" : self . STEP_PREFIX + str ( step . id ), \"target\" : self . ARTIFACT_PREFIX + str ( artifact . id ), }, \"classes\" : f \"edge-arrow \" f \" { self . STATUS_CLASS_MAPPING [ step . status ] } \" + ( \" dashed\" if artifact . is_cached else \" solid\" ), } ) for artifact_name , artifact in step . inputs . items (): edges . append ( { \"data\" : { \"source\" : self . ARTIFACT_PREFIX + str ( artifact . id ), \"target\" : self . STEP_PREFIX + str ( step . id ), }, \"classes\" : \"edge-arrow \" + ( f \" { self . STATUS_CLASS_MAPPING [ ExecutionStatus . CACHED ] } dashed\" if artifact . is_cached else f \" { self . STATUS_CLASS_MAPPING [ step . status ] } solid\" ), } ) app . layout = dbc . Row ( [ dbc . Container ( f \"Run: { object . name } \" , class_name = \"h1\" ), dbc . Row ( [ dbc . Col ( [ dbc . Row ( [ html . Span ( [ html . Span ( [ html . I ( className = \"bi bi-circle-fill me-1\" ), \"Step\" , ], className = \"me-2\" , ), html . Span ( [ html . I ( className = \"bi bi-square-fill me-1\" ), \"Artifact\" , ], className = \"me-4\" , ), dbc . Badge ( \"Completed\" , color = COLOR_BLUE , className = \"me-1\" , ), dbc . Badge ( \"Cached\" , color = COLOR_GREEN , className = \"me-1\" , ), dbc . Badge ( \"Running\" , color = COLOR_YELLOW , className = \"me-1\" , ), dbc . Badge ( \"Failed\" , color = COLOR_RED , className = \"me-1\" , ), ] ), ] ), dbc . Row ( [ cyto . Cytoscape ( id = \"cytoscape\" , layout = { \"name\" : \"breadthfirst\" , \"roots\" : f '[id = \" { first_step_id } \"]' , }, elements = edges + nodes , stylesheet = STYLESHEET , style = { \"width\" : \"100%\" , \"height\" : \"800px\" , }, zoom = 1 , ) ] ), dbc . Row ( [ dbc . Button ( \"Reset\" , id = \"bt-reset\" , color = \"primary\" , className = \"me-1\" , ) ] ), ] ), dbc . Col ( [ dcc . Markdown ( id = \"markdown-selected-node-data\" ), ] ), ] ), ], className = \"p-5\" , ) @app . callback ( # type: ignore[misc] Output ( \"markdown-selected-node-data\" , \"children\" ), Input ( \"cytoscape\" , \"selectedNodeData\" ), ) def display_data ( data_list : List [ Dict [ str , Any ]]) -> str : \"\"\"Callback for the text area below the graph\"\"\" if data_list is None : return \"Click on a node in the diagram.\" text = \"\" for data in data_list : text += f '## { data [ \"execution_id\" ] } / { data [ \"name\" ] } ' + \" \\n\\n \" if data [ \"type\" ] == \"artifact\" : for item in [ \"artifact_data_type\" , \"is_cached\" , \"producer_step_id\" , \"parent_step_id\" , \"uri\" , ]: text += f \"** { item } **: { data [ item ] } \" + \" \\n\\n \" elif data [ \"type\" ] == \"step\" : text += \"### Inputs:\" + \" \\n\\n \" for k , v in data [ \"inputs\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" text += \"### Outputs:\" + \" \\n\\n \" for k , v in data [ \"outputs\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" text += \"### Params:\" for k , v in data [ \"parameters\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" return text @app . callback ( # type: ignore[misc] [ Output ( \"cytoscape\" , \"zoom\" ), Output ( \"cytoscape\" , \"elements\" )], [ Input ( \"bt-reset\" , \"n_clicks\" )], ) def reset_layout ( n_clicks : int , ) -> List [ Union [ int , List [ Dict [ str , Collection [ str ]]]]]: \"\"\"Resets the layout\"\"\" logger . debug ( n_clicks , \"clicked in reset button.\" ) return [ 1 , edges + nodes ] app . run_server () return app","title":"PipelineRunLineageVisualizer"},{"location":"api_docs/integrations/#zenml.integrations.evidently","text":"The Evidently integration provides a way to monitor your models in production. It includes a way to detect data drift and different kinds of model performance issues. The results of Evidently calculations can either be exported as an interactive dashboard (visualized as an html file or in your Jupyter notebook), or as a JSON file.","title":"evidently"},{"location":"api_docs/integrations/#zenml.integrations.evidently.EvidentlyIntegration","text":"Definition of Evidently integration for ZenML. Source code in zenml/integrations/evidently/__init__.py class EvidentlyIntegration ( Integration ): \"\"\"Definition of [Evidently](https://github.com/evidentlyai/evidently) integration for ZenML.\"\"\" NAME = EVIDENTLY REQUIREMENTS = [ \"evidently==v0.1.40.dev0\" ]","title":"EvidentlyIntegration"},{"location":"api_docs/integrations/#zenml.integrations.evidently.steps","text":"","title":"steps"},{"location":"api_docs/integrations/#zenml.integrations.evidently.steps.evidently_profile","text":"","title":"evidently_profile"},{"location":"api_docs/integrations/#zenml.integrations.evidently.steps.evidently_profile.EvidentlyProfileConfig","text":"Config class for Evidently profile steps. column_mapping: properties of the dataframe's columns used !!! profile_section \"a string that identifies the profile section to be used.\" The following are valid options supported by Evidently: - \"datadrift\" - \"categoricaltargetdrift\" - \"numericaltargetdrift\" - \"classificationmodelperformance\" - \"regressionmodelperformance\" - \"probabilisticmodelperformance\" Source code in zenml/integrations/evidently/steps/evidently_profile.py class EvidentlyProfileConfig ( BaseDriftDetectionConfig ): \"\"\"Config class for Evidently profile steps. column_mapping: properties of the dataframe's columns used profile_section: a string that identifies the profile section to be used. The following are valid options supported by Evidently: - \"datadrift\" - \"categoricaltargetdrift\" - \"numericaltargetdrift\" - \"classificationmodelperformance\" - \"regressionmodelperformance\" - \"probabilisticmodelperformance\" \"\"\" def get_profile_sections_and_tabs ( self , ) -> Tuple [ List [ ProfileSection ], List [ Tab ]]: try : return ( [ profile_mapper [ profile ]() for profile in self . profile_sections ], [ dashboard_mapper [ profile ]() for profile in self . profile_sections ], ) except KeyError : nl = \" \\n \" raise ValueError ( f \"Invalid profile section: { self . profile_sections } \\n\\n \" f \"Valid and supported options are: { nl } - \" f ' { f \" { nl } - \" . join ( list ( profile_mapper . keys ())) } ' ) column_mapping : Optional [ ColumnMapping ] profile_sections : Sequence [ str ]","title":"EvidentlyProfileConfig"},{"location":"api_docs/integrations/#zenml.integrations.evidently.steps.evidently_profile.EvidentlyProfileStep","text":"Simple step implementation which implements Evidently's functionality for creating a profile. Source code in zenml/integrations/evidently/steps/evidently_profile.py class EvidentlyProfileStep ( BaseDriftDetectionStep ): \"\"\"Simple step implementation which implements Evidently's functionality for creating a profile.\"\"\" OUTPUT_SPEC = { \"profile\" : DataAnalysisArtifact , \"dashboard\" : DataAnalysisArtifact , } def entrypoint ( # type: ignore[override] self , reference_dataset : DataArtifact , comparison_dataset : DataArtifact , config : EvidentlyProfileConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] profile = dict , dashboard = str ): \"\"\"Main entrypoint for the Evidently categorical target drift detection step. Args: reference_dataset: a Pandas dataframe comparison_dataset: a Pandas dataframe of new data you wish to compare against the reference data config: the configuration for the step context: the context of the step Returns: profile: dictionary report extracted from an Evidently Profile generated for the data drift dashboard: HTML report extracted from an Evidently Dashboard generated for the data drift \"\"\" sections , tabs = config . get_profile_sections_and_tabs () data_drift_dashboard = Dashboard ( tabs = tabs ) data_drift_dashboard . calculate ( reference_dataset , comparison_dataset , column_mapping = config . column_mapping or None , ) data_drift_profile = Profile ( sections = sections ) data_drift_profile . calculate ( reference_dataset , comparison_dataset , column_mapping = config . column_mapping or None , ) return [ data_drift_profile . object (), data_drift_dashboard . html ()] CONFIG_CLASS ( BaseDriftDetectionConfig ) pydantic-model Config class for Evidently profile steps. column_mapping: properties of the dataframe's columns used !!! profile_section \"a string that identifies the profile section to be used.\" The following are valid options supported by Evidently: - \"datadrift\" - \"categoricaltargetdrift\" - \"numericaltargetdrift\" - \"classificationmodelperformance\" - \"regressionmodelperformance\" - \"probabilisticmodelperformance\" Source code in zenml/integrations/evidently/steps/evidently_profile.py class EvidentlyProfileConfig ( BaseDriftDetectionConfig ): \"\"\"Config class for Evidently profile steps. column_mapping: properties of the dataframe's columns used profile_section: a string that identifies the profile section to be used. The following are valid options supported by Evidently: - \"datadrift\" - \"categoricaltargetdrift\" - \"numericaltargetdrift\" - \"classificationmodelperformance\" - \"regressionmodelperformance\" - \"probabilisticmodelperformance\" \"\"\" def get_profile_sections_and_tabs ( self , ) -> Tuple [ List [ ProfileSection ], List [ Tab ]]: try : return ( [ profile_mapper [ profile ]() for profile in self . profile_sections ], [ dashboard_mapper [ profile ]() for profile in self . profile_sections ], ) except KeyError : nl = \" \\n \" raise ValueError ( f \"Invalid profile section: { self . profile_sections } \\n\\n \" f \"Valid and supported options are: { nl } - \" f ' { f \" { nl } - \" . join ( list ( profile_mapper . keys ())) } ' ) column_mapping : Optional [ ColumnMapping ] profile_sections : Sequence [ str ] entrypoint ( self , reference_dataset , comparison_dataset , config , context ) Main entrypoint for the Evidently categorical target drift detection step. Parameters: Name Type Description Default reference_dataset DataArtifact a Pandas dataframe required comparison_dataset DataArtifact a Pandas dataframe of new data you wish to compare against the reference data required config EvidentlyProfileConfig the configuration for the step required context StepContext the context of the step required Returns: Type Description profile dictionary report extracted from an Evidently Profile generated for the data drift dashboard: HTML report extracted from an Evidently Dashboard generated for the data drift Source code in zenml/integrations/evidently/steps/evidently_profile.py def entrypoint ( # type: ignore[override] self , reference_dataset : DataArtifact , comparison_dataset : DataArtifact , config : EvidentlyProfileConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] profile = dict , dashboard = str ): \"\"\"Main entrypoint for the Evidently categorical target drift detection step. Args: reference_dataset: a Pandas dataframe comparison_dataset: a Pandas dataframe of new data you wish to compare against the reference data config: the configuration for the step context: the context of the step Returns: profile: dictionary report extracted from an Evidently Profile generated for the data drift dashboard: HTML report extracted from an Evidently Dashboard generated for the data drift \"\"\" sections , tabs = config . get_profile_sections_and_tabs () data_drift_dashboard = Dashboard ( tabs = tabs ) data_drift_dashboard . calculate ( reference_dataset , comparison_dataset , column_mapping = config . column_mapping or None , ) data_drift_profile = Profile ( sections = sections ) data_drift_profile . calculate ( reference_dataset , comparison_dataset , column_mapping = config . column_mapping or None , ) return [ data_drift_profile . object (), data_drift_dashboard . html ()]","title":"EvidentlyProfileStep"},{"location":"api_docs/integrations/#zenml.integrations.evidently.visualizers","text":"","title":"visualizers"},{"location":"api_docs/integrations/#zenml.integrations.evidently.visualizers.evidently_visualizer","text":"","title":"evidently_visualizer"},{"location":"api_docs/integrations/#zenml.integrations.evidently.visualizers.evidently_visualizer.EvidentlyVisualizer","text":"The implementation of an Evidently Visualizer. Source code in zenml/integrations/evidently/visualizers/evidently_visualizer.py class EvidentlyVisualizer ( BaseStepVisualizer ): \"\"\"The implementation of an Evidently Visualizer.\"\"\" @abstractmethod def visualize ( self , object : StepView , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize components Args: object: StepView fetched from run.get_step(). \"\"\" for artifact_view in object . outputs . values (): # filter out anything but data analysis artifacts if ( artifact_view . type == DataAnalysisArtifact . __name__ and artifact_view . data_type == \"builtins.str\" ): artifact = artifact_view . read () self . generate_facet ( artifact ) def generate_facet ( self , html_ : str ) -> None : \"\"\"Generate a Facet Overview Args: h: HTML represented as a string. \"\"\" if self . running_in_notebook (): from IPython.core.display import HTML , display display ( HTML ( html_ )) else : logger . warn ( \"The magic functions are only usable in a Jupyter notebook.\" ) with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : zenml . io . utils . write_file_contents_as_string ( f . name , html_ ) url = f \"file:/// { f . name } \" logger . info ( \"Opening %s in a new browser..\" % f . name ) webbrowser . open ( url , new = 2 ) generate_facet ( self , html_ ) Generate a Facet Overview Parameters: Name Type Description Default h HTML represented as a string. required Source code in zenml/integrations/evidently/visualizers/evidently_visualizer.py def generate_facet ( self , html_ : str ) -> None : \"\"\"Generate a Facet Overview Args: h: HTML represented as a string. \"\"\" if self . running_in_notebook (): from IPython.core.display import HTML , display display ( HTML ( html_ )) else : logger . warn ( \"The magic functions are only usable in a Jupyter notebook.\" ) with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : zenml . io . utils . write_file_contents_as_string ( f . name , html_ ) url = f \"file:/// { f . name } \" logger . info ( \"Opening %s in a new browser..\" % f . name ) webbrowser . open ( url , new = 2 ) visualize ( self , object , * args , ** kwargs ) Method to visualize components Parameters: Name Type Description Default object StepView StepView fetched from run.get_step(). required Source code in zenml/integrations/evidently/visualizers/evidently_visualizer.py @abstractmethod def visualize ( self , object : StepView , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize components Args: object: StepView fetched from run.get_step(). \"\"\" for artifact_view in object . outputs . values (): # filter out anything but data analysis artifacts if ( artifact_view . type == DataAnalysisArtifact . __name__ and artifact_view . data_type == \"builtins.str\" ): artifact = artifact_view . read () self . generate_facet ( artifact )","title":"EvidentlyVisualizer"},{"location":"api_docs/integrations/#zenml.integrations.facets","text":"The Facets integration provides a simple way to visualize post-execution objects like PipelineView , PipelineRunView and StepView . These objects can be extended using the BaseVisualization class. This integration requires facets-overview be installed in your Python environment.","title":"facets"},{"location":"api_docs/integrations/#zenml.integrations.facets.FacetsIntegration","text":"Definition of Facet integration for ZenML. Source code in zenml/integrations/facets/__init__.py class FacetsIntegration ( Integration ): \"\"\"Definition of [Facet](https://pair-code.github.io/facets/) integration for ZenML.\"\"\" NAME = FACETS REQUIREMENTS = [ \"facets-overview>=1.0.0\" , \"IPython\" ]","title":"FacetsIntegration"},{"location":"api_docs/integrations/#zenml.integrations.facets.visualizers","text":"","title":"visualizers"},{"location":"api_docs/integrations/#zenml.integrations.facets.visualizers.facet_statistics_visualizer","text":"","title":"facet_statistics_visualizer"},{"location":"api_docs/integrations/#zenml.integrations.facets.visualizers.facet_statistics_visualizer.FacetStatisticsVisualizer","text":"The base implementation of a ZenML Visualizer. Source code in zenml/integrations/facets/visualizers/facet_statistics_visualizer.py class FacetStatisticsVisualizer ( BaseStepVisualizer ): \"\"\"The base implementation of a ZenML Visualizer.\"\"\" @abstractmethod def visualize ( self , object : StepView , magic : bool = False , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize components Args: object: StepView fetched from run.get_step(). magic: Whether to render in a Jupyter notebook or not. \"\"\" datasets = [] for output_name , artifact_view in object . outputs . items (): df = artifact_view . read () if type ( df ) is not pd . DataFrame : logger . warning ( \"` %s ` is not a pd.DataFrame. You can only visualize \" \"statistics of steps that output pandas dataframes. \" \"Skipping this output..\" % output_name ) else : datasets . append ({ \"name\" : output_name , \"table\" : df }) h = self . generate_html ( datasets ) self . generate_facet ( h , magic ) def generate_html ( self , datasets : List [ Dict [ Text , pd . DataFrame ]]) -> str : \"\"\"Generates html for facet. Args: datasets: List of dicts of dataframes to be visualized as stats. Returns: HTML template with proto string embedded. \"\"\" proto = GenericFeatureStatisticsGenerator () . ProtoFromDataFrames ( datasets ) protostr = base64 . b64encode ( proto . SerializeToString ()) . decode ( \"utf-8\" ) template = os . path . join ( os . path . abspath ( os . path . dirname ( __file__ )), \"stats.html\" , ) html_template = zenml . io . utils . read_file_contents_as_string ( template ) html_ = html_template . replace ( \"protostr\" , protostr ) return html_ def generate_facet ( self , html_ : str , magic : bool = False ) -> None : \"\"\"Generate a Facet Overview Args: h: HTML represented as a string. magic: Whether to magically materialize facet in a notebook. \"\"\" if magic : if not self . running_in_notebook (): raise EnvironmentError ( \"The magic functions are only usable in a Jupyter notebook.\" ) display ( HTML ( html_ )) else : with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : zenml . io . utils . write_file_contents_as_string ( f . name , html_ ) url = f \"file:/// { f . name } \" logger . info ( \"Opening %s in a new browser..\" % f . name ) webbrowser . open ( url , new = 2 ) generate_facet ( self , html_ , magic = False ) Generate a Facet Overview Parameters: Name Type Description Default h HTML represented as a string. required magic bool Whether to magically materialize facet in a notebook. False Source code in zenml/integrations/facets/visualizers/facet_statistics_visualizer.py def generate_facet ( self , html_ : str , magic : bool = False ) -> None : \"\"\"Generate a Facet Overview Args: h: HTML represented as a string. magic: Whether to magically materialize facet in a notebook. \"\"\" if magic : if not self . running_in_notebook (): raise EnvironmentError ( \"The magic functions are only usable in a Jupyter notebook.\" ) display ( HTML ( html_ )) else : with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : zenml . io . utils . write_file_contents_as_string ( f . name , html_ ) url = f \"file:/// { f . name } \" logger . info ( \"Opening %s in a new browser..\" % f . name ) webbrowser . open ( url , new = 2 ) generate_html ( self , datasets ) Generates html for facet. Parameters: Name Type Description Default datasets List[Dict[str, pandas.core.frame.DataFrame]] List of dicts of dataframes to be visualized as stats. required Returns: Type Description str HTML template with proto string embedded. Source code in zenml/integrations/facets/visualizers/facet_statistics_visualizer.py def generate_html ( self , datasets : List [ Dict [ Text , pd . DataFrame ]]) -> str : \"\"\"Generates html for facet. Args: datasets: List of dicts of dataframes to be visualized as stats. Returns: HTML template with proto string embedded. \"\"\" proto = GenericFeatureStatisticsGenerator () . ProtoFromDataFrames ( datasets ) protostr = base64 . b64encode ( proto . SerializeToString ()) . decode ( \"utf-8\" ) template = os . path . join ( os . path . abspath ( os . path . dirname ( __file__ )), \"stats.html\" , ) html_template = zenml . io . utils . read_file_contents_as_string ( template ) html_ = html_template . replace ( \"protostr\" , protostr ) return html_ visualize ( self , object , magic = False , * args , ** kwargs ) Method to visualize components Parameters: Name Type Description Default object StepView StepView fetched from run.get_step(). required magic bool Whether to render in a Jupyter notebook or not. False Source code in zenml/integrations/facets/visualizers/facet_statistics_visualizer.py @abstractmethod def visualize ( self , object : StepView , magic : bool = False , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize components Args: object: StepView fetched from run.get_step(). magic: Whether to render in a Jupyter notebook or not. \"\"\" datasets = [] for output_name , artifact_view in object . outputs . items (): df = artifact_view . read () if type ( df ) is not pd . DataFrame : logger . warning ( \"` %s ` is not a pd.DataFrame. You can only visualize \" \"statistics of steps that output pandas dataframes. \" \"Skipping this output..\" % output_name ) else : datasets . append ({ \"name\" : output_name , \"table\" : df }) h = self . generate_html ( datasets ) self . generate_facet ( h , magic )","title":"FacetStatisticsVisualizer"},{"location":"api_docs/integrations/#zenml.integrations.gcp","text":"The GCP integration submodule provides a way to run ZenML pipelines in a cloud environment. Specifically, it allows the use of cloud artifact stores, metadata stores, and an io module to handle file operations on Google Cloud Storage (GCS).","title":"gcp"},{"location":"api_docs/integrations/#zenml.integrations.gcp.GcpIntegration","text":"Definition of Google Cloud Platform integration for ZenML. Source code in zenml/integrations/gcp/__init__.py class GcpIntegration ( Integration ): \"\"\"Definition of Google Cloud Platform integration for ZenML.\"\"\" NAME = GCP REQUIREMENTS = [ \"gcsfs\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.gcp import artifact_stores # noqa from zenml.integrations.gcp import io # noqa","title":"GcpIntegration"},{"location":"api_docs/integrations/#zenml.integrations.gcp.GcpIntegration.activate","text":"Activates the integration. Source code in zenml/integrations/gcp/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.gcp import artifact_stores # noqa from zenml.integrations.gcp import io # noqa","title":"activate()"},{"location":"api_docs/integrations/#zenml.integrations.gcp.artifact_stores","text":"","title":"artifact_stores"},{"location":"api_docs/integrations/#zenml.integrations.gcp.artifact_stores.gcp_artifact_store","text":"","title":"gcp_artifact_store"},{"location":"api_docs/integrations/#zenml.integrations.gcp.artifact_stores.gcp_artifact_store.GCPArtifactStore","text":"Artifact Store for Google Cloud Storage based artifacts. Source code in zenml/integrations/gcp/artifact_stores/gcp_artifact_store.py class GCPArtifactStore ( BaseArtifactStore ): \"\"\"Artifact Store for Google Cloud Storage based artifacts.\"\"\" supports_local_execution = True supports_remote_execution = True @property def flavor ( self ) -> ArtifactStoreFlavor : \"\"\"The artifact store flavor.\"\"\" return ArtifactStoreFlavor . GCP @validator ( \"path\" ) def ensure_gcs_path ( cls , path : str ) -> str : \"\"\"Ensures that the path is a valid gcs path.\"\"\" if not path . startswith ( \"gs://\" ): raise ValueError ( f \"Path ' { path } ' specified for GCPArtifactStore is not a \" f \"valid gcs path, i.e., starting with `gs://`.\" ) return path flavor : ArtifactStoreFlavor property readonly The artifact store flavor. ensure_gcs_path ( path ) classmethod Ensures that the path is a valid gcs path. Source code in zenml/integrations/gcp/artifact_stores/gcp_artifact_store.py @validator ( \"path\" ) def ensure_gcs_path ( cls , path : str ) -> str : \"\"\"Ensures that the path is a valid gcs path.\"\"\" if not path . startswith ( \"gs://\" ): raise ValueError ( f \"Path ' { path } ' specified for GCPArtifactStore is not a \" f \"valid gcs path, i.e., starting with `gs://`.\" ) return path","title":"GCPArtifactStore"},{"location":"api_docs/integrations/#zenml.integrations.gcp.io","text":"","title":"io"},{"location":"api_docs/integrations/#zenml.integrations.gcp.io.gcs_plugin","text":"Plugin which is created to add Google Cloud Store support to ZenML. It inherits from the base Filesystem created by TFX and overwrites the corresponding functions thanks to gcsfs.","title":"gcs_plugin"},{"location":"api_docs/integrations/#zenml.integrations.gcp.io.gcs_plugin.ZenGCS","text":"Filesystem that delegates to Google Cloud Store using gcsfs. Note : To allow TFX to check for various error conditions, we need to raise their custom NotFoundError instead of the builtin python FileNotFoundError. Source code in zenml/integrations/gcp/io/gcs_plugin.py class ZenGCS ( Filesystem ): \"\"\"Filesystem that delegates to Google Cloud Store using gcsfs. **Note**: To allow TFX to check for various error conditions, we need to raise their custom `NotFoundError` instead of the builtin python FileNotFoundError.\"\"\" SUPPORTED_SCHEMES = [ \"gs://\" ] fs : gcsfs . GCSFileSystem = None @classmethod def _ensure_filesystem_set ( cls ) -> None : \"\"\"Ensures that the filesystem is set.\"\"\" if ZenGCS . fs is None : ZenGCS . fs = gcsfs . GCSFileSystem () @staticmethod def open ( path : PathType , mode : str = \"r\" ) -> Any : \"\"\"Open a file at the given path. Args: path: Path of the file to open. mode: Mode in which to open the file. Currently only 'rb' and 'wb' to read and write binary files are supported. \"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . open ( path = path , mode = mode ) except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def copy ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Copy a file. Args: src: The path to copy from. dst: The path to copy to. overwrite: If a file already exists at the destination, this method will overwrite it if overwrite=`True` and raise a FileExistsError otherwise. Raises: FileNotFoundError: If the source file does not exist. FileExistsError: If a file already exists at the destination and overwrite is not set to `True`. \"\"\" ZenGCS . _ensure_filesystem_set () if not overwrite and ZenGCS . fs . exists ( dst ): raise FileExistsError ( f \"Unable to copy to destination ' { convert_to_str ( dst ) } ', \" f \"file already exists. Set `overwrite=True` to copy anyway.\" ) # TODO [ENG-151]: Check if it works with overwrite=True or if we need to # manually remove it first try : ZenGCS . fs . copy ( path1 = src , path2 = dst ) except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def exists ( path : PathType ) -> bool : \"\"\"Check whether a path exists.\"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . exists ( path = path ) # type: ignore[no-any-return] @staticmethod def glob ( pattern : PathType ) -> List [ PathType ]: \"\"\"Return all paths that match the given glob pattern. The glob pattern may include: - '*' to match any number of characters - '?' to match a single character - '[...]' to match one of the characters inside the brackets - '**' as the full name of a path component to match to search in subdirectories of any depth (e.g. '/some_dir/**/some_file) Args: pattern: The glob pattern to match, see details above. Returns: A list of paths that match the given glob pattern. \"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . glob ( path = pattern ) # type: ignore[no-any-return] @staticmethod def isdir ( path : PathType ) -> bool : \"\"\"Check whether a path is a directory.\"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . isdir ( path = path ) # type: ignore[no-any-return] @staticmethod def listdir ( path : PathType ) -> List [ PathType ]: \"\"\"Return a list of files in a directory.\"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . listdir ( path = path ) # type: ignore[no-any-return] except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def makedirs ( path : PathType ) -> None : \"\"\"Create a directory at the given path. If needed also create missing parent directories.\"\"\" ZenGCS . _ensure_filesystem_set () ZenGCS . fs . makedirs ( path = path , exist_ok = True ) @staticmethod def mkdir ( path : PathType ) -> None : \"\"\"Create a directory at the given path.\"\"\" ZenGCS . _ensure_filesystem_set () ZenGCS . fs . makedir ( path = path ) @staticmethod def remove ( path : PathType ) -> None : \"\"\"Remove the file at the given path.\"\"\" ZenGCS . _ensure_filesystem_set () try : ZenGCS . fs . rm_file ( path = path ) except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def rename ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Rename source file to destination file. Args: src: The path of the file to rename. dst: The path to rename the source file to. overwrite: If a file already exists at the destination, this method will overwrite it if overwrite=`True` and raise a FileExistsError otherwise. Raises: FileNotFoundError: If the source file does not exist. FileExistsError: If a file already exists at the destination and overwrite is not set to `True`. \"\"\" ZenGCS . _ensure_filesystem_set () if not overwrite and ZenGCS . fs . exists ( dst ): raise FileExistsError ( f \"Unable to rename file to ' { convert_to_str ( dst ) } ', \" f \"file already exists. Set `overwrite=True` to rename anyway.\" ) # TODO [ENG-152]: Check if it works with overwrite=True or if we need # to manually remove it first try : ZenGCS . fs . rename ( path1 = src , path2 = dst ) except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def rmtree ( path : PathType ) -> None : \"\"\"Remove the given directory.\"\"\" ZenGCS . _ensure_filesystem_set () try : ZenGCS . fs . delete ( path = path , recursive = True ) except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def stat ( path : PathType ) -> Dict [ str , Any ]: \"\"\"Return stat info for the given path.\"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . stat ( path = path ) # type: ignore[no-any-return] except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def walk ( top : PathType , topdown : bool = True , onerror : Optional [ Callable [ ... , None ]] = None , ) -> Iterable [ Tuple [ PathType , List [ PathType ], List [ PathType ]]]: \"\"\"Return an iterator that walks the contents of the given directory. Args: top: Path of directory to walk. topdown: Unused argument to conform to interface. onerror: Unused argument to conform to interface. Returns: An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. \"\"\" ZenGCS . _ensure_filesystem_set () # TODO [ENG-153]: Additional params return ZenGCS . fs . walk ( path = top ) # type: ignore[no-any-return] copy ( src , dst , overwrite = False ) staticmethod Copy a file. Parameters: Name Type Description Default src Union[bytes, str] The path to copy from. required dst Union[bytes, str] The path to copy to. required overwrite bool If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. False Exceptions: Type Description FileNotFoundError If the source file does not exist. FileExistsError If a file already exists at the destination and overwrite is not set to True . Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def copy ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Copy a file. Args: src: The path to copy from. dst: The path to copy to. overwrite: If a file already exists at the destination, this method will overwrite it if overwrite=`True` and raise a FileExistsError otherwise. Raises: FileNotFoundError: If the source file does not exist. FileExistsError: If a file already exists at the destination and overwrite is not set to `True`. \"\"\" ZenGCS . _ensure_filesystem_set () if not overwrite and ZenGCS . fs . exists ( dst ): raise FileExistsError ( f \"Unable to copy to destination ' { convert_to_str ( dst ) } ', \" f \"file already exists. Set `overwrite=True` to copy anyway.\" ) # TODO [ENG-151]: Check if it works with overwrite=True or if we need to # manually remove it first try : ZenGCS . fs . copy ( path1 = src , path2 = dst ) except FileNotFoundError as e : raise NotFoundError () from e exists ( path ) staticmethod Check whether a path exists. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def exists ( path : PathType ) -> bool : \"\"\"Check whether a path exists.\"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . exists ( path = path ) # type: ignore[no-any-return] glob ( pattern ) staticmethod Return all paths that match the given glob pattern. The glob pattern may include: - ' ' to match any number of characters - '?' to match a single character - '[...]' to match one of the characters inside the brackets - ' ' as the full name of a path component to match to search in subdirectories of any depth (e.g. '/some_dir/ */some_file) Parameters: Name Type Description Default pattern Union[bytes, str] The glob pattern to match, see details above. required Returns: Type Description List[Union[bytes, str]] A list of paths that match the given glob pattern. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def glob ( pattern : PathType ) -> List [ PathType ]: \"\"\"Return all paths that match the given glob pattern. The glob pattern may include: - '*' to match any number of characters - '?' to match a single character - '[...]' to match one of the characters inside the brackets - '**' as the full name of a path component to match to search in subdirectories of any depth (e.g. '/some_dir/**/some_file) Args: pattern: The glob pattern to match, see details above. Returns: A list of paths that match the given glob pattern. \"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . glob ( path = pattern ) # type: ignore[no-any-return] isdir ( path ) staticmethod Check whether a path is a directory. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def isdir ( path : PathType ) -> bool : \"\"\"Check whether a path is a directory.\"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . isdir ( path = path ) # type: ignore[no-any-return] listdir ( path ) staticmethod Return a list of files in a directory. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def listdir ( path : PathType ) -> List [ PathType ]: \"\"\"Return a list of files in a directory.\"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . listdir ( path = path ) # type: ignore[no-any-return] except FileNotFoundError as e : raise NotFoundError () from e makedirs ( path ) staticmethod Create a directory at the given path. If needed also create missing parent directories. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def makedirs ( path : PathType ) -> None : \"\"\"Create a directory at the given path. If needed also create missing parent directories.\"\"\" ZenGCS . _ensure_filesystem_set () ZenGCS . fs . makedirs ( path = path , exist_ok = True ) mkdir ( path ) staticmethod Create a directory at the given path. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def mkdir ( path : PathType ) -> None : \"\"\"Create a directory at the given path.\"\"\" ZenGCS . _ensure_filesystem_set () ZenGCS . fs . makedir ( path = path ) open ( path , mode = 'r' ) staticmethod Open a file at the given path. Parameters: Name Type Description Default path Union[bytes, str] Path of the file to open. required mode str Mode in which to open the file. Currently only 'rb' and 'wb' to read and write binary files are supported. 'r' Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def open ( path : PathType , mode : str = \"r\" ) -> Any : \"\"\"Open a file at the given path. Args: path: Path of the file to open. mode: Mode in which to open the file. Currently only 'rb' and 'wb' to read and write binary files are supported. \"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . open ( path = path , mode = mode ) except FileNotFoundError as e : raise NotFoundError () from e remove ( path ) staticmethod Remove the file at the given path. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def remove ( path : PathType ) -> None : \"\"\"Remove the file at the given path.\"\"\" ZenGCS . _ensure_filesystem_set () try : ZenGCS . fs . rm_file ( path = path ) except FileNotFoundError as e : raise NotFoundError () from e rename ( src , dst , overwrite = False ) staticmethod Rename source file to destination file. Parameters: Name Type Description Default src Union[bytes, str] The path of the file to rename. required dst Union[bytes, str] The path to rename the source file to. required overwrite bool If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. False Exceptions: Type Description FileNotFoundError If the source file does not exist. FileExistsError If a file already exists at the destination and overwrite is not set to True . Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def rename ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Rename source file to destination file. Args: src: The path of the file to rename. dst: The path to rename the source file to. overwrite: If a file already exists at the destination, this method will overwrite it if overwrite=`True` and raise a FileExistsError otherwise. Raises: FileNotFoundError: If the source file does not exist. FileExistsError: If a file already exists at the destination and overwrite is not set to `True`. \"\"\" ZenGCS . _ensure_filesystem_set () if not overwrite and ZenGCS . fs . exists ( dst ): raise FileExistsError ( f \"Unable to rename file to ' { convert_to_str ( dst ) } ', \" f \"file already exists. Set `overwrite=True` to rename anyway.\" ) # TODO [ENG-152]: Check if it works with overwrite=True or if we need # to manually remove it first try : ZenGCS . fs . rename ( path1 = src , path2 = dst ) except FileNotFoundError as e : raise NotFoundError () from e rmtree ( path ) staticmethod Remove the given directory. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def rmtree ( path : PathType ) -> None : \"\"\"Remove the given directory.\"\"\" ZenGCS . _ensure_filesystem_set () try : ZenGCS . fs . delete ( path = path , recursive = True ) except FileNotFoundError as e : raise NotFoundError () from e stat ( path ) staticmethod Return stat info for the given path. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def stat ( path : PathType ) -> Dict [ str , Any ]: \"\"\"Return stat info for the given path.\"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . stat ( path = path ) # type: ignore[no-any-return] except FileNotFoundError as e : raise NotFoundError () from e walk ( top , topdown = True , onerror = None ) staticmethod Return an iterator that walks the contents of the given directory. Parameters: Name Type Description Default top Union[bytes, str] Path of directory to walk. required topdown bool Unused argument to conform to interface. True onerror Optional[Callable[..., NoneType]] Unused argument to conform to interface. None Returns: Type Description Iterable[Tuple[Union[bytes, str], List[Union[bytes, str]], List[Union[bytes, str]]]] An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def walk ( top : PathType , topdown : bool = True , onerror : Optional [ Callable [ ... , None ]] = None , ) -> Iterable [ Tuple [ PathType , List [ PathType ], List [ PathType ]]]: \"\"\"Return an iterator that walks the contents of the given directory. Args: top: Path of directory to walk. topdown: Unused argument to conform to interface. onerror: Unused argument to conform to interface. Returns: An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. \"\"\" ZenGCS . _ensure_filesystem_set () # TODO [ENG-153]: Additional params return ZenGCS . fs . walk ( path = top ) # type: ignore[no-any-return]","title":"ZenGCS"},{"location":"api_docs/integrations/#zenml.integrations.graphviz","text":"","title":"graphviz"},{"location":"api_docs/integrations/#zenml.integrations.graphviz.GraphvizIntegration","text":"Definition of Graphviz integration for ZenML. Source code in zenml/integrations/graphviz/__init__.py class GraphvizIntegration ( Integration ): \"\"\"Definition of Graphviz integration for ZenML.\"\"\" NAME = GRAPHVIZ REQUIREMENTS = [ \"graphviz>=0.17\" ] SYSTEM_REQUIREMENTS = { \"graphviz\" : \"dot\" }","title":"GraphvizIntegration"},{"location":"api_docs/integrations/#zenml.integrations.graphviz.visualizers","text":"","title":"visualizers"},{"location":"api_docs/integrations/#zenml.integrations.graphviz.visualizers.pipeline_run_dag_visualizer","text":"","title":"pipeline_run_dag_visualizer"},{"location":"api_docs/integrations/#zenml.integrations.graphviz.visualizers.pipeline_run_dag_visualizer.PipelineRunDagVisualizer","text":"Visualize the lineage of runs in a pipeline. Source code in zenml/integrations/graphviz/visualizers/pipeline_run_dag_visualizer.py class PipelineRunDagVisualizer ( BasePipelineRunVisualizer ): \"\"\"Visualize the lineage of runs in a pipeline.\"\"\" ARTIFACT_DEFAULT_COLOR = \"blue\" ARTIFACT_CACHED_COLOR = \"green\" ARTIFACT_SHAPE = \"box\" ARTIFACT_PREFIX = \"artifact_\" STEP_COLOR = \"#431D93\" STEP_SHAPE = \"ellipse\" STEP_PREFIX = \"step_\" FONT = \"Roboto\" @abstractmethod def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> graphviz . Digraph : \"\"\"Creates a pipeline lineage diagram using graphviz.\"\"\" logger . warning ( \"This integration is not completed yet. Results might be unexpected.\" ) dot = graphviz . Digraph ( comment = object . name ) # link the steps together for step in object . steps : # add each step as a node dot . node ( self . STEP_PREFIX + str ( step . id ), step . entrypoint_name , shape = self . STEP_SHAPE , ) # for each parent of a step, add an edge for artifact_name , artifact in step . outputs . items (): dot . node ( self . ARTIFACT_PREFIX + str ( artifact . id ), f \" { artifact_name } \\n \" f \"( { artifact . _data_type } )\" , shape = self . ARTIFACT_SHAPE , ) dot . edge ( self . STEP_PREFIX + str ( step . id ), self . ARTIFACT_PREFIX + str ( artifact . id ), ) for artifact_name , artifact in step . inputs . items (): dot . edge ( self . ARTIFACT_PREFIX + str ( artifact . id ), self . STEP_PREFIX + str ( step . id ), ) with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : dot . render ( filename = f . name , format = \"png\" , view = True , cleanup = True ) return dot visualize ( self , object , * args , ** kwargs ) Creates a pipeline lineage diagram using graphviz. Source code in zenml/integrations/graphviz/visualizers/pipeline_run_dag_visualizer.py @abstractmethod def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> graphviz . Digraph : \"\"\"Creates a pipeline lineage diagram using graphviz.\"\"\" logger . warning ( \"This integration is not completed yet. Results might be unexpected.\" ) dot = graphviz . Digraph ( comment = object . name ) # link the steps together for step in object . steps : # add each step as a node dot . node ( self . STEP_PREFIX + str ( step . id ), step . entrypoint_name , shape = self . STEP_SHAPE , ) # for each parent of a step, add an edge for artifact_name , artifact in step . outputs . items (): dot . node ( self . ARTIFACT_PREFIX + str ( artifact . id ), f \" { artifact_name } \\n \" f \"( { artifact . _data_type } )\" , shape = self . ARTIFACT_SHAPE , ) dot . edge ( self . STEP_PREFIX + str ( step . id ), self . ARTIFACT_PREFIX + str ( artifact . id ), ) for artifact_name , artifact in step . inputs . items (): dot . edge ( self . ARTIFACT_PREFIX + str ( artifact . id ), self . STEP_PREFIX + str ( step . id ), ) with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : dot . render ( filename = f . name , format = \"png\" , view = True , cleanup = True ) return dot","title":"PipelineRunDagVisualizer"},{"location":"api_docs/integrations/#zenml.integrations.integration","text":"","title":"integration"},{"location":"api_docs/integrations/#zenml.integrations.integration.Integration","text":"Base class for integration in ZenML Source code in zenml/integrations/integration.py class Integration ( metaclass = IntegrationMeta ): \"\"\"Base class for integration in ZenML\"\"\" NAME = \"base_integration\" REQUIREMENTS : List [ str ] = [] SYSTEM_REQUIREMENTS : Dict [ str , str ] = {} @classmethod def check_installation ( cls ) -> bool : \"\"\"Method to check whether the required packages are installed\"\"\" try : for requirement , command in cls . SYSTEM_REQUIREMENTS . items (): result = shutil . which ( command ) if result is None : logger . debug ( \"Unable to find the required packages for %s on your \" \"system. Please install the packages on your system \" \"and try again.\" , requirement , ) return False for r in cls . REQUIREMENTS : pkg_resources . get_distribution ( r ) logger . debug ( f \"Integration { cls . NAME } is installed correctly with \" f \"requirements { cls . REQUIREMENTS } .\" ) return True except pkg_resources . DistributionNotFound as e : logger . debug ( f \"Unable to find required package ' { e . req } ' for \" f \"integration { cls . NAME } .\" ) return False except pkg_resources . VersionConflict as e : logger . debug ( f \"VersionConflict error when loading installation { cls . NAME } : \" f \" { str ( e ) } \" ) return False @staticmethod def activate () -> None : \"\"\"Abstract method to activate the integration\"\"\"","title":"Integration"},{"location":"api_docs/integrations/#zenml.integrations.integration.Integration.activate","text":"Abstract method to activate the integration Source code in zenml/integrations/integration.py @staticmethod def activate () -> None : \"\"\"Abstract method to activate the integration\"\"\"","title":"activate()"},{"location":"api_docs/integrations/#zenml.integrations.integration.Integration.check_installation","text":"Method to check whether the required packages are installed Source code in zenml/integrations/integration.py @classmethod def check_installation ( cls ) -> bool : \"\"\"Method to check whether the required packages are installed\"\"\" try : for requirement , command in cls . SYSTEM_REQUIREMENTS . items (): result = shutil . which ( command ) if result is None : logger . debug ( \"Unable to find the required packages for %s on your \" \"system. Please install the packages on your system \" \"and try again.\" , requirement , ) return False for r in cls . REQUIREMENTS : pkg_resources . get_distribution ( r ) logger . debug ( f \"Integration { cls . NAME } is installed correctly with \" f \"requirements { cls . REQUIREMENTS } .\" ) return True except pkg_resources . DistributionNotFound as e : logger . debug ( f \"Unable to find required package ' { e . req } ' for \" f \"integration { cls . NAME } .\" ) return False except pkg_resources . VersionConflict as e : logger . debug ( f \"VersionConflict error when loading installation { cls . NAME } : \" f \" { str ( e ) } \" ) return False","title":"check_installation()"},{"location":"api_docs/integrations/#zenml.integrations.integration.IntegrationMeta","text":"Metaclass responsible for registering different Integration subclasses Source code in zenml/integrations/integration.py class IntegrationMeta ( type ): \"\"\"Metaclass responsible for registering different Integration subclasses\"\"\" def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"IntegrationMeta\" : \"\"\"Hook into creation of an Integration class.\"\"\" cls = cast ( Type [ \"Integration\" ], super () . __new__ ( mcs , name , bases , dct )) if name != \"Integration\" : integration_registry . register_integration ( cls . NAME , cls ) return cls","title":"IntegrationMeta"},{"location":"api_docs/integrations/#zenml.integrations.integration.IntegrationMeta.__new__","text":"Hook into creation of an Integration class. Source code in zenml/integrations/integration.py def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"IntegrationMeta\" : \"\"\"Hook into creation of an Integration class.\"\"\" cls = cast ( Type [ \"Integration\" ], super () . __new__ ( mcs , name , bases , dct )) if name != \"Integration\" : integration_registry . register_integration ( cls . NAME , cls ) return cls","title":"__new__()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow","text":"The Kubeflow integration sub-module powers an alternative to the local orchestrator. You can enable it by registering the Kubeflow orchestrator with the CLI tool.","title":"kubeflow"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.KubeflowIntegration","text":"Definition of Kubeflow Integration for ZenML. Source code in zenml/integrations/kubeflow/__init__.py class KubeflowIntegration ( Integration ): \"\"\"Definition of Kubeflow Integration for ZenML.\"\"\" NAME = KUBEFLOW REQUIREMENTS = [ \"kfp==1.8.9\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates all classes required for the airflow integration.\"\"\" from zenml.integrations.kubeflow import metadata_stores # noqa from zenml.integrations.kubeflow import orchestrators # noqa","title":"KubeflowIntegration"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.KubeflowIntegration.activate","text":"Activates all classes required for the airflow integration. Source code in zenml/integrations/kubeflow/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates all classes required for the airflow integration.\"\"\" from zenml.integrations.kubeflow import metadata_stores # noqa from zenml.integrations.kubeflow import orchestrators # noqa","title":"activate()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.container_entrypoint","text":"Main entrypoint for containers with Kubeflow TFX component executors.","title":"container_entrypoint"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.container_entrypoint.main","text":"Runs a single step defined by the command line arguments. Source code in zenml/integrations/kubeflow/container_entrypoint.py def main () -> None : \"\"\"Runs a single step defined by the command line arguments.\"\"\" # Log to the container's stdout so Kubeflow Pipelines UI can display logs to # the user. logging . basicConfig ( stream = sys . stdout , level = logging . INFO ) logging . getLogger () . setLevel ( logging . INFO ) args = _parse_command_line_arguments () tfx_pipeline = pipeline_pb2 . Pipeline () json_format . Parse ( args . tfx_ir , tfx_pipeline ) _resolve_runtime_parameters ( tfx_pipeline , args . run_name , args . runtime_parameter ) node_id = args . node_id pipeline_node = _get_pipeline_node ( tfx_pipeline , node_id ) deployment_config = runner_utils . extract_local_deployment_config ( tfx_pipeline ) executor_spec = runner_utils . extract_executor_spec ( deployment_config , node_id ) custom_driver_spec = runner_utils . extract_custom_driver_spec ( deployment_config , node_id ) custom_executor_operators = { executable_spec_pb2 . ContainerExecutableSpec : kubernetes_executor_operator . KubernetesExecutorOperator } # make sure all integrations are activated so all materializers etc. are # available integration_registry . activate_integrations () metadata_store = Repository () . active_stack . metadata_store metadata_connection = metadata . Metadata ( metadata_store . get_tfx_metadata_config () ) # import the user main module to register all the materializers importlib . import_module ( args . main_module ) if hasattr ( executor_spec , \"class_path\" ): executor_module_parts = getattr ( executor_spec , \"class_path\" ) . split ( \".\" ) executor_class_target_module_name = \".\" . join ( executor_module_parts [: - 1 ]) _create_executor_class ( step_source_module_name = args . step_module , step_function_name = args . step_function_name , executor_class_target_module_name = executor_class_target_module_name , input_artifact_type_mapping = json . loads ( args . input_artifact_types ), ) else : raise RuntimeError ( f \"No class path found inside executor spec: { executor_spec } .\" ) component_launcher = launcher . Launcher ( pipeline_node = pipeline_node , mlmd_connection = metadata_connection , pipeline_info = tfx_pipeline . pipeline_info , pipeline_runtime_spec = tfx_pipeline . runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , custom_executor_operators = custom_executor_operators , ) execution_info = execute_step ( component_launcher ) if execution_info : _dump_ui_metadata ( pipeline_node , execution_info , args . metadata_ui_path )","title":"main()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.docker_utils","text":"","title":"docker_utils"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.docker_utils.build_docker_image","text":"Builds a docker image. Parameters: Name Type Description Default build_context_path str Path to a directory that will be sent to the docker daemon as build context. required image_name str The name to use for the created docker image. required dockerfile_path Optional[str] Optional path to a dockerfile. If no value is given, a temporary dockerfile will be created. None dockerignore_path Optional[str] Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside build_context_path are included in the build context. None requirements Optional[AbstractSet[str]] Optional list of pip requirements to install. This will only be used if no value is given for dockerfile_path . None use_local_requirements bool If True and no values are given for dockerfile_path and requirements , then the packages installed in the environment of the current python processed will be installed in the docker image. False base_image Optional[str] The image to use as base for the docker image. None Source code in zenml/integrations/kubeflow/docker_utils.py def build_docker_image ( build_context_path : str , image_name : str , dockerfile_path : Optional [ str ] = None , dockerignore_path : Optional [ str ] = None , requirements : Optional [ AbstractSet [ str ]] = None , use_local_requirements : bool = False , base_image : Optional [ str ] = None , ) -> None : \"\"\"Builds a docker image. Args: build_context_path: Path to a directory that will be sent to the docker daemon as build context. image_name: The name to use for the created docker image. dockerfile_path: Optional path to a dockerfile. If no value is given, a temporary dockerfile will be created. dockerignore_path: Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside `build_context_path` are included in the build context. requirements: Optional list of pip requirements to install. This will only be used if no value is given for `dockerfile_path`. use_local_requirements: If `True` and no values are given for `dockerfile_path` and `requirements`, then the packages installed in the environment of the current python processed will be installed in the docker image. base_image: The image to use as base for the docker image. \"\"\" if not requirements and use_local_requirements : local_requirements = get_current_environment_requirements () requirements = { f \" { package } == { version } \" for package , version in local_requirements . items () if package != \"zenml\" # exclude ZenML } logger . info ( \"Using requirements from local environment to build \" \"docker image: %s \" , requirements , ) if dockerfile_path : dockerfile_contents = zenml . io . utils . read_file_contents_as_string ( dockerfile_path ) else : dockerfile_contents = generate_dockerfile_contents ( requirements = requirements , base_image = base_image or DEFAULT_BASE_IMAGE , ) build_context = create_custom_build_context ( build_context_path = build_context_path , dockerfile_contents = dockerfile_contents , dockerignore_path = dockerignore_path , ) # If a custom base image is provided, make sure to always pull the # latest version of that image. If no base image is provided, we use # the static default ZenML image so there is no need to constantly pull always_pull_base_image = bool ( base_image ) logger . info ( \"Building docker image ' %s ', this might take a while...\" , image_name ) docker_client = DockerClient . from_env () # We use the client api directly here so we can stream the logs output_stream = docker_client . images . client . api . build ( fileobj = build_context , custom_context = True , tag = image_name , pull = always_pull_base_image , rm = False , # don't remove intermediate containers ) _process_stream ( output_stream ) logger . info ( \"Finished building docker image.\" )","title":"build_docker_image()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.docker_utils.create_custom_build_context","text":"Creates a docker build context. Parameters: Name Type Description Default build_context_path str Path to a directory that will be sent to the docker daemon as build context. required dockerfile_contents str File contents of the Dockerfile to use for the build. required dockerignore_path Optional[str] Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside build_context_path are included in the build context. None Returns: Type Description Any Docker build context that can be passed when building a docker image. Source code in zenml/integrations/kubeflow/docker_utils.py def create_custom_build_context ( build_context_path : str , dockerfile_contents : str , dockerignore_path : Optional [ str ] = None , ) -> Any : \"\"\"Creates a docker build context. Args: build_context_path: Path to a directory that will be sent to the docker daemon as build context. dockerfile_contents: File contents of the Dockerfile to use for the build. dockerignore_path: Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside `build_context_path` are included in the build context. Returns: Docker build context that can be passed when building a docker image. \"\"\" exclude_patterns = [] default_dockerignore_path = os . path . join ( build_context_path , \".dockerignore\" ) if dockerignore_path : exclude_patterns = _parse_dockerignore ( dockerignore_path ) elif fileio . file_exists ( default_dockerignore_path ): logger . info ( \"Using dockerignore found at path ' %s ' to create docker \" \"build context.\" , default_dockerignore_path , ) exclude_patterns = _parse_dockerignore ( default_dockerignore_path ) else : logger . info ( \"No explicit dockerignore specified and no file called \" \".dockerignore exists at the build context root ( %s ).\" \"Creating docker build context with all files inside the build \" \"context root directory.\" , build_context_path , ) logger . debug ( \"Exclude patterns for creating docker build context: %s \" , exclude_patterns , ) no_ignores_found = not exclude_patterns files = docker_build_utils . exclude_paths ( build_context_path , patterns = exclude_patterns ) extra_files = [( \"Dockerfile\" , dockerfile_contents )] context = docker_build_utils . create_archive ( root = build_context_path , files = sorted ( files ), gzip = False , extra_files = extra_files , ) build_context_size = os . path . getsize ( context . name ) if build_context_size > 50 * 1024 * 1024 and no_ignores_found : # The build context exceeds 50MiB and we didn't find any excludes # in dockerignore files -> remind to specify a .dockerignore file logger . warning ( \"Build context size for docker image: %s . If you believe this is \" \"unreasonably large, make sure to include a .dockerignore file at \" \"the root of your build context ( %s ) or specify a custom file \" \"when defining your pipeline.\" , string_utils . get_human_readable_filesize ( build_context_size ), default_dockerignore_path , ) return context","title":"create_custom_build_context()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.docker_utils.generate_dockerfile_contents","text":"Generates a Dockerfile. Parameters: Name Type Description Default base_image str The image to use as base for the dockerfile. required command Optional[str] The default command that gets executed when running a container of an image created by this dockerfile. None requirements Optional[AbstractSet[str]] Optional list of pip requirements to install. None Returns: Type Description str Content of a dockerfile. Source code in zenml/integrations/kubeflow/docker_utils.py def generate_dockerfile_contents ( base_image : str , command : Optional [ str ] = None , requirements : Optional [ AbstractSet [ str ]] = None , ) -> str : \"\"\"Generates a Dockerfile. Args: base_image: The image to use as base for the dockerfile. command: The default command that gets executed when running a container of an image created by this dockerfile. requirements: Optional list of pip requirements to install. Returns: Content of a dockerfile. \"\"\" lines = [ f \"FROM { base_image } \" , \"WORKDIR /app\" ] if requirements : lines . extend ( [ f \"RUN pip install --no-cache { ' ' . join ( requirements ) } \" , ] ) lines . append ( \"COPY . .\" ) if command : lines . append ( f \"CMD { command } \" ) return \" \\n \" . join ( lines )","title":"generate_dockerfile_contents()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.docker_utils.get_current_environment_requirements","text":"Returns a dict of package requirements for the environment that the current python process is running in. Source code in zenml/integrations/kubeflow/docker_utils.py def get_current_environment_requirements () -> Dict [ str , str ]: \"\"\"Returns a dict of package requirements for the environment that the current python process is running in.\"\"\" return { distribution . key : distribution . version for distribution in pkg_resources . working_set }","title":"get_current_environment_requirements()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.docker_utils.get_image_digest","text":"Gets the digest of a docker image. Parameters: Name Type Description Default image_name str Name of the image to get the digest for. required Returns: Type Description Optional[str] Returns the repo digest for the given image if there exists exactly one. If there are zero or multiple repo digests, returns None . Source code in zenml/integrations/kubeflow/docker_utils.py def get_image_digest ( image_name : str ) -> Optional [ str ]: \"\"\"Gets the digest of a docker image. Args: image_name: Name of the image to get the digest for. Returns: Returns the repo digest for the given image if there exists exactly one. If there are zero or multiple repo digests, returns `None`. \"\"\" docker_client = DockerClient . from_env () image = docker_client . images . get ( image_name ) repo_digests = image . attrs [ \"RepoDigests\" ] if len ( repo_digests ) == 1 : return cast ( str , repo_digests [ 0 ]) else : logger . debug ( \"Found zero or more repo digests for docker image ' %s ': %s \" , image_name , repo_digests , ) return None","title":"get_image_digest()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.docker_utils.push_docker_image","text":"Pushes a docker image to a container registry. Parameters: Name Type Description Default image_name str The full name (including a tag) of the image to push. required Source code in zenml/integrations/kubeflow/docker_utils.py def push_docker_image ( image_name : str ) -> None : \"\"\"Pushes a docker image to a container registry. Args: image_name: The full name (including a tag) of the image to push. \"\"\" logger . info ( \"Pushing docker image ' %s '.\" , image_name ) docker_client = DockerClient . from_env () output_stream = docker_client . images . push ( image_name , stream = True ) _process_stream ( output_stream ) logger . info ( \"Finished pushing docker image.\" )","title":"push_docker_image()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.metadata_stores","text":"","title":"metadata_stores"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.metadata_stores.kubeflow_metadata_store","text":"","title":"kubeflow_metadata_store"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.metadata_stores.kubeflow_metadata_store.KubeflowMetadataStore","text":"Kubeflow MySQL backend for ZenML metadata store. Source code in zenml/integrations/kubeflow/metadata_stores/kubeflow_metadata_store.py class KubeflowMetadataStore ( MySQLMetadataStore ): \"\"\"Kubeflow MySQL backend for ZenML metadata store.\"\"\" host : str = \"127.0.0.1\" port : int = 3306 database : str = \"metadb\" username : str = \"root\" password : str = \"\" @property def flavor ( self ) -> MetadataStoreFlavor : \"\"\"The metadata store flavor.\"\"\" return MetadataStoreFlavor . KUBEFLOW def get_tfx_metadata_config ( self , ) -> Union [ metadata_store_pb2 . ConnectionConfig , metadata_store_pb2 . MetadataStoreClientConfig , ]: \"\"\"Return tfx metadata config for the kubeflow metadata store.\"\"\" if inside_kfp_pod (): connection_config = metadata_store_pb2 . MetadataStoreClientConfig () connection_config . host = os . environ [ \"METADATA_GRPC_SERVICE_HOST\" ] connection_config . port = int ( os . environ [ \"METADATA_GRPC_SERVICE_PORT\" ] ) return connection_config else : return super () . get_tfx_metadata_config () flavor : MetadataStoreFlavor property readonly The metadata store flavor. get_tfx_metadata_config ( self ) Return tfx metadata config for the kubeflow metadata store. Source code in zenml/integrations/kubeflow/metadata_stores/kubeflow_metadata_store.py def get_tfx_metadata_config ( self , ) -> Union [ metadata_store_pb2 . ConnectionConfig , metadata_store_pb2 . MetadataStoreClientConfig , ]: \"\"\"Return tfx metadata config for the kubeflow metadata store.\"\"\" if inside_kfp_pod (): connection_config = metadata_store_pb2 . MetadataStoreClientConfig () connection_config . host = os . environ [ \"METADATA_GRPC_SERVICE_HOST\" ] connection_config . port = int ( os . environ [ \"METADATA_GRPC_SERVICE_PORT\" ] ) return connection_config else : return super () . get_tfx_metadata_config ()","title":"KubeflowMetadataStore"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.metadata_stores.kubeflow_metadata_store.inside_kfp_pod","text":"Returns if the current python process is running inside a KFP Pod. Source code in zenml/integrations/kubeflow/metadata_stores/kubeflow_metadata_store.py def inside_kfp_pod () -> bool : \"\"\"Returns if the current python process is running inside a KFP Pod.\"\"\" if \"KFP_POD_NAME\" not in os . environ : return False try : k8s_config . load_incluster_config () return True except k8s_config . ConfigException : return False","title":"inside_kfp_pod()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators","text":"","title":"orchestrators"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.kubeflow_component","text":"Kubeflow Pipelines based implementation of TFX components. These components are lightweight wrappers around the KFP DSL's ContainerOp, and ensure that the container gets called with the right set of input arguments. It also ensures that each component exports named output attributes that are consistent with those provided by the native TFX components, thus ensuring that both types of pipeline definitions are compatible. Note: This requires Kubeflow Pipelines SDK to be installed.","title":"kubeflow_component"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.kubeflow_component.KubeflowComponent","text":"Base component for all Kubeflow pipelines TFX components. Returns a wrapper around a KFP DSL ContainerOp class, and adds named output attributes that match the output names for the corresponding native TFX components. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_component.py class KubeflowComponent : \"\"\"Base component for all Kubeflow pipelines TFX components. Returns a wrapper around a KFP DSL ContainerOp class, and adds named output attributes that match the output names for the corresponding native TFX components. \"\"\" def __init__ ( self , component : tfx_base_component . BaseComponent , depends_on : Set [ dsl . ContainerOp ], image : str , tfx_ir : pipeline_pb2 . Pipeline , # type: ignore[valid-type] pod_labels_to_attach : Dict [ str , str ], main_module : str , step_module : str , step_function_name : str , runtime_parameters : List [ data_types . RuntimeParameter ], metadata_ui_path : str = \"/tmp/mlpipeline-ui-metadata.json\" , ): \"\"\"Creates a new Kubeflow-based component. This class essentially wraps a dsl.ContainerOp construct in Kubeflow Pipelines. Args: component: The logical TFX component to wrap. depends_on: The set of upstream KFP ContainerOp components that this component will depend on. image: The container image to use for this component. tfx_ir: The TFX intermedia representation of the pipeline. pod_labels_to_attach: Dict of pod labels to attach to the GKE pod. runtime_parameters: Runtime parameters of the pipeline. metadata_ui_path: File location for metadata-ui-metadata.json file. \"\"\" utils . replace_placeholder ( component ) input_artifact_type_mapping = _get_input_artifact_type_mapping ( component ) arguments = [ \"--node_id\" , component . id , \"--tfx_ir\" , json_format . MessageToJson ( tfx_ir ), \"--metadata_ui_path\" , metadata_ui_path , \"--main_module\" , main_module , \"--step_module\" , step_module , \"--step_function_name\" , step_function_name , \"--input_artifact_types\" , json . dumps ( input_artifact_type_mapping ), \"--run_name\" , \"{{workflow.annotations.pipelines.kubeflow.org/run_name}}\" , ] for param in runtime_parameters : arguments . append ( \"--runtime_parameter\" ) arguments . append ( _encode_runtime_parameter ( param )) stack = Repository () . active_stack artifact_store = stack . artifact_store metadata_store = stack . metadata_store volumes : Dict [ str , k8s_client . V1Volume ] = {} has_local_repos = False if isinstance ( artifact_store , LocalArtifactStore ): has_local_repos = True host_path = k8s_client . V1HostPathVolumeSource ( path = artifact_store . path , type = \"Directory\" ) volumes [ artifact_store . path ] = k8s_client . V1Volume ( name = \"local-artifact-store\" , host_path = host_path ) logger . debug ( \"Adding host path volume for local artifact store (path: %s ) \" \"in kubeflow pipelines container.\" , artifact_store . path , ) if isinstance ( metadata_store , SQLiteMetadataStore ): has_local_repos = True metadata_store_dir = os . path . dirname ( metadata_store . uri ) host_path = k8s_client . V1HostPathVolumeSource ( path = metadata_store_dir , type = \"Directory\" ) volumes [ metadata_store_dir ] = k8s_client . V1Volume ( name = \"local-metadata-store\" , host_path = host_path ) logger . debug ( \"Adding host path volume for local metadata store (uri: %s ) \" \"in kubeflow pipelines container.\" , metadata_store . uri , ) self . container_op = dsl . ContainerOp ( name = component . id , command = CONTAINER_ENTRYPOINT_COMMAND , image = image , arguments = arguments , output_artifact_paths = { \"mlpipeline-ui-metadata\" : metadata_ui_path , }, pvolumes = volumes , ) if has_local_repos : if sys . platform == \"win32\" : # File permissions are not checked on Windows. This if clause # prevents mypy from complaining about unused 'type: ignore' # statements pass else : # Run KFP containers in the context of the local UID/GID # to ensure that the artifact and metadata stores can be shared # with the local pipeline runs. self . container_op . container . security_context = ( k8s_client . V1SecurityContext ( run_as_user = os . getuid (), run_as_group = os . getgid (), ) ) logger . debug ( \"Setting security context UID and GID to local user/group \" \"in kubeflow pipelines container.\" ) for op in depends_on : self . container_op . after ( op ) self . container_op . container . add_env_variable ( k8s_client . V1EnvVar ( name = ENV_ZENML_PREVENT_PIPELINE_EXECUTION , value = \"True\" ) ) for k , v in pod_labels_to_attach . items (): self . container_op . add_pod_label ( k , v ) __init__ ( self , component , depends_on , image , tfx_ir , pod_labels_to_attach , main_module , step_module , step_function_name , runtime_parameters , metadata_ui_path = '/tmp/mlpipeline-ui-metadata.json' ) special Creates a new Kubeflow-based component. This class essentially wraps a dsl.ContainerOp construct in Kubeflow Pipelines. Parameters: Name Type Description Default component BaseComponent The logical TFX component to wrap. required depends_on Set[kfp.dsl._container_op.ContainerOp] The set of upstream KFP ContainerOp components that this component will depend on. required image str The container image to use for this component. required tfx_ir Pipeline The TFX intermedia representation of the pipeline. required pod_labels_to_attach Dict[str, str] Dict of pod labels to attach to the GKE pod. required runtime_parameters List[tfx.orchestration.data_types.RuntimeParameter] Runtime parameters of the pipeline. required metadata_ui_path str File location for metadata-ui-metadata.json file. '/tmp/mlpipeline-ui-metadata.json' Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_component.py def __init__ ( self , component : tfx_base_component . BaseComponent , depends_on : Set [ dsl . ContainerOp ], image : str , tfx_ir : pipeline_pb2 . Pipeline , # type: ignore[valid-type] pod_labels_to_attach : Dict [ str , str ], main_module : str , step_module : str , step_function_name : str , runtime_parameters : List [ data_types . RuntimeParameter ], metadata_ui_path : str = \"/tmp/mlpipeline-ui-metadata.json\" , ): \"\"\"Creates a new Kubeflow-based component. This class essentially wraps a dsl.ContainerOp construct in Kubeflow Pipelines. Args: component: The logical TFX component to wrap. depends_on: The set of upstream KFP ContainerOp components that this component will depend on. image: The container image to use for this component. tfx_ir: The TFX intermedia representation of the pipeline. pod_labels_to_attach: Dict of pod labels to attach to the GKE pod. runtime_parameters: Runtime parameters of the pipeline. metadata_ui_path: File location for metadata-ui-metadata.json file. \"\"\" utils . replace_placeholder ( component ) input_artifact_type_mapping = _get_input_artifact_type_mapping ( component ) arguments = [ \"--node_id\" , component . id , \"--tfx_ir\" , json_format . MessageToJson ( tfx_ir ), \"--metadata_ui_path\" , metadata_ui_path , \"--main_module\" , main_module , \"--step_module\" , step_module , \"--step_function_name\" , step_function_name , \"--input_artifact_types\" , json . dumps ( input_artifact_type_mapping ), \"--run_name\" , \"{{workflow.annotations.pipelines.kubeflow.org/run_name}}\" , ] for param in runtime_parameters : arguments . append ( \"--runtime_parameter\" ) arguments . append ( _encode_runtime_parameter ( param )) stack = Repository () . active_stack artifact_store = stack . artifact_store metadata_store = stack . metadata_store volumes : Dict [ str , k8s_client . V1Volume ] = {} has_local_repos = False if isinstance ( artifact_store , LocalArtifactStore ): has_local_repos = True host_path = k8s_client . V1HostPathVolumeSource ( path = artifact_store . path , type = \"Directory\" ) volumes [ artifact_store . path ] = k8s_client . V1Volume ( name = \"local-artifact-store\" , host_path = host_path ) logger . debug ( \"Adding host path volume for local artifact store (path: %s ) \" \"in kubeflow pipelines container.\" , artifact_store . path , ) if isinstance ( metadata_store , SQLiteMetadataStore ): has_local_repos = True metadata_store_dir = os . path . dirname ( metadata_store . uri ) host_path = k8s_client . V1HostPathVolumeSource ( path = metadata_store_dir , type = \"Directory\" ) volumes [ metadata_store_dir ] = k8s_client . V1Volume ( name = \"local-metadata-store\" , host_path = host_path ) logger . debug ( \"Adding host path volume for local metadata store (uri: %s ) \" \"in kubeflow pipelines container.\" , metadata_store . uri , ) self . container_op = dsl . ContainerOp ( name = component . id , command = CONTAINER_ENTRYPOINT_COMMAND , image = image , arguments = arguments , output_artifact_paths = { \"mlpipeline-ui-metadata\" : metadata_ui_path , }, pvolumes = volumes , ) if has_local_repos : if sys . platform == \"win32\" : # File permissions are not checked on Windows. This if clause # prevents mypy from complaining about unused 'type: ignore' # statements pass else : # Run KFP containers in the context of the local UID/GID # to ensure that the artifact and metadata stores can be shared # with the local pipeline runs. self . container_op . container . security_context = ( k8s_client . V1SecurityContext ( run_as_user = os . getuid (), run_as_group = os . getgid (), ) ) logger . debug ( \"Setting security context UID and GID to local user/group \" \"in kubeflow pipelines container.\" ) for op in depends_on : self . container_op . after ( op ) self . container_op . container . add_env_variable ( k8s_client . V1EnvVar ( name = ENV_ZENML_PREVENT_PIPELINE_EXECUTION , value = \"True\" ) ) for k , v in pod_labels_to_attach . items (): self . container_op . add_pod_label ( k , v )","title":"KubeflowComponent"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.kubeflow_dag_runner","text":"The below code is copied from the TFX source repo with minor changes. All credits goes to the TFX team for the core implementation","title":"kubeflow_dag_runner"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.kubeflow_dag_runner.KubeflowDagRunner","text":"Kubeflow Pipelines runner. Constructs a pipeline definition YAML file based on the TFX logical pipeline. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py class KubeflowDagRunner ( tfx_runner . TfxRunner ): \"\"\"Kubeflow Pipelines runner. Constructs a pipeline definition YAML file based on the TFX logical pipeline. \"\"\" def __init__ ( self , config : KubeflowDagRunnerConfig , output_path : str , pod_labels_to_attach : Optional [ Dict [ str , str ]] = None , ): \"\"\"Initializes KubeflowDagRunner for compiling a Kubeflow Pipeline. Args: config: A KubeflowDagRunnerConfig object to specify runtime configuration when running the pipeline under Kubeflow. output_path: Path where the pipeline definition file will be stored. pod_labels_to_attach: Optional set of pod labels to attach to GKE pod spinned up for this pipeline. Default to the 3 labels: 1. add-pod-env: true, 2. pipeline SDK type, 3. pipeline unique ID, where 2 and 3 are instrumentation of usage tracking. \"\"\" super () . __init__ ( config ) self . _kubeflow_config = config self . _output_path = output_path self . _compiler = compiler . Compiler () self . _tfx_compiler = tfx_compiler . Compiler () self . _params : List [ dsl . PipelineParam ] = [] self . _params_by_component_id : Dict [ str , List [ data_types . RuntimeParameter ] ] = collections . defaultdict ( list ) self . _deduped_parameter_names : Set [ str ] = set () self . _pod_labels_to_attach = ( pod_labels_to_attach or get_default_pod_labels () ) def _parse_parameter_from_component ( self , component : tfx_base_component . BaseComponent ) -> None : \"\"\"Extract embedded RuntimeParameter placeholders from a component. Extract embedded RuntimeParameter placeholders from a component, then append the corresponding dsl.PipelineParam to KubeflowDagRunner. Args: component: a TFX component. \"\"\" deduped_parameter_names_for_component = set () for parameter in component . exec_properties . values (): if not isinstance ( parameter , data_types . RuntimeParameter ): continue # Ignore pipeline root because it will be added later. if parameter . name == tfx_pipeline . ROOT_PARAMETER . name : continue if parameter . name in deduped_parameter_names_for_component : continue deduped_parameter_names_for_component . add ( parameter . name ) self . _params_by_component_id [ component . id ] . append ( parameter ) if parameter . name not in self . _deduped_parameter_names : self . _deduped_parameter_names . add ( parameter . name ) dsl_parameter = dsl . PipelineParam ( name = parameter . name , value = str ( parameter . default ) ) self . _params . append ( dsl_parameter ) def _parse_parameter_from_pipeline ( self , pipeline : tfx_pipeline . Pipeline ) -> None : \"\"\"Extract all the RuntimeParameter placeholders from the pipeline.\"\"\" for component in pipeline . components : self . _parse_parameter_from_component ( component ) def _construct_pipeline_graph ( self , pipeline : tfx_pipeline . Pipeline ) -> None : \"\"\"Constructs a Kubeflow Pipeline graph. Args: pipeline: The logical TFX pipeline to base the construction on. pipeline_root: dsl.PipelineParam representing the pipeline root. \"\"\" component_to_kfp_op : Dict [ base_node . BaseNode , dsl . ContainerOp ] = {} tfx_ir = self . _generate_tfx_ir ( pipeline ) # Assumption: There is a partial ordering of components in the list, # i.e. if component A depends on component B and C, then A appears # after B and C in the list. for component in pipeline . components : # Keep track of the set of upstream dsl.ContainerOps for this # component. depends_on = set () for upstream_component in component . upstream_nodes : depends_on . add ( component_to_kfp_op [ upstream_component ]) # remove the extra pipeline node information tfx_node_ir = self . _dehydrate_tfx_ir ( tfx_ir , component . id ) from zenml.utils import source_utils main_module_file = sys . modules [ \"__main__\" ] . __file__ main_module = source_utils . get_module_source_from_file_path ( os . path . abspath ( main_module_file ) ) step_module = component . component_type . split ( \".\" )[: - 1 ] if step_module [ 0 ] == \"__main__\" : step_module = main_module else : step_module = \".\" . join ( step_module ) kfp_component = KubeflowComponent ( main_module = main_module , step_module = step_module , step_function_name = component . id , component = component , depends_on = depends_on , image = self . _kubeflow_config . image , pod_labels_to_attach = self . _pod_labels_to_attach , tfx_ir = tfx_node_ir , metadata_ui_path = self . _kubeflow_config . metadata_ui_path , runtime_parameters = self . _params_by_component_id [ component . id ], ) for operator in self . _kubeflow_config . pipeline_operator_funcs : kfp_component . container_op . apply ( operator ) component_to_kfp_op [ component ] = kfp_component . container_op def _del_unused_field ( self , node_id : str , message_dict : MutableMapping [ str , Any ] ) -> None : \"\"\"Remove fields that are not used by the pipeline.\"\"\" for item in list ( message_dict . keys ()): if item != node_id : del message_dict [ item ] def _dehydrate_tfx_ir ( self , original_pipeline : pipeline_pb2 . Pipeline , node_id : str # type: ignore[valid-type] # noqa ) -> pipeline_pb2 . Pipeline : # type: ignore[valid-type] \"\"\"Dehydrate the TFX IR to remove unused fields.\"\"\" pipeline = copy . deepcopy ( original_pipeline ) for node in pipeline . nodes : # type: ignore[attr-defined] if ( node . WhichOneof ( \"node\" ) == \"pipeline_node\" and node . pipeline_node . node_info . id == node_id ): del pipeline . nodes [:] # type: ignore[attr-defined] pipeline . nodes . extend ([ node ]) # type: ignore[attr-defined] break deployment_config = pipeline_pb2 . IntermediateDeploymentConfig () pipeline . deployment_config . Unpack ( deployment_config ) # type: ignore[attr-defined] # noqa self . _del_unused_field ( node_id , deployment_config . executor_specs ) self . _del_unused_field ( node_id , deployment_config . custom_driver_specs ) self . _del_unused_field ( node_id , deployment_config . node_level_platform_configs ) pipeline . deployment_config . Pack ( deployment_config ) # type: ignore[attr-defined] # noqa return pipeline def _generate_tfx_ir ( self , pipeline : tfx_pipeline . Pipeline ) -> Optional [ pipeline_pb2 . Pipeline ]: # type: ignore[valid-type] \"\"\"Generate the TFX IR from the logical TFX pipeline.\"\"\" result = self . _tfx_compiler . compile ( pipeline ) return result def run ( self , pipeline : tfx_pipeline . Pipeline ) -> None : \"\"\"Compiles and outputs a Kubeflow Pipeline YAML definition file. Args: pipeline: The logical TFX pipeline to use when building the Kubeflow pipeline. \"\"\" for component in pipeline . components : # TODO(b/187122662): Pass through pip dependencies as a first-class # component flag. if isinstance ( component , tfx_base_component . BaseComponent ): component . _resolve_pip_dependencies ( # pylint: disable=protected-access pipeline . pipeline_info . pipeline_root ) def _construct_pipeline () -> None : \"\"\"Creates Kubeflow ContainerOps for each TFX component encountered in the pipeline definition.\"\"\" self . _construct_pipeline_graph ( pipeline ) # Need to run this first to get self._params populated. Then KFP # compiler can correctly match default value with PipelineParam. self . _parse_parameter_from_pipeline ( pipeline ) # Create workflow spec and write out to package. self . _compiler . _create_and_write_workflow ( # pylint: disable=protected-access pipeline_func = _construct_pipeline , pipeline_name = pipeline . pipeline_info . pipeline_name , params_list = self . _params , package_path = self . _output_path , ) logger . info ( \"Finished writing kubeflow pipeline definition file ' %s '.\" , self . _output_path , ) __init__ ( self , config , output_path , pod_labels_to_attach = None ) special Initializes KubeflowDagRunner for compiling a Kubeflow Pipeline. Parameters: Name Type Description Default config KubeflowDagRunnerConfig A KubeflowDagRunnerConfig object to specify runtime configuration when running the pipeline under Kubeflow. required output_path str Path where the pipeline definition file will be stored. required pod_labels_to_attach Optional[Dict[str, str]] Optional set of pod labels to attach to GKE pod spinned up for this pipeline. Default to the 3 labels: 1. add-pod-env: true, 2. pipeline SDK type, 3. pipeline unique ID, where 2 and 3 are instrumentation of usage tracking. None Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py def __init__ ( self , config : KubeflowDagRunnerConfig , output_path : str , pod_labels_to_attach : Optional [ Dict [ str , str ]] = None , ): \"\"\"Initializes KubeflowDagRunner for compiling a Kubeflow Pipeline. Args: config: A KubeflowDagRunnerConfig object to specify runtime configuration when running the pipeline under Kubeflow. output_path: Path where the pipeline definition file will be stored. pod_labels_to_attach: Optional set of pod labels to attach to GKE pod spinned up for this pipeline. Default to the 3 labels: 1. add-pod-env: true, 2. pipeline SDK type, 3. pipeline unique ID, where 2 and 3 are instrumentation of usage tracking. \"\"\" super () . __init__ ( config ) self . _kubeflow_config = config self . _output_path = output_path self . _compiler = compiler . Compiler () self . _tfx_compiler = tfx_compiler . Compiler () self . _params : List [ dsl . PipelineParam ] = [] self . _params_by_component_id : Dict [ str , List [ data_types . RuntimeParameter ] ] = collections . defaultdict ( list ) self . _deduped_parameter_names : Set [ str ] = set () self . _pod_labels_to_attach = ( pod_labels_to_attach or get_default_pod_labels () ) run ( self , pipeline ) Compiles and outputs a Kubeflow Pipeline YAML definition file. Parameters: Name Type Description Default pipeline Pipeline The logical TFX pipeline to use when building the Kubeflow pipeline. required Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py def run ( self , pipeline : tfx_pipeline . Pipeline ) -> None : \"\"\"Compiles and outputs a Kubeflow Pipeline YAML definition file. Args: pipeline: The logical TFX pipeline to use when building the Kubeflow pipeline. \"\"\" for component in pipeline . components : # TODO(b/187122662): Pass through pip dependencies as a first-class # component flag. if isinstance ( component , tfx_base_component . BaseComponent ): component . _resolve_pip_dependencies ( # pylint: disable=protected-access pipeline . pipeline_info . pipeline_root ) def _construct_pipeline () -> None : \"\"\"Creates Kubeflow ContainerOps for each TFX component encountered in the pipeline definition.\"\"\" self . _construct_pipeline_graph ( pipeline ) # Need to run this first to get self._params populated. Then KFP # compiler can correctly match default value with PipelineParam. self . _parse_parameter_from_pipeline ( pipeline ) # Create workflow spec and write out to package. self . _compiler . _create_and_write_workflow ( # pylint: disable=protected-access pipeline_func = _construct_pipeline , pipeline_name = pipeline . pipeline_info . pipeline_name , params_list = self . _params , package_path = self . _output_path , ) logger . info ( \"Finished writing kubeflow pipeline definition file ' %s '.\" , self . _output_path , )","title":"KubeflowDagRunner"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.kubeflow_dag_runner.KubeflowDagRunnerConfig","text":"Runtime configuration parameters specific to execution on Kubeflow. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py class KubeflowDagRunnerConfig ( pipeline_config . PipelineConfig ): \"\"\"Runtime configuration parameters specific to execution on Kubeflow.\"\"\" def __init__ ( self , image : str , pipeline_operator_funcs : Optional [ List [ OpFunc ]] = None , supported_launcher_classes : Optional [ List [ Type [ base_component_launcher . BaseComponentLauncher ]] ] = None , metadata_ui_path : str = \"/tmp/mlpipeline-ui-metadata.json\" , ** kwargs : Any ): \"\"\"Creates a KubeflowDagRunnerConfig object. The user can use pipeline_operator_funcs to apply modifications to ContainerOps used in the pipeline. For example, to ensure the pipeline steps mount a GCP secret, and a Persistent Volume, one can create config object like so: from kfp import gcp, onprem mount_secret_op = gcp.use_secret('my-secret-name) mount_volume_op = onprem.mount_pvc( \"my-persistent-volume-claim\", \"my-volume-name\", \"/mnt/volume-mount-path\") config = KubeflowDagRunnerConfig( pipeline_operator_funcs=[mount_secret_op, mount_volume_op] ) Args: image: The docker image to use in the pipeline. pipeline_operator_funcs: A list of ContainerOp modifying functions that will be applied to every container step in the pipeline. supported_launcher_classes: A list of component launcher classes that are supported by the current pipeline. List sequence determines the order in which launchers are chosen for each component being run. metadata_ui_path: File location for metadata-ui-metadata.json file. **kwargs: keyword args for PipelineConfig. \"\"\" supported_launcher_classes = supported_launcher_classes or [ in_process_component_launcher . InProcessComponentLauncher , kubernetes_component_launcher . KubernetesComponentLauncher , ] super () . __init__ ( supported_launcher_classes = supported_launcher_classes , ** kwargs ) self . pipeline_operator_funcs = ( pipeline_operator_funcs or get_default_pipeline_operator_funcs () ) self . image = image self . metadata_ui_path = metadata_ui_path __init__ ( self , image , pipeline_operator_funcs = None , supported_launcher_classes = None , metadata_ui_path = '/tmp/mlpipeline-ui-metadata.json' , ** kwargs ) special Creates a KubeflowDagRunnerConfig object. The user can use pipeline_operator_funcs to apply modifications to ContainerOps used in the pipeline. For example, to ensure the pipeline steps mount a GCP secret, and a Persistent Volume, one can create config object like so: from kfp import gcp, onprem mount_secret_op = gcp.use_secret('my-secret-name) mount_volume_op = onprem.mount_pvc( \"my-persistent-volume-claim\", \"my-volume-name\", \"/mnt/volume-mount-path\") config = KubeflowDagRunnerConfig( pipeline_operator_funcs=[mount_secret_op, mount_volume_op] ) Parameters: Name Type Description Default image str The docker image to use in the pipeline. required pipeline_operator_funcs Optional[List[Callable[[kfp.dsl._container_op.ContainerOp], Union[kfp.dsl._container_op.ContainerOp, NoneType]]]] A list of ContainerOp modifying functions that will be applied to every container step in the pipeline. None supported_launcher_classes Optional[List[Type[tfx.orchestration.launcher.base_component_launcher.BaseComponentLauncher]]] A list of component launcher classes that are supported by the current pipeline. List sequence determines the order in which launchers are chosen for each component being run. None metadata_ui_path str File location for metadata-ui-metadata.json file. '/tmp/mlpipeline-ui-metadata.json' **kwargs Any keyword args for PipelineConfig. {} Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py def __init__ ( self , image : str , pipeline_operator_funcs : Optional [ List [ OpFunc ]] = None , supported_launcher_classes : Optional [ List [ Type [ base_component_launcher . BaseComponentLauncher ]] ] = None , metadata_ui_path : str = \"/tmp/mlpipeline-ui-metadata.json\" , ** kwargs : Any ): \"\"\"Creates a KubeflowDagRunnerConfig object. The user can use pipeline_operator_funcs to apply modifications to ContainerOps used in the pipeline. For example, to ensure the pipeline steps mount a GCP secret, and a Persistent Volume, one can create config object like so: from kfp import gcp, onprem mount_secret_op = gcp.use_secret('my-secret-name) mount_volume_op = onprem.mount_pvc( \"my-persistent-volume-claim\", \"my-volume-name\", \"/mnt/volume-mount-path\") config = KubeflowDagRunnerConfig( pipeline_operator_funcs=[mount_secret_op, mount_volume_op] ) Args: image: The docker image to use in the pipeline. pipeline_operator_funcs: A list of ContainerOp modifying functions that will be applied to every container step in the pipeline. supported_launcher_classes: A list of component launcher classes that are supported by the current pipeline. List sequence determines the order in which launchers are chosen for each component being run. metadata_ui_path: File location for metadata-ui-metadata.json file. **kwargs: keyword args for PipelineConfig. \"\"\" supported_launcher_classes = supported_launcher_classes or [ in_process_component_launcher . InProcessComponentLauncher , kubernetes_component_launcher . KubernetesComponentLauncher , ] super () . __init__ ( supported_launcher_classes = supported_launcher_classes , ** kwargs ) self . pipeline_operator_funcs = ( pipeline_operator_funcs or get_default_pipeline_operator_funcs () ) self . image = image self . metadata_ui_path = metadata_ui_path","title":"KubeflowDagRunnerConfig"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.kubeflow_dag_runner.get_default_pipeline_operator_funcs","text":"Returns a default list of pipeline operator functions. Parameters: Name Type Description Default use_gcp_sa bool If true, mount a GCP service account secret to each pod, with the name _KUBEFLOW_GCP_SECRET_NAME. False Returns: Type Description List[Callable[[kfp.dsl._container_op.ContainerOp], Optional[kfp.dsl._container_op.ContainerOp]]] A list of functions with type OpFunc. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py def get_default_pipeline_operator_funcs ( use_gcp_sa : bool = False , ) -> List [ OpFunc ]: \"\"\"Returns a default list of pipeline operator functions. Args: use_gcp_sa: If true, mount a GCP service account secret to each pod, with the name _KUBEFLOW_GCP_SECRET_NAME. Returns: A list of functions with type OpFunc. \"\"\" # Enables authentication for GCP services if needed. gcp_secret_op = gcp . use_gcp_secret ( _KUBEFLOW_GCP_SECRET_NAME ) # Mounts configmap containing Metadata gRPC server configuration. mount_config_map_op = _mount_config_map_op ( \"metadata-grpc-configmap\" ) if use_gcp_sa : return [ gcp_secret_op , mount_config_map_op ] else : return [ mount_config_map_op ]","title":"get_default_pipeline_operator_funcs()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.kubeflow_dag_runner.get_default_pod_labels","text":"Returns the default pod label dict for Kubeflow. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py def get_default_pod_labels () -> Dict [ str , str ]: \"\"\"Returns the default pod label dict for Kubeflow.\"\"\" # KFP default transformers add pod env: # https://github.com/kubeflow/pipelines/blob/0.1.32/sdk/python/kfp/compiler/_default_transformers.py result = { \"add-pod-env\" : \"true\" , telemetry_utils . LABEL_KFP_SDK_ENV : \"tfx\" } return result","title":"get_default_pod_labels()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.kubeflow_orchestrator","text":"","title":"kubeflow_orchestrator"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.kubeflow_orchestrator.KubeflowOrchestrator","text":"Orchestrator responsible for running pipelines using Kubeflow. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py class KubeflowOrchestrator ( BaseOrchestrator ): \"\"\"Orchestrator responsible for running pipelines using Kubeflow.\"\"\" custom_docker_base_image_name : Optional [ str ] = None kubeflow_pipelines_ui_port : int = DEFAULT_KFP_UI_PORT kubernetes_context : Optional [ str ] = None supports_local_execution = True supports_remote_execution = True @property def flavor ( self ) -> OrchestratorFlavor : \"\"\"The orchestrator flavor.\"\"\" return OrchestratorFlavor . KUBEFLOW @property def validator ( self ) -> Optional [ StackValidator ]: \"\"\"Validates that the stack contains a container registry.\"\"\" return StackValidator ( required_components = { StackComponentType . CONTAINER_REGISTRY } ) def get_docker_image_name ( self , pipeline_name : str ) -> str : \"\"\"Returns the full docker image name including registry and tag.\"\"\" base_image_name = f \"zenml-kubeflow: { pipeline_name } \" container_registry = Repository () . active_stack . container_registry if container_registry : registry_uri = container_registry . uri . rstrip ( \"/\" ) return f \" { registry_uri } / { base_image_name } \" else : return base_image_name @property def root_directory ( self ) -> str : \"\"\"Returns path to the root directory for all files concerning this orchestrator.\"\"\" return os . path . join ( zenml . io . utils . get_global_config_directory (), \"kubeflow\" , str ( self . uuid ), ) @property def pipeline_directory ( self ) -> str : \"\"\"Returns path to a directory in which the kubeflow pipeline files are stored.\"\"\" return os . path . join ( self . root_directory , \"pipelines\" ) def prepare_pipeline_deployment ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , runtime_configuration : \"RuntimeConfiguration\" , ) -> None : \"\"\"Builds a docker image for the current environment and uploads it to a container registry if configured. \"\"\" from zenml.integrations.kubeflow.docker_utils import ( build_docker_image , push_docker_image , ) image_name = self . get_docker_image_name ( pipeline . name ) requirements = { \"kubernetes\" , * stack . requirements ( exclude_components = { StackComponentType . ORCHESTRATOR } ), * self . _get_pipeline_requirements ( pipeline ), } logger . debug ( \"Kubeflow docker container requirements: %s \" , requirements ) build_docker_image ( build_context_path = str ( Repository () . root ), image_name = image_name , dockerignore_path = pipeline . dockerignore_file , requirements = requirements , base_image = self . custom_docker_base_image_name , ) if stack . container_registry : push_docker_image ( image_name ) def run_pipeline ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , run_name : str ) -> Any : \"\"\"Runs a pipeline on Kubeflow Pipelines.\"\"\" from zenml.integrations.kubeflow.docker_utils import get_image_digest image_name = self . get_docker_image_name ( pipeline . name ) image_name = get_image_digest ( image_name ) or image_name fileio . make_dirs ( self . pipeline_directory ) pipeline_file_path = os . path . join ( self . pipeline_directory , f \" { pipeline . name } .yaml\" ) runner_config = KubeflowDagRunnerConfig ( image = image_name ) runner = KubeflowDagRunner ( config = runner_config , output_path = pipeline_file_path ) tfx_pipeline = create_tfx_pipeline ( pipeline , stack = stack ) runner . run ( tfx_pipeline ) self . _upload_and_run_pipeline ( pipeline_file_path = pipeline_file_path , run_name = run_name , enable_cache = pipeline . enable_cache , ) def _upload_and_run_pipeline ( self , pipeline_file_path : str , run_name : str , enable_cache : bool ) -> None : \"\"\"Tries to upload and run a KFP pipeline. Args: pipeline_file_path: Path to the pipeline definition file. run_name: A name for the pipeline run that will be started. enable_cache: Whether caching is enabled for this pipeline run. \"\"\" try : if self . kubernetes_context : logger . info ( \"Running in kubernetes context ' %s '.\" , self . kubernetes_context , ) # load kubernetes config to authorize the KFP client config . load_kube_config ( context = self . kubernetes_context ) # upload the pipeline to Kubeflow and start it client = kfp . Client () result = client . create_run_from_pipeline_package ( pipeline_file_path , arguments = {}, run_name = run_name , enable_caching = enable_cache , ) logger . info ( \"Started pipeline run with ID ' %s '.\" , result . run_id ) except urllib3 . exceptions . HTTPError as error : logger . warning ( \"Failed to upload Kubeflow pipeline: %s . \" \"Please make sure your kube config is configured and the \" \"current context is set correctly.\" , error , ) def _get_pipeline_requirements ( self , pipeline : \"BasePipeline\" ) -> Set [ str ]: \"\"\"Gets list of requirements for a pipeline.\"\"\" if pipeline . requirements_file and fileio . file_exists ( pipeline . requirements_file ): logger . debug ( \"Using requirements from file %s .\" , pipeline . requirements_file ) with fileio . open ( pipeline . requirements_file , \"r\" ) as f : return { requirement . strip () for requirement in f . read () . split ( \" \\n \" ) } else : return set () @property def _pid_file_path ( self ) -> str : \"\"\"Returns path to the daemon PID file.\"\"\" return os . path . join ( self . root_directory , \"kubeflow_daemon.pid\" ) @property def log_file ( self ) -> str : \"\"\"Path of the daemon log file.\"\"\" return os . path . join ( self . root_directory , \"kubeflow_daemon.log\" ) @property def _k3d_cluster_name ( self ) -> str : \"\"\"Returns the K3D cluster name.\"\"\" # K3D only allows cluster names with up to 32 characters, use the # first 8 chars of the orchestrator UUID as identifier return f \"zenml-kubeflow- { str ( self . uuid )[: 8 ] } \" def _get_k3d_registry_name ( self , port : int ) -> str : \"\"\"Returns the K3D registry name.\"\"\" return f \"k3d-zenml-kubeflow-registry.localhost: { port } \" @property def _k3d_registry_config_path ( self ) -> str : \"\"\"Returns the path to the K3D registry config yaml.\"\"\" return os . path . join ( self . root_directory , \"k3d_registry.yaml\" ) def list_manual_setup_steps ( self , container_registry_name : str , container_registry_path : str ) -> None : \"\"\"Logs manual steps needed to setup the Kubeflow local orchestrator.\"\"\" global_config_dir_path = zenml . io . utils . get_global_config_directory () kubeflow_commands = [ f \"> k3d cluster create CLUSTER_NAME --registry-create { container_registry_name } --registry-config { container_registry_path } --volume { global_config_dir_path } : { global_config_dir_path } \\n \" , f \"> kubectl --context CLUSTER_NAME apply -k github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref= { KFP_VERSION } &timeout=1m\" , \"> kubectl --context CLUSTER_NAME wait --timeout=60s --for condition=established crd/applications.app.k8s.io\" , f \"> kubectl --context CLUSTER_NAME apply -k github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref= { KFP_VERSION } &timeout=1m\" , f \"> kubectl --namespace kubeflow port-forward svc/ml-pipeline-ui { self . kubeflow_pipelines_ui_port } :80\" , ] logger . error ( \"Unable to spin up local Kubeflow Pipelines deployment.\" ) logger . info ( \"If you wish to spin up this Kubeflow local orchestrator manually, \" \"please enter the following commands (substituting where appropriate): \\n \" ) logger . info ( \" \\n \" . join ( kubeflow_commands )) @property def is_provisioned ( self ) -> bool : \"\"\"Returns if a local k3d cluster for this orchestrator exists.\"\"\" if not local_deployment_utils . check_prerequisites (): # if any prerequisites are missing there is certainly no # local deployment running return False return local_deployment_utils . k3d_cluster_exists ( cluster_name = self . _k3d_cluster_name ) @property def is_running ( self ) -> bool : \"\"\"Returns if the local k3d cluster for this orchestrator is running.\"\"\" if not self . is_provisioned : return False return local_deployment_utils . k3d_cluster_running ( cluster_name = self . _k3d_cluster_name ) def provision ( self ) -> None : \"\"\"Provisions a local Kubeflow Pipelines deployment.\"\"\" if self . is_running : logger . info ( \"Found already existing local Kubeflow Pipelines deployment. \" \"If there are any issues with the existing deployment, please \" \"run 'zenml orchestrator down' to delete it.\" ) return if not local_deployment_utils . check_prerequisites (): logger . error ( \"Unable to provision local Kubeflow Pipelines deployment: \" \"Please install 'k3d' and 'kubectl' and try again.\" ) return container_registry = Repository () . active_stack . container_registry if not container_registry : logger . error ( \"Unable to provision local Kubeflow Pipelines deployment: \" \"Missing container registry in current stack.\" ) return logger . info ( \"Provisioning local Kubeflow Pipelines deployment...\" ) fileio . make_dirs ( self . root_directory ) container_registry_port = int ( container_registry . uri . split ( \":\" )[ - 1 ]) container_registry_name = self . _get_k3d_registry_name ( port = container_registry_port ) local_deployment_utils . write_local_registry_yaml ( yaml_path = self . _k3d_registry_config_path , registry_name = container_registry_name , registry_uri = container_registry . uri , ) try : local_deployment_utils . create_k3d_cluster ( cluster_name = self . _k3d_cluster_name , registry_name = container_registry_name , registry_config_path = self . _k3d_registry_config_path , ) kubernetes_context = f \"k3d- { self . _k3d_cluster_name } \" local_deployment_utils . deploy_kubeflow_pipelines ( kubernetes_context = kubernetes_context ) port = self . kubeflow_pipelines_ui_port if ( port == DEFAULT_KFP_UI_PORT and not networking_utils . port_available ( port ) ): # if the user didn't specify a specific port and the default # port is occupied, fallback to a random open port port = networking_utils . find_available_port () local_deployment_utils . start_kfp_ui_daemon ( pid_file_path = self . _pid_file_path , log_file_path = self . log_file , port = port , ) except Exception as e : logger . error ( e ) self . list_manual_setup_steps ( container_registry_name , self . _k3d_registry_config_path ) self . deprovision () def deprovision ( self ) -> None : \"\"\"Deprovisions a local Kubeflow Pipelines deployment.\"\"\" if self . is_running : local_deployment_utils . delete_k3d_cluster ( cluster_name = self . _k3d_cluster_name ) if fileio . file_exists ( self . _pid_file_path ): if sys . platform == \"win32\" : # Daemon functionality is not supported on Windows, so the PID # file won't exist. This if clause exists just for mypy to not # complain about missing functions pass else : from zenml.utils import daemon daemon . stop_daemon ( self . _pid_file_path , kill_children = True ) fileio . remove ( self . _pid_file_path ) if fileio . file_exists ( self . log_file ): fileio . remove ( self . log_file ) logger . info ( \"Local kubeflow pipelines deployment deprovisioned.\" ) def resume ( self ) -> None : \"\"\"Resumes the local k3d cluster.\"\"\" if self . is_running : logger . info ( \"Local kubeflow pipelines deployment already running.\" ) return if not self . is_provisioned : raise ProvisioningError ( \"Unable to resume local kubeflow pipelines deployment: No \" \"resources provisioned for local deployment.\" ) local_deployment_utils . start_k3d_cluster ( cluster_name = self . _k3d_cluster_name ) def suspend ( self ) -> None : \"\"\"Suspends the local k3d cluster.\"\"\" if not self . is_running : logger . info ( \"Local kubeflow pipelines deployment not running.\" ) return local_deployment_utils . stop_k3d_cluster ( cluster_name = self . _k3d_cluster_name ) flavor : OrchestratorFlavor property readonly The orchestrator flavor. is_provisioned : bool property readonly Returns if a local k3d cluster for this orchestrator exists. is_running : bool property readonly Returns if the local k3d cluster for this orchestrator is running. log_file : str property readonly Path of the daemon log file. pipeline_directory : str property readonly Returns path to a directory in which the kubeflow pipeline files are stored. root_directory : str property readonly Returns path to the root directory for all files concerning this orchestrator. validator : Optional [ zenml . stack . stack_validator . StackValidator ] property readonly Validates that the stack contains a container registry. deprovision ( self ) Deprovisions a local Kubeflow Pipelines deployment. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def deprovision ( self ) -> None : \"\"\"Deprovisions a local Kubeflow Pipelines deployment.\"\"\" if self . is_running : local_deployment_utils . delete_k3d_cluster ( cluster_name = self . _k3d_cluster_name ) if fileio . file_exists ( self . _pid_file_path ): if sys . platform == \"win32\" : # Daemon functionality is not supported on Windows, so the PID # file won't exist. This if clause exists just for mypy to not # complain about missing functions pass else : from zenml.utils import daemon daemon . stop_daemon ( self . _pid_file_path , kill_children = True ) fileio . remove ( self . _pid_file_path ) if fileio . file_exists ( self . log_file ): fileio . remove ( self . log_file ) logger . info ( \"Local kubeflow pipelines deployment deprovisioned.\" ) get_docker_image_name ( self , pipeline_name ) Returns the full docker image name including registry and tag. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def get_docker_image_name ( self , pipeline_name : str ) -> str : \"\"\"Returns the full docker image name including registry and tag.\"\"\" base_image_name = f \"zenml-kubeflow: { pipeline_name } \" container_registry = Repository () . active_stack . container_registry if container_registry : registry_uri = container_registry . uri . rstrip ( \"/\" ) return f \" { registry_uri } / { base_image_name } \" else : return base_image_name list_manual_setup_steps ( self , container_registry_name , container_registry_path ) Logs manual steps needed to setup the Kubeflow local orchestrator. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def list_manual_setup_steps ( self , container_registry_name : str , container_registry_path : str ) -> None : \"\"\"Logs manual steps needed to setup the Kubeflow local orchestrator.\"\"\" global_config_dir_path = zenml . io . utils . get_global_config_directory () kubeflow_commands = [ f \"> k3d cluster create CLUSTER_NAME --registry-create { container_registry_name } --registry-config { container_registry_path } --volume { global_config_dir_path } : { global_config_dir_path } \\n \" , f \"> kubectl --context CLUSTER_NAME apply -k github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref= { KFP_VERSION } &timeout=1m\" , \"> kubectl --context CLUSTER_NAME wait --timeout=60s --for condition=established crd/applications.app.k8s.io\" , f \"> kubectl --context CLUSTER_NAME apply -k github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref= { KFP_VERSION } &timeout=1m\" , f \"> kubectl --namespace kubeflow port-forward svc/ml-pipeline-ui { self . kubeflow_pipelines_ui_port } :80\" , ] logger . error ( \"Unable to spin up local Kubeflow Pipelines deployment.\" ) logger . info ( \"If you wish to spin up this Kubeflow local orchestrator manually, \" \"please enter the following commands (substituting where appropriate): \\n \" ) logger . info ( \" \\n \" . join ( kubeflow_commands )) prepare_pipeline_deployment ( self , pipeline , stack , runtime_configuration ) Builds a docker image for the current environment and uploads it to a container registry if configured. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def prepare_pipeline_deployment ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , runtime_configuration : \"RuntimeConfiguration\" , ) -> None : \"\"\"Builds a docker image for the current environment and uploads it to a container registry if configured. \"\"\" from zenml.integrations.kubeflow.docker_utils import ( build_docker_image , push_docker_image , ) image_name = self . get_docker_image_name ( pipeline . name ) requirements = { \"kubernetes\" , * stack . requirements ( exclude_components = { StackComponentType . ORCHESTRATOR } ), * self . _get_pipeline_requirements ( pipeline ), } logger . debug ( \"Kubeflow docker container requirements: %s \" , requirements ) build_docker_image ( build_context_path = str ( Repository () . root ), image_name = image_name , dockerignore_path = pipeline . dockerignore_file , requirements = requirements , base_image = self . custom_docker_base_image_name , ) if stack . container_registry : push_docker_image ( image_name ) provision ( self ) Provisions a local Kubeflow Pipelines deployment. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def provision ( self ) -> None : \"\"\"Provisions a local Kubeflow Pipelines deployment.\"\"\" if self . is_running : logger . info ( \"Found already existing local Kubeflow Pipelines deployment. \" \"If there are any issues with the existing deployment, please \" \"run 'zenml orchestrator down' to delete it.\" ) return if not local_deployment_utils . check_prerequisites (): logger . error ( \"Unable to provision local Kubeflow Pipelines deployment: \" \"Please install 'k3d' and 'kubectl' and try again.\" ) return container_registry = Repository () . active_stack . container_registry if not container_registry : logger . error ( \"Unable to provision local Kubeflow Pipelines deployment: \" \"Missing container registry in current stack.\" ) return logger . info ( \"Provisioning local Kubeflow Pipelines deployment...\" ) fileio . make_dirs ( self . root_directory ) container_registry_port = int ( container_registry . uri . split ( \":\" )[ - 1 ]) container_registry_name = self . _get_k3d_registry_name ( port = container_registry_port ) local_deployment_utils . write_local_registry_yaml ( yaml_path = self . _k3d_registry_config_path , registry_name = container_registry_name , registry_uri = container_registry . uri , ) try : local_deployment_utils . create_k3d_cluster ( cluster_name = self . _k3d_cluster_name , registry_name = container_registry_name , registry_config_path = self . _k3d_registry_config_path , ) kubernetes_context = f \"k3d- { self . _k3d_cluster_name } \" local_deployment_utils . deploy_kubeflow_pipelines ( kubernetes_context = kubernetes_context ) port = self . kubeflow_pipelines_ui_port if ( port == DEFAULT_KFP_UI_PORT and not networking_utils . port_available ( port ) ): # if the user didn't specify a specific port and the default # port is occupied, fallback to a random open port port = networking_utils . find_available_port () local_deployment_utils . start_kfp_ui_daemon ( pid_file_path = self . _pid_file_path , log_file_path = self . log_file , port = port , ) except Exception as e : logger . error ( e ) self . list_manual_setup_steps ( container_registry_name , self . _k3d_registry_config_path ) self . deprovision () resume ( self ) Resumes the local k3d cluster. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def resume ( self ) -> None : \"\"\"Resumes the local k3d cluster.\"\"\" if self . is_running : logger . info ( \"Local kubeflow pipelines deployment already running.\" ) return if not self . is_provisioned : raise ProvisioningError ( \"Unable to resume local kubeflow pipelines deployment: No \" \"resources provisioned for local deployment.\" ) local_deployment_utils . start_k3d_cluster ( cluster_name = self . _k3d_cluster_name ) run_pipeline ( self , pipeline , stack , run_name ) Runs a pipeline on Kubeflow Pipelines. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def run_pipeline ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , run_name : str ) -> Any : \"\"\"Runs a pipeline on Kubeflow Pipelines.\"\"\" from zenml.integrations.kubeflow.docker_utils import get_image_digest image_name = self . get_docker_image_name ( pipeline . name ) image_name = get_image_digest ( image_name ) or image_name fileio . make_dirs ( self . pipeline_directory ) pipeline_file_path = os . path . join ( self . pipeline_directory , f \" { pipeline . name } .yaml\" ) runner_config = KubeflowDagRunnerConfig ( image = image_name ) runner = KubeflowDagRunner ( config = runner_config , output_path = pipeline_file_path ) tfx_pipeline = create_tfx_pipeline ( pipeline , stack = stack ) runner . run ( tfx_pipeline ) self . _upload_and_run_pipeline ( pipeline_file_path = pipeline_file_path , run_name = run_name , enable_cache = pipeline . enable_cache , ) suspend ( self ) Suspends the local k3d cluster. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def suspend ( self ) -> None : \"\"\"Suspends the local k3d cluster.\"\"\" if not self . is_running : logger . info ( \"Local kubeflow pipelines deployment not running.\" ) return local_deployment_utils . stop_k3d_cluster ( cluster_name = self . _k3d_cluster_name )","title":"KubeflowOrchestrator"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.kubeflow_utils","text":"Common utility for Kubeflow-based orchestrator.","title":"kubeflow_utils"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.kubeflow_utils.replace_placeholder","text":"Replaces the RuntimeParameter placeholders with kfp.dsl.PipelineParam. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_utils.py def replace_placeholder ( component : base_node . BaseNode ) -> None : \"\"\"Replaces the RuntimeParameter placeholders with kfp.dsl.PipelineParam.\"\"\" keys = list ( component . exec_properties . keys ()) for key in keys : exec_property = component . exec_properties [ key ] if not isinstance ( exec_property , data_types . RuntimeParameter ): continue component . exec_properties [ key ] = str ( dsl . PipelineParam ( name = exec_property . name ) )","title":"replace_placeholder()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.local_deployment_utils","text":"","title":"local_deployment_utils"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.local_deployment_utils.check_prerequisites","text":"Checks whether all prerequisites for a local kubeflow pipelines deployment are installed. Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def check_prerequisites () -> bool : \"\"\"Checks whether all prerequisites for a local kubeflow pipelines deployment are installed.\"\"\" k3d_installed = shutil . which ( \"k3d\" ) is not None kubectl_installed = shutil . which ( \"kubectl\" ) is not None logger . debug ( \"Local kubeflow deployment prerequisites: K3D - %s , Kubectl - %s \" , k3d_installed , kubectl_installed , ) return k3d_installed and kubectl_installed","title":"check_prerequisites()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.local_deployment_utils.create_k3d_cluster","text":"Creates a K3D cluster. Parameters: Name Type Description Default cluster_name str Name of the cluster to create. required registry_name str Name of the registry to create for this cluster. required registry_config_path str Path to the registry config file. required Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def create_k3d_cluster ( cluster_name : str , registry_name : str , registry_config_path : str ) -> None : \"\"\"Creates a K3D cluster. Args: cluster_name: Name of the cluster to create. registry_name: Name of the registry to create for this cluster. registry_config_path: Path to the registry config file. \"\"\" logger . info ( \"Creating local K3D cluster ' %s '.\" , cluster_name ) global_config_dir_path = zenml . io . utils . get_global_config_directory () subprocess . check_call ( [ \"k3d\" , \"cluster\" , \"create\" , cluster_name , \"--registry-create\" , registry_name , \"--registry-config\" , registry_config_path , \"--volume\" , f \" { global_config_dir_path } : { global_config_dir_path } \" , ] ) logger . info ( \"Finished K3D cluster creation.\" )","title":"create_k3d_cluster()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.local_deployment_utils.delete_k3d_cluster","text":"Deletes a K3D cluster with the given name. Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def delete_k3d_cluster ( cluster_name : str ) -> None : \"\"\"Deletes a K3D cluster with the given name.\"\"\" subprocess . check_call ([ \"k3d\" , \"cluster\" , \"delete\" , cluster_name ]) logger . info ( \"Deleted local k3d cluster ' %s '.\" , cluster_name )","title":"delete_k3d_cluster()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.local_deployment_utils.deploy_kubeflow_pipelines","text":"Deploys Kubeflow Pipelines. Parameters: Name Type Description Default kubernetes_context str The kubernetes context on which Kubeflow Pipelines should be deployed. required Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def deploy_kubeflow_pipelines ( kubernetes_context : str ) -> None : \"\"\"Deploys Kubeflow Pipelines. Args: kubernetes_context: The kubernetes context on which Kubeflow Pipelines should be deployed. \"\"\" logger . info ( \"Deploying Kubeflow Pipelines.\" ) subprocess . check_call ( [ \"kubectl\" , \"--context\" , kubernetes_context , \"apply\" , \"-k\" , f \"github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref= { KFP_VERSION } &timeout=1m\" , ] ) subprocess . check_call ( [ \"kubectl\" , \"--context\" , kubernetes_context , \"wait\" , \"--timeout=60s\" , \"--for\" , \"condition=established\" , \"crd/applications.app.k8s.io\" , ] ) subprocess . check_call ( [ \"kubectl\" , \"--context\" , kubernetes_context , \"apply\" , \"-k\" , f \"github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref= { KFP_VERSION } &timeout=1m\" , ] ) logger . info ( \"Waiting for all Kubeflow Pipelines pods to be ready (this might \" \"take a few minutes).\" ) while True : logger . info ( \"Current pod status:\" ) subprocess . check_call ( [ \"kubectl\" , \"--context\" , kubernetes_context , \"--namespace\" , \"kubeflow\" , \"get\" , \"pods\" , ] ) if kubeflow_pipelines_ready ( kubernetes_context = kubernetes_context ): break logger . info ( \"One or more pods not ready yet, waiting for 30 seconds...\" ) time . sleep ( 30 ) logger . info ( \"Finished Kubeflow Pipelines setup.\" )","title":"deploy_kubeflow_pipelines()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.local_deployment_utils.k3d_cluster_exists","text":"Checks whether there exists a K3D cluster with the given name. Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def k3d_cluster_exists ( cluster_name : str ) -> bool : \"\"\"Checks whether there exists a K3D cluster with the given name.\"\"\" output = subprocess . check_output ( [ \"k3d\" , \"cluster\" , \"list\" , \"--output\" , \"json\" ] ) clusters = json . loads ( output ) for cluster in clusters : if cluster [ \"name\" ] == cluster_name : return True return False","title":"k3d_cluster_exists()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.local_deployment_utils.k3d_cluster_running","text":"Checks whether the K3D cluster with the given name is running. Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def k3d_cluster_running ( cluster_name : str ) -> bool : \"\"\"Checks whether the K3D cluster with the given name is running.\"\"\" output = subprocess . check_output ( [ \"k3d\" , \"cluster\" , \"list\" , \"--output\" , \"json\" ] ) clusters = json . loads ( output ) for cluster in clusters : if cluster [ \"name\" ] == cluster_name : server_count : int = cluster [ \"serversCount\" ] servers_running : int = cluster [ \"serversRunning\" ] return servers_running == server_count return False","title":"k3d_cluster_running()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.local_deployment_utils.kubeflow_pipelines_ready","text":"Returns whether all Kubeflow Pipelines pods are ready. Parameters: Name Type Description Default kubernetes_context str The kubernetes context in which the pods should be checked. required Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def kubeflow_pipelines_ready ( kubernetes_context : str ) -> bool : \"\"\"Returns whether all Kubeflow Pipelines pods are ready. Args: kubernetes_context: The kubernetes context in which the pods should be checked. \"\"\" try : subprocess . check_call ( [ \"kubectl\" , \"--context\" , kubernetes_context , \"--namespace\" , \"kubeflow\" , \"wait\" , \"--for\" , \"condition=ready\" , \"--timeout=0s\" , \"pods\" , \"--all\" , ], stdout = subprocess . DEVNULL , stderr = subprocess . DEVNULL , ) return True except subprocess . CalledProcessError : return False","title":"kubeflow_pipelines_ready()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.local_deployment_utils.start_k3d_cluster","text":"Starts a K3D cluster with the given name. Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def start_k3d_cluster ( cluster_name : str ) -> None : \"\"\"Starts a K3D cluster with the given name.\"\"\" subprocess . check_call ([ \"k3d\" , \"cluster\" , \"start\" , cluster_name ]) logger . info ( \"Started local k3d cluster ' %s '.\" , cluster_name )","title":"start_k3d_cluster()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.local_deployment_utils.start_kfp_ui_daemon","text":"Starts a daemon process that forwards ports so the Kubeflow Pipelines UI is accessible in the browser. Parameters: Name Type Description Default pid_file_path str Path where the file with the daemons process ID should be written. required log_file_path str Path to a file where the daemon logs should be written. required port int Port on which the UI should be accessible. required Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def start_kfp_ui_daemon ( pid_file_path : str , log_file_path : str , port : int ) -> None : \"\"\"Starts a daemon process that forwards ports so the Kubeflow Pipelines UI is accessible in the browser. Args: pid_file_path: Path where the file with the daemons process ID should be written. log_file_path: Path to a file where the daemon logs should be written. port: Port on which the UI should be accessible. \"\"\" command = [ \"kubectl\" , \"--namespace\" , \"kubeflow\" , \"port-forward\" , \"svc/ml-pipeline-ui\" , f \" { port } :80\" , ] if not networking_utils . port_available ( port ): modified_command = command . copy () modified_command [ - 1 ] = \"PORT:80\" logger . warning ( \"Unable to port-forward Kubeflow Pipelines UI to local port %d \" \"because the port is occupied. In order to access the Kubeflow \" \"Pipelines UI at http://localhost:PORT/, please run ' %s ' in a \" \"separate command line shell (replace PORT with a free port of \" \"your choice).\" , port , \" \" . join ( modified_command ), ) elif sys . platform == \"win32\" : logger . warning ( \"Daemon functionality not supported on Windows. \" \"In order to access the Kubeflow Pipelines UI at \" \"http://localhost: %d /, please run ' %s ' in a separate command \" \"line shell.\" , port , \" \" . join ( command ), ) else : from zenml.utils import daemon def _daemon_function () -> None : \"\"\"Port-forwards the Kubeflow Pipelines UI pod.\"\"\" subprocess . check_call ( command ) daemon . run_as_daemon ( _daemon_function , pid_file = pid_file_path , log_file = log_file_path ) logger . info ( \"Started Kubeflow Pipelines UI daemon (check the daemon logs at %s \" \"in case you're not able to view the UI). The Kubeflow Pipelines \" \"UI should now be accessible at http://localhost: %d /.\" , log_file_path , port , )","title":"start_kfp_ui_daemon()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.local_deployment_utils.stop_k3d_cluster","text":"Stops a K3D cluster with the given name. Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def stop_k3d_cluster ( cluster_name : str ) -> None : \"\"\"Stops a K3D cluster with the given name.\"\"\" subprocess . check_call ([ \"k3d\" , \"cluster\" , \"stop\" , cluster_name ]) logger . info ( \"Stopped local k3d cluster ' %s '.\" , cluster_name )","title":"stop_k3d_cluster()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.local_deployment_utils.write_local_registry_yaml","text":"Writes a K3D registry config file. Parameters: Name Type Description Default yaml_path str Path where the config file should be written to. required registry_name str Name of the registry. required registry_uri str URI of the registry. required Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def write_local_registry_yaml ( yaml_path : str , registry_name : str , registry_uri : str ) -> None : \"\"\"Writes a K3D registry config file. Args: yaml_path: Path where the config file should be written to. registry_name: Name of the registry. registry_uri: URI of the registry. \"\"\" yaml_content = { \"mirrors\" : { registry_uri : { \"endpoint\" : [ f \"http:// { registry_name } \" ]}} } yaml_utils . write_yaml ( yaml_path , yaml_content )","title":"write_local_registry_yaml()"},{"location":"api_docs/integrations/#zenml.integrations.mlflow","text":"The mlflow integrations currently enables you to use mlflow tracking as a convenient way to visualize your experiment runs within the mlflow ui","title":"mlflow"},{"location":"api_docs/integrations/#zenml.integrations.mlflow.MlflowIntegration","text":"Definition of Plotly integration for ZenML. Source code in zenml/integrations/mlflow/__init__.py class MlflowIntegration ( Integration ): \"\"\"Definition of Plotly integration for ZenML.\"\"\" NAME = MLFLOW REQUIREMENTS = [ \"mlflow>=1.2.0\" ]","title":"MlflowIntegration"},{"location":"api_docs/integrations/#zenml.integrations.mlflow.mlflow_utils","text":"","title":"mlflow_utils"},{"location":"api_docs/integrations/#zenml.integrations.mlflow.mlflow_utils.enable_mlflow","text":"Outer decorator function for the creation of a ZenML pipeline with mlflow tracking enabled. In order for a pipeline to run within the context of mlflow, the mlflow experiment should be associated with the pipeline directly. Each separate pipeline run needs to be associated directly with a mlflow experiment. For this, the init and run methods need to be extended accordingly. Parameters: Name Type Description Default _pipeline Type[zenml.pipelines.base_pipeline.BasePipeline] The decorated pipeline required experiment_name Optional[str] Experiment name to use for mlflow None Returns: Type Description Type[zenml.pipelines.base_pipeline.BasePipeline] the inner decorator which has a pipeline with the two methods extended Source code in zenml/integrations/mlflow/mlflow_utils.py def enable_mlflow ( _pipeline : Type [ BasePipeline ], experiment_name : Optional [ str ] = None ) -> Type [ BasePipeline ]: \"\"\"Outer decorator function for the creation of a ZenML pipeline with mlflow tracking enabled. In order for a pipeline to run within the context of mlflow, the mlflow experiment should be associated with the pipeline directly. Each separate pipeline run needs to be associated directly with a mlflow experiment. For this, the __init__ and run methods need to be extended accordingly. Args: _pipeline: The decorated pipeline experiment_name: Experiment name to use for mlflow Returns: the inner decorator which has a pipeline with the two methods extended \"\"\" def inner_decorator ( pipeline : Type [ BasePipeline ]) -> Type [ BasePipeline ]: \"\"\"Inner decorator function for the creation of a ZenML Pipeline with mlflow The __init__ and run method are both extended. Args: pipeline: BasePipeline which will be extended Returns: the class of a newly generated ZenML Pipeline with mlflow \"\"\" # TODO [ENG-369]: Do we need to create a new class here or can we simply # extend the methods of the original pipeline class? return type ( # noqa pipeline . __name__ , ( pipeline ,), { \"__init__\" : enable_mlflow_init ( pipeline . __init__ , experiment_name ), \"run\" : enable_mlflow_run ( pipeline . run ), }, ) return inner_decorator ( _pipeline )","title":"enable_mlflow()"},{"location":"api_docs/integrations/#zenml.integrations.mlflow.mlflow_utils.enable_mlflow_init","text":"Outer decorator function for extending the init method for pipelines that should be run using mlflow Parameters: Name Type Description Default original_init Callable[[zenml.pipelines.base_pipeline.BasePipeline, zenml.steps.base_step.BaseStep, Any], NoneType] The init method that should be extended required experiment Optional[str] The users chosen experiment name to use for mlflow None Returns: Type Description Callable[..., NoneType] the inner decorator which extends the init method Source code in zenml/integrations/mlflow/mlflow_utils.py def enable_mlflow_init ( original_init : Callable [[ BasePipeline , BaseStep , Any ], None ], experiment : Optional [ str ] = None , ) -> Callable [ ... , None ]: \"\"\"Outer decorator function for extending the __init__ method for pipelines that should be run using mlflow Args: original_init: The __init__ method that should be extended experiment: The users chosen experiment name to use for mlflow Returns: the inner decorator which extends the __init__ method \"\"\" def inner_decorator ( self : BasePipeline , * args : BaseStep , ** kwargs : Any ) -> None : \"\"\"Inner decorator overwriting the pipeline __init__ Makes sure mlflow is properly set up and all mlflow logging takes place within one mlflow experiment that is associated with the pipeline \"\"\" original_init ( self , * args , ** kwargs ) setup_mlflow ( backend_store_uri = local_mlflow_backend (), experiment_name = experiment if experiment else self . name , ) return inner_decorator","title":"enable_mlflow_init()"},{"location":"api_docs/integrations/#zenml.integrations.mlflow.mlflow_utils.enable_mlflow_run","text":"Outer decorator function for extending the run method for pipelines that should be run using mlflow Parameters: Name Type Description Default run Callable[..., Any] The run method that should be extended required Returns: Type Description Callable[..., Any] the inner decorator which extends the run method Source code in zenml/integrations/mlflow/mlflow_utils.py def enable_mlflow_run ( run : Callable [ ... , Any ]) -> Callable [ ... , Any ]: \"\"\"Outer decorator function for extending the run method for pipelines that should be run using mlflow Args: run: The run method that should be extended Returns: the inner decorator which extends the run method \"\"\" def inner_decorator ( self : BasePipeline , run_name : Optional [ str ] = None ) -> Any : \"\"\"Inner decorator used to extend the run method of a pipeline. This ensures each pipeline run is run within a different mlflow context. Args: self: self of the original pipeline class run_name: Optional name for the run. \"\"\" with mlflow . start_run ( run_name = run_name ): run ( self , run_name ) return inner_decorator","title":"enable_mlflow_run()"},{"location":"api_docs/integrations/#zenml.integrations.mlflow.mlflow_utils.local_mlflow_backend","text":"Returns the local mlflow backend inside the global zenml directory Source code in zenml/integrations/mlflow/mlflow_utils.py def local_mlflow_backend () -> str : \"\"\"Returns the local mlflow backend inside the global zenml directory\"\"\" local_mlflow_backend_uri = os . path . join ( get_global_config_directory (), \"local_stores\" , \"mlruns\" ) if not os . path . exists ( local_mlflow_backend_uri ): os . makedirs ( local_mlflow_backend_uri ) # TODO [medium]: safely access (possibly non-existent) artifact stores return \"file:\" + local_mlflow_backend_uri","title":"local_mlflow_backend()"},{"location":"api_docs/integrations/#zenml.integrations.mlflow.mlflow_utils.setup_mlflow","text":"Setup all mlflow related configurations. This includes specifying which mlflow tracking uri should b e used and which experiment the tracking will be associated with. Parameters: Name Type Description Default backend_store_uri Optional[str] The mlflow backend to log to None experiment_name str The experiment name under which all runs will be tracked 'default' Source code in zenml/integrations/mlflow/mlflow_utils.py def setup_mlflow ( backend_store_uri : Optional [ str ] = None , experiment_name : str = \"default\" ) -> None : \"\"\"Setup all mlflow related configurations. This includes specifying which mlflow tracking uri should b e used and which experiment the tracking will be associated with. Args: backend_store_uri: The mlflow backend to log to experiment_name: The experiment name under which all runs will be tracked \"\"\" # TODO [ENG-316]: Implement a way to get the mlflow token and set # it as env variable at MLFLOW_TRACKING_TOKEN if not backend_store_uri : backend_store_uri = local_mlflow_backend () set_tracking_uri ( backend_store_uri ) # Set which experiment is used within mlflow set_experiment ( experiment_name )","title":"setup_mlflow()"},{"location":"api_docs/integrations/#zenml.integrations.plotly","text":"","title":"plotly"},{"location":"api_docs/integrations/#zenml.integrations.plotly.PlotlyIntegration","text":"Definition of Plotly integration for ZenML. Source code in zenml/integrations/plotly/__init__.py class PlotlyIntegration ( Integration ): \"\"\"Definition of Plotly integration for ZenML.\"\"\" NAME = PLOTLY REQUIREMENTS = [ \"plotly>=5.4.0\" ]","title":"PlotlyIntegration"},{"location":"api_docs/integrations/#zenml.integrations.plotly.visualizers","text":"","title":"visualizers"},{"location":"api_docs/integrations/#zenml.integrations.plotly.visualizers.pipeline_lineage_visualizer","text":"","title":"pipeline_lineage_visualizer"},{"location":"api_docs/integrations/#zenml.integrations.plotly.visualizers.pipeline_lineage_visualizer.PipelineLineageVisualizer","text":"Visualize the lineage of runs in a pipeline using plotly. Source code in zenml/integrations/plotly/visualizers/pipeline_lineage_visualizer.py class PipelineLineageVisualizer ( BasePipelineVisualizer ): \"\"\"Visualize the lineage of runs in a pipeline using plotly.\"\"\" @abstractmethod def visualize ( self , object : PipelineView , * args : Any , ** kwargs : Any ) -> Figure : \"\"\"Creates a pipeline lineage diagram using plotly.\"\"\" logger . warning ( \"This integration is not completed yet. Results might be unexpected.\" ) category_dict = {} dimensions = [ \"run\" ] for run in object . runs : category_dict [ run . name ] = { \"run\" : run . name } for step in run . steps : category_dict [ run . name ] . update ( { step . entrypoint_name : str ( step . id ), } ) if step . entrypoint_name not in dimensions : dimensions . append ( f \" { step . entrypoint_name } \" ) category_df = pd . DataFrame . from_dict ( category_dict , orient = \"index\" ) category_df = category_df . reset_index () fig = px . parallel_categories ( category_df , dimensions , color = None , labels = \"status\" , ) fig . show () return fig visualize ( self , object , * args , ** kwargs ) Creates a pipeline lineage diagram using plotly. Source code in zenml/integrations/plotly/visualizers/pipeline_lineage_visualizer.py @abstractmethod def visualize ( self , object : PipelineView , * args : Any , ** kwargs : Any ) -> Figure : \"\"\"Creates a pipeline lineage diagram using plotly.\"\"\" logger . warning ( \"This integration is not completed yet. Results might be unexpected.\" ) category_dict = {} dimensions = [ \"run\" ] for run in object . runs : category_dict [ run . name ] = { \"run\" : run . name } for step in run . steps : category_dict [ run . name ] . update ( { step . entrypoint_name : str ( step . id ), } ) if step . entrypoint_name not in dimensions : dimensions . append ( f \" { step . entrypoint_name } \" ) category_df = pd . DataFrame . from_dict ( category_dict , orient = \"index\" ) category_df = category_df . reset_index () fig = px . parallel_categories ( category_df , dimensions , color = None , labels = \"status\" , ) fig . show () return fig","title":"PipelineLineageVisualizer"},{"location":"api_docs/integrations/#zenml.integrations.pytorch","text":"","title":"pytorch"},{"location":"api_docs/integrations/#zenml.integrations.pytorch.PytorchIntegration","text":"Definition of PyTorch integration for ZenML. Source code in zenml/integrations/pytorch/__init__.py class PytorchIntegration ( Integration ): \"\"\"Definition of PyTorch integration for ZenML.\"\"\" NAME = PYTORCH REQUIREMENTS = [ \"torch\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.pytorch import materializers # noqa","title":"PytorchIntegration"},{"location":"api_docs/integrations/#zenml.integrations.pytorch.PytorchIntegration.activate","text":"Activates the integration. Source code in zenml/integrations/pytorch/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.pytorch import materializers # noqa","title":"activate()"},{"location":"api_docs/integrations/#zenml.integrations.pytorch.materializers","text":"","title":"materializers"},{"location":"api_docs/integrations/#zenml.integrations.pytorch.materializers.pytorch_materializer","text":"","title":"pytorch_materializer"},{"location":"api_docs/integrations/#zenml.integrations.pytorch.materializers.pytorch_materializer.PyTorchMaterializer","text":"Materializer to read/write Pytorch models. Source code in zenml/integrations/pytorch/materializers/pytorch_materializer.py class PyTorchMaterializer ( BaseMaterializer ): \"\"\"Materializer to read/write Pytorch models.\"\"\" ASSOCIATED_TYPES = [ Module , TorchDict ] ASSOCIATED_ARTIFACT_TYPES = [ ModelArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> Union [ Module , TorchDict ]: \"\"\"Reads and returns a PyTorch model. Returns: A loaded pytorch model. \"\"\" super () . handle_input ( data_type ) return torch . load ( os . path . join ( self . artifact . uri , DEFAULT_FILENAME )) # type: ignore[no-untyped-call] # noqa def handle_return ( self , model : Union [ Module , TorchDict ]) -> None : \"\"\"Writes a PyTorch model. Args: model: A torch.nn.Module or a dict to pass into model.save \"\"\" super () . handle_return ( model ) torch . save ( model , os . path . join ( self . artifact . uri , DEFAULT_FILENAME )) handle_input ( self , data_type ) Reads and returns a PyTorch model. Returns: Type Description Union[torch.nn.modules.module.Module, zenml.integrations.pytorch.materializers.pytorch_types.TorchDict] A loaded pytorch model. Source code in zenml/integrations/pytorch/materializers/pytorch_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Union [ Module , TorchDict ]: \"\"\"Reads and returns a PyTorch model. Returns: A loaded pytorch model. \"\"\" super () . handle_input ( data_type ) return torch . load ( os . path . join ( self . artifact . uri , DEFAULT_FILENAME )) # type: ignore[no-untyped-call] # noqa handle_return ( self , model ) Writes a PyTorch model. Parameters: Name Type Description Default model Union[torch.nn.modules.module.Module, zenml.integrations.pytorch.materializers.pytorch_types.TorchDict] A torch.nn.Module or a dict to pass into model.save required Source code in zenml/integrations/pytorch/materializers/pytorch_materializer.py def handle_return ( self , model : Union [ Module , TorchDict ]) -> None : \"\"\"Writes a PyTorch model. Args: model: A torch.nn.Module or a dict to pass into model.save \"\"\" super () . handle_return ( model ) torch . save ( model , os . path . join ( self . artifact . uri , DEFAULT_FILENAME ))","title":"PyTorchMaterializer"},{"location":"api_docs/integrations/#zenml.integrations.pytorch.materializers.pytorch_types","text":"","title":"pytorch_types"},{"location":"api_docs/integrations/#zenml.integrations.pytorch.materializers.pytorch_types.TorchDict","text":"A type of dict that represents saving a model. Source code in zenml/integrations/pytorch/materializers/pytorch_types.py class TorchDict ( Dict [ str , Any ]): \"\"\"A type of dict that represents saving a model.\"\"\"","title":"TorchDict"},{"location":"api_docs/integrations/#zenml.integrations.pytorch_lightning","text":"","title":"pytorch_lightning"},{"location":"api_docs/integrations/#zenml.integrations.pytorch_lightning.PytorchLightningIntegration","text":"Definition of PyTorch Lightning integration for ZenML. Source code in zenml/integrations/pytorch_lightning/__init__.py class PytorchLightningIntegration ( Integration ): \"\"\"Definition of PyTorch Lightning integration for ZenML.\"\"\" NAME = PYTORCH_L REQUIREMENTS = [ \"pytorch_lightning\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.pytorch_lightning import materializers # noqa","title":"PytorchLightningIntegration"},{"location":"api_docs/integrations/#zenml.integrations.pytorch_lightning.PytorchLightningIntegration.activate","text":"Activates the integration. Source code in zenml/integrations/pytorch_lightning/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.pytorch_lightning import materializers # noqa","title":"activate()"},{"location":"api_docs/integrations/#zenml.integrations.pytorch_lightning.materializers","text":"","title":"materializers"},{"location":"api_docs/integrations/#zenml.integrations.pytorch_lightning.materializers.pytorch_lightning_materializer","text":"","title":"pytorch_lightning_materializer"},{"location":"api_docs/integrations/#zenml.integrations.pytorch_lightning.materializers.pytorch_lightning_materializer.PyTorchLightningMaterializer","text":"Materializer to read/write Pytorch models. Source code in zenml/integrations/pytorch_lightning/materializers/pytorch_lightning_materializer.py class PyTorchLightningMaterializer ( BaseMaterializer ): \"\"\"Materializer to read/write Pytorch models.\"\"\" ASSOCIATED_TYPES = [ Trainer ] ASSOCIATED_ARTIFACT_TYPES = [ ModelArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> Trainer : \"\"\"Reads and returns a PyTorch Lightning trainer. Returns: A PyTorch Lightning trainer object. \"\"\" super () . handle_input ( data_type ) return Trainer ( resume_from_checkpoint = os . path . join ( self . artifact . uri , CHECKPOINT_NAME ) ) def handle_return ( self , trainer : Trainer ) -> None : \"\"\"Writes a PyTorch Lightning trainer. Args: trainer: A PyTorch Lightning trainer object. \"\"\" super () . handle_return ( trainer ) trainer . save_checkpoint ( os . path . join ( self . artifact . uri , CHECKPOINT_NAME ) ) handle_input ( self , data_type ) Reads and returns a PyTorch Lightning trainer. Returns: Type Description Trainer A PyTorch Lightning trainer object. Source code in zenml/integrations/pytorch_lightning/materializers/pytorch_lightning_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Trainer : \"\"\"Reads and returns a PyTorch Lightning trainer. Returns: A PyTorch Lightning trainer object. \"\"\" super () . handle_input ( data_type ) return Trainer ( resume_from_checkpoint = os . path . join ( self . artifact . uri , CHECKPOINT_NAME ) ) handle_return ( self , trainer ) Writes a PyTorch Lightning trainer. Parameters: Name Type Description Default trainer Trainer A PyTorch Lightning trainer object. required Source code in zenml/integrations/pytorch_lightning/materializers/pytorch_lightning_materializer.py def handle_return ( self , trainer : Trainer ) -> None : \"\"\"Writes a PyTorch Lightning trainer. Args: trainer: A PyTorch Lightning trainer object. \"\"\" super () . handle_return ( trainer ) trainer . save_checkpoint ( os . path . join ( self . artifact . uri , CHECKPOINT_NAME ) )","title":"PyTorchLightningMaterializer"},{"location":"api_docs/integrations/#zenml.integrations.registry","text":"","title":"registry"},{"location":"api_docs/integrations/#zenml.integrations.registry.IntegrationRegistry","text":"Registry to keep track of ZenML Integrations Source code in zenml/integrations/registry.py class IntegrationRegistry ( object ): \"\"\"Registry to keep track of ZenML Integrations\"\"\" def __init__ ( self ) -> None : \"\"\"Initializing the integration registry\"\"\" self . _integrations : Dict [ str , Type [ \"Integration\" ]] = {} @property def integrations ( self ) -> Dict [ str , Type [ \"Integration\" ]]: \"\"\"Method to get integrations dictionary. Returns: A dict of integration key to type of `Integration`. \"\"\" return self . _integrations @integrations . setter def integrations ( self , i : Any ) -> None : \"\"\"Setter method for the integrations property\"\"\" raise IntegrationError ( \"Please do not manually change the integrations within the \" \"registry. If you would like to register a new integration \" \"manually, please use \" \"`integration_registry.register_integration()`.\" ) def register_integration ( self , key : str , type_ : Type [ \"Integration\" ] ) -> None : \"\"\"Method to register an integration with a given name\"\"\" self . _integrations [ key ] = type_ def activate_integrations ( self ) -> None : \"\"\"Method to activate the integrations with are registered in the registry\"\"\" for name , integration in self . _integrations . items (): if integration . check_installation (): integration . activate () logger . debug ( f \"Integration ` { name } ` is activated.\" ) else : logger . debug ( f \"Integration ` { name } ` could not be activated.\" ) @property def list_integration_names ( self ) -> List [ str ]: \"\"\"Get a list of all possible integrations\"\"\" return [ name for name in self . _integrations ] def select_integration_requirements ( self , integration_name : Optional [ str ] = None ) -> List [ str ]: \"\"\"Select the requirements for a given integration or all integrations\"\"\" if integration_name : if integration_name in self . list_integration_names : return self . _integrations [ integration_name ] . REQUIREMENTS else : raise KeyError ( f \"Version { integration_name } does not exist. \" f \"Currently the following integrations are implemented. \" f \" { self . list_integration_names } \" ) else : return [ requirement for name in self . list_integration_names for requirement in self . _integrations [ name ] . REQUIREMENTS ] def is_installed ( self , integration_name : Optional [ str ] = None ) -> bool : \"\"\"Checks if all requirements for an integration are installed\"\"\" if integration_name in self . list_integration_names : return self . _integrations [ integration_name ] . check_installation () elif not integration_name : all_installed = [ self . _integrations [ item ] . check_installation () for item in self . list_integration_names ] return any ( all_installed ) else : raise KeyError ( f \"Integration ' { integration_name } ' not found. \" f \"Currently the following integrations are available: \" f \" { self . list_integration_names } \" )","title":"IntegrationRegistry"},{"location":"api_docs/integrations/#zenml.integrations.registry.IntegrationRegistry.integrations","text":"Method to get integrations dictionary. Returns: Type Description Dict[str, Type[Integration]] A dict of integration key to type of Integration .","title":"integrations"},{"location":"api_docs/integrations/#zenml.integrations.registry.IntegrationRegistry.list_integration_names","text":"Get a list of all possible integrations","title":"list_integration_names"},{"location":"api_docs/integrations/#zenml.integrations.registry.IntegrationRegistry.__init__","text":"Initializing the integration registry Source code in zenml/integrations/registry.py def __init__ ( self ) -> None : \"\"\"Initializing the integration registry\"\"\" self . _integrations : Dict [ str , Type [ \"Integration\" ]] = {}","title":"__init__()"},{"location":"api_docs/integrations/#zenml.integrations.registry.IntegrationRegistry.activate_integrations","text":"Method to activate the integrations with are registered in the registry Source code in zenml/integrations/registry.py def activate_integrations ( self ) -> None : \"\"\"Method to activate the integrations with are registered in the registry\"\"\" for name , integration in self . _integrations . items (): if integration . check_installation (): integration . activate () logger . debug ( f \"Integration ` { name } ` is activated.\" ) else : logger . debug ( f \"Integration ` { name } ` could not be activated.\" )","title":"activate_integrations()"},{"location":"api_docs/integrations/#zenml.integrations.registry.IntegrationRegistry.is_installed","text":"Checks if all requirements for an integration are installed Source code in zenml/integrations/registry.py def is_installed ( self , integration_name : Optional [ str ] = None ) -> bool : \"\"\"Checks if all requirements for an integration are installed\"\"\" if integration_name in self . list_integration_names : return self . _integrations [ integration_name ] . check_installation () elif not integration_name : all_installed = [ self . _integrations [ item ] . check_installation () for item in self . list_integration_names ] return any ( all_installed ) else : raise KeyError ( f \"Integration ' { integration_name } ' not found. \" f \"Currently the following integrations are available: \" f \" { self . list_integration_names } \" )","title":"is_installed()"},{"location":"api_docs/integrations/#zenml.integrations.registry.IntegrationRegistry.register_integration","text":"Method to register an integration with a given name Source code in zenml/integrations/registry.py def register_integration ( self , key : str , type_ : Type [ \"Integration\" ] ) -> None : \"\"\"Method to register an integration with a given name\"\"\" self . _integrations [ key ] = type_","title":"register_integration()"},{"location":"api_docs/integrations/#zenml.integrations.registry.IntegrationRegistry.select_integration_requirements","text":"Select the requirements for a given integration or all integrations Source code in zenml/integrations/registry.py def select_integration_requirements ( self , integration_name : Optional [ str ] = None ) -> List [ str ]: \"\"\"Select the requirements for a given integration or all integrations\"\"\" if integration_name : if integration_name in self . list_integration_names : return self . _integrations [ integration_name ] . REQUIREMENTS else : raise KeyError ( f \"Version { integration_name } does not exist. \" f \"Currently the following integrations are implemented. \" f \" { self . list_integration_names } \" ) else : return [ requirement for name in self . list_integration_names for requirement in self . _integrations [ name ] . REQUIREMENTS ]","title":"select_integration_requirements()"},{"location":"api_docs/integrations/#zenml.integrations.sklearn","text":"","title":"sklearn"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.SklearnIntegration","text":"Definition of sklearn integration for ZenML. Source code in zenml/integrations/sklearn/__init__.py class SklearnIntegration ( Integration ): \"\"\"Definition of sklearn integration for ZenML.\"\"\" NAME = SKLEARN REQUIREMENTS = [ \"scikit-learn\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.sklearn import materializers # noqa","title":"SklearnIntegration"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.SklearnIntegration.activate","text":"Activates the integration. Source code in zenml/integrations/sklearn/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.sklearn import materializers # noqa","title":"activate()"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.helpers","text":"","title":"helpers"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.helpers.digits","text":"","title":"digits"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.helpers.digits.get_digits","text":"Returns the digits dataset in the form of a tuple of numpy arrays. Source code in zenml/integrations/sklearn/helpers/digits.py def get_digits () -> Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]: \"\"\"Returns the digits dataset in the form of a tuple of numpy arrays.\"\"\" digits = load_digits () # flatten the images n_samples = len ( digits . images ) data = digits . images . reshape (( n_samples , - 1 )) # Split data into 50% train and 50% test subsets X_train , X_test , y_train , y_test = train_test_split ( data , digits . target , test_size = 0.5 , shuffle = False ) return X_train , X_test , y_train , y_test","title":"get_digits()"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.helpers.digits.get_digits_model","text":"Creates a support vector classifier for digits dataset. Source code in zenml/integrations/sklearn/helpers/digits.py def get_digits_model () -> ClassifierMixin : \"\"\"Creates a support vector classifier for digits dataset.\"\"\" return SVC ( gamma = 0.001 )","title":"get_digits_model()"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.materializers","text":"","title":"materializers"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.materializers.sklearn_materializer","text":"","title":"sklearn_materializer"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.materializers.sklearn_materializer.SklearnMaterializer","text":"Materializer to read data to and from sklearn. Source code in zenml/integrations/sklearn/materializers/sklearn_materializer.py class SklearnMaterializer ( BaseMaterializer ): \"\"\"Materializer to read data to and from sklearn.\"\"\" ASSOCIATED_TYPES = [ BaseEstimator , ClassifierMixin , ClusterMixin , BiclusterMixin , OutlierMixin , RegressorMixin , MetaEstimatorMixin , MultiOutputMixin , DensityMixin , TransformerMixin , ] ASSOCIATED_ARTIFACT_TYPES = [ ModelArtifact ] def handle_input ( self , data_type : Type [ Any ] ) -> Union [ BaseEstimator , ClassifierMixin , ClusterMixin , BiclusterMixin , OutlierMixin , RegressorMixin , MetaEstimatorMixin , MultiOutputMixin , DensityMixin , TransformerMixin , ]: \"\"\"Reads a base sklearn model from a pickle file.\"\"\" super () . handle_input ( data_type ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) with fileio . open ( filepath , \"rb\" ) as fid : clf = pickle . load ( fid ) return clf def handle_return ( self , clf : Union [ BaseEstimator , ClassifierMixin , ClusterMixin , BiclusterMixin , OutlierMixin , RegressorMixin , MetaEstimatorMixin , MultiOutputMixin , DensityMixin , TransformerMixin , ], ) -> None : \"\"\"Creates a pickle for a sklearn model. Args: clf: A sklearn model. \"\"\" super () . handle_return ( clf ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) with fileio . open ( filepath , \"wb\" ) as fid : pickle . dump ( clf , fid ) handle_input ( self , data_type ) Reads a base sklearn model from a pickle file. Source code in zenml/integrations/sklearn/materializers/sklearn_materializer.py def handle_input ( self , data_type : Type [ Any ] ) -> Union [ BaseEstimator , ClassifierMixin , ClusterMixin , BiclusterMixin , OutlierMixin , RegressorMixin , MetaEstimatorMixin , MultiOutputMixin , DensityMixin , TransformerMixin , ]: \"\"\"Reads a base sklearn model from a pickle file.\"\"\" super () . handle_input ( data_type ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) with fileio . open ( filepath , \"rb\" ) as fid : clf = pickle . load ( fid ) return clf handle_return ( self , clf ) Creates a pickle for a sklearn model. Parameters: Name Type Description Default clf Union[sklearn.base.BaseEstimator, sklearn.base.ClassifierMixin, sklearn.base.ClusterMixin, sklearn.base.BiclusterMixin, sklearn.base.OutlierMixin, sklearn.base.RegressorMixin, sklearn.base.MetaEstimatorMixin, sklearn.base.MultiOutputMixin, sklearn.base.DensityMixin, sklearn.base.TransformerMixin] A sklearn model. required Source code in zenml/integrations/sklearn/materializers/sklearn_materializer.py def handle_return ( self , clf : Union [ BaseEstimator , ClassifierMixin , ClusterMixin , BiclusterMixin , OutlierMixin , RegressorMixin , MetaEstimatorMixin , MultiOutputMixin , DensityMixin , TransformerMixin , ], ) -> None : \"\"\"Creates a pickle for a sklearn model. Args: clf: A sklearn model. \"\"\" super () . handle_return ( clf ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) with fileio . open ( filepath , \"wb\" ) as fid : pickle . dump ( clf , fid )","title":"SklearnMaterializer"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.steps","text":"","title":"steps"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.steps.sklearn_evaluator","text":"","title":"sklearn_evaluator"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.steps.sklearn_evaluator.SklearnEvaluator","text":"A simple step implementation which utilizes sklearn to evaluate the performance of a given model on a given test dataset Source code in zenml/integrations/sklearn/steps/sklearn_evaluator.py class SklearnEvaluator ( BaseEvaluatorStep ): \"\"\"A simple step implementation which utilizes sklearn to evaluate the performance of a given model on a given test dataset\"\"\" def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , model : tf . keras . Model , config : SklearnEvaluatorConfig , ) -> dict : # type: ignore[type-arg] \"\"\"Method which is responsible for the computation of the evaluation Args: dataset: a pandas Dataframe which represents the test dataset model: a trained tensorflow Keras model config: the configuration for the step Returns: a dictionary which has the evaluation report \"\"\" labels = dataset . pop ( config . label_class_column ) predictions = model . predict ( dataset ) predicted_classes = [ 1 if v > 0.5 else 0 for v in predictions ] report = classification_report ( labels , predicted_classes , output_dict = True ) return report # type: ignore[no-any-return] CONFIG_CLASS ( BaseEvaluatorConfig ) pydantic-model Config class for the sklearn evaluator Source code in zenml/integrations/sklearn/steps/sklearn_evaluator.py class SklearnEvaluatorConfig ( BaseEvaluatorConfig ): \"\"\"Config class for the sklearn evaluator\"\"\" label_class_column : str entrypoint ( self , dataset , model , config ) Method which is responsible for the computation of the evaluation Parameters: Name Type Description Default dataset DataFrame a pandas Dataframe which represents the test dataset required model Model a trained tensorflow Keras model required config SklearnEvaluatorConfig the configuration for the step required Returns: Type Description dict a dictionary which has the evaluation report Source code in zenml/integrations/sklearn/steps/sklearn_evaluator.py def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , model : tf . keras . Model , config : SklearnEvaluatorConfig , ) -> dict : # type: ignore[type-arg] \"\"\"Method which is responsible for the computation of the evaluation Args: dataset: a pandas Dataframe which represents the test dataset model: a trained tensorflow Keras model config: the configuration for the step Returns: a dictionary which has the evaluation report \"\"\" labels = dataset . pop ( config . label_class_column ) predictions = model . predict ( dataset ) predicted_classes = [ 1 if v > 0.5 else 0 for v in predictions ] report = classification_report ( labels , predicted_classes , output_dict = True ) return report # type: ignore[no-any-return]","title":"SklearnEvaluator"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.steps.sklearn_evaluator.SklearnEvaluatorConfig","text":"Config class for the sklearn evaluator Source code in zenml/integrations/sklearn/steps/sklearn_evaluator.py class SklearnEvaluatorConfig ( BaseEvaluatorConfig ): \"\"\"Config class for the sklearn evaluator\"\"\" label_class_column : str","title":"SklearnEvaluatorConfig"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.steps.sklearn_splitter","text":"","title":"sklearn_splitter"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.steps.sklearn_splitter.SklearnSplitter","text":"A simple step implementation which utilizes sklearn to split a given dataset into train, test and validation splits Source code in zenml/integrations/sklearn/steps/sklearn_splitter.py class SklearnSplitter ( BaseSplitStep ): \"\"\"A simple step implementation which utilizes sklearn to split a given dataset into train, test and validation splits\"\"\" def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , config : SklearnSplitterConfig , ) -> Output ( # type:ignore[valid-type] train = pd . DataFrame , test = pd . DataFrame , validation = pd . DataFrame ): \"\"\"Method which is responsible for the splitting logic Args: dataset: a pandas Dataframe which entire dataset config: the configuration for the step Returns: three dataframes representing the splits \"\"\" if ( any ( [ split not in config . ratios for split in [ \"train\" , \"test\" , \"validation\" ] ] ) or len ( config . ratios ) != 3 ): raise KeyError ( f \"Make sure that you only use 'train', 'test' and \" f \"'validation' as keys in the ratios dict. Current keys: \" f \" { config . ratios . keys () } \" ) if sum ( config . ratios . values ()) != 1 : raise ValueError ( f \"Make sure that the ratios sum up to 1. Current \" f \"ratios: { config . ratios } \" ) train_dataset , test_dataset = train_test_split ( dataset , test_size = config . ratios [ \"test\" ] ) train_dataset , val_dataset = train_test_split ( train_dataset , test_size = ( config . ratios [ \"validation\" ] / ( config . ratios [ \"validation\" ] + config . ratios [ \"train\" ]) ), ) return train_dataset , test_dataset , val_dataset CONFIG_CLASS ( BaseSplitStepConfig ) pydantic-model Config class for the sklearn splitter Source code in zenml/integrations/sklearn/steps/sklearn_splitter.py class SklearnSplitterConfig ( BaseSplitStepConfig ): \"\"\"Config class for the sklearn splitter\"\"\" ratios : Dict [ str , float ] entrypoint ( self , dataset , config ) Method which is responsible for the splitting logic Parameters: Name Type Description Default dataset DataFrame a pandas Dataframe which entire dataset required config SklearnSplitterConfig the configuration for the step required Returns: Type Description <zenml.steps.step_output.Output object at 0x7f2d9364ca60> three dataframes representing the splits Source code in zenml/integrations/sklearn/steps/sklearn_splitter.py def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , config : SklearnSplitterConfig , ) -> Output ( # type:ignore[valid-type] train = pd . DataFrame , test = pd . DataFrame , validation = pd . DataFrame ): \"\"\"Method which is responsible for the splitting logic Args: dataset: a pandas Dataframe which entire dataset config: the configuration for the step Returns: three dataframes representing the splits \"\"\" if ( any ( [ split not in config . ratios for split in [ \"train\" , \"test\" , \"validation\" ] ] ) or len ( config . ratios ) != 3 ): raise KeyError ( f \"Make sure that you only use 'train', 'test' and \" f \"'validation' as keys in the ratios dict. Current keys: \" f \" { config . ratios . keys () } \" ) if sum ( config . ratios . values ()) != 1 : raise ValueError ( f \"Make sure that the ratios sum up to 1. Current \" f \"ratios: { config . ratios } \" ) train_dataset , test_dataset = train_test_split ( dataset , test_size = config . ratios [ \"test\" ] ) train_dataset , val_dataset = train_test_split ( train_dataset , test_size = ( config . ratios [ \"validation\" ] / ( config . ratios [ \"validation\" ] + config . ratios [ \"train\" ]) ), ) return train_dataset , test_dataset , val_dataset","title":"SklearnSplitter"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.steps.sklearn_splitter.SklearnSplitterConfig","text":"Config class for the sklearn splitter Source code in zenml/integrations/sklearn/steps/sklearn_splitter.py class SklearnSplitterConfig ( BaseSplitStepConfig ): \"\"\"Config class for the sklearn splitter\"\"\" ratios : Dict [ str , float ]","title":"SklearnSplitterConfig"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.steps.sklearn_standard_scaler","text":"","title":"sklearn_standard_scaler"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.steps.sklearn_standard_scaler.SklearnStandardScaler","text":"Simple step implementation which utilizes the StandardScaler from sklearn to transform the numeric columns of a pd.DataFrame Source code in zenml/integrations/sklearn/steps/sklearn_standard_scaler.py class SklearnStandardScaler ( BasePreprocessorStep ): \"\"\"Simple step implementation which utilizes the StandardScaler from sklearn to transform the numeric columns of a pd.DataFrame\"\"\" def entrypoint ( # type: ignore[override] self , train_dataset : pd . DataFrame , test_dataset : pd . DataFrame , validation_dataset : pd . DataFrame , statistics : pd . DataFrame , schema : pd . DataFrame , config : SklearnStandardScalerConfig , ) -> Output ( # type:ignore[valid-type] train_transformed = pd . DataFrame , test_transformed = pd . DataFrame , validation_transformed = pd . DataFrame , ): \"\"\"Main entrypoint function for the StandardScaler Args: train_dataset: pd.DataFrame, the training dataset test_dataset: pd.DataFrame, the test dataset validation_dataset: pd.DataFrame, the validation dataset statistics: pd.DataFrame, the statistics over the train dataset schema: pd.DataFrame, the detected schema of the dataset config: the configuration for the step Returns: the transformed train, test and validation datasets as pd.DataFrames \"\"\" schema_dict = { k : v [ 0 ] for k , v in schema . to_dict () . items ()} # Exclude columns feature_set = set ( train_dataset . columns ) - set ( config . exclude_columns ) for feature , feature_type in schema_dict . items (): if feature_type != \"int64\" and feature_type != \"float64\" : feature_set . remove ( feature ) logger . warning ( f \" { feature } column is a not numeric, thus it is excluded \" f \"from the standard scaling.\" ) transform_feature_set = feature_set - set ( config . ignore_columns ) # Transform the datasets scaler = StandardScaler () scaler . mean_ = statistics [ \"mean\" ][ transform_feature_set ] scaler . scale_ = statistics [ \"std\" ][ transform_feature_set ] train_dataset [ list ( transform_feature_set )] = scaler . transform ( train_dataset [ transform_feature_set ] ) test_dataset [ list ( transform_feature_set )] = scaler . transform ( test_dataset [ transform_feature_set ] ) validation_dataset [ list ( transform_feature_set )] = scaler . transform ( validation_dataset [ transform_feature_set ] ) return train_dataset , test_dataset , validation_dataset CONFIG_CLASS ( BasePreprocessorConfig ) pydantic-model Config class for the sklearn standard scaler ignore_columns: a list of column names which should not be scaled exclude_columns: a list of column names to be excluded from the dataset Source code in zenml/integrations/sklearn/steps/sklearn_standard_scaler.py class SklearnStandardScalerConfig ( BasePreprocessorConfig ): \"\"\"Config class for the sklearn standard scaler ignore_columns: a list of column names which should not be scaled exclude_columns: a list of column names to be excluded from the dataset \"\"\" ignore_columns : List [ str ] = [] exclude_columns : List [ str ] = [] entrypoint ( self , train_dataset , test_dataset , validation_dataset , statistics , schema , config ) Main entrypoint function for the StandardScaler Parameters: Name Type Description Default train_dataset DataFrame pd.DataFrame, the training dataset required test_dataset DataFrame pd.DataFrame, the test dataset required validation_dataset DataFrame pd.DataFrame, the validation dataset required statistics DataFrame pd.DataFrame, the statistics over the train dataset required schema DataFrame pd.DataFrame, the detected schema of the dataset required config SklearnStandardScalerConfig the configuration for the step required Returns: Type Description <zenml.steps.step_output.Output object at 0x7f2d93663d00> the transformed train, test and validation datasets as pd.DataFrames Source code in zenml/integrations/sklearn/steps/sklearn_standard_scaler.py def entrypoint ( # type: ignore[override] self , train_dataset : pd . DataFrame , test_dataset : pd . DataFrame , validation_dataset : pd . DataFrame , statistics : pd . DataFrame , schema : pd . DataFrame , config : SklearnStandardScalerConfig , ) -> Output ( # type:ignore[valid-type] train_transformed = pd . DataFrame , test_transformed = pd . DataFrame , validation_transformed = pd . DataFrame , ): \"\"\"Main entrypoint function for the StandardScaler Args: train_dataset: pd.DataFrame, the training dataset test_dataset: pd.DataFrame, the test dataset validation_dataset: pd.DataFrame, the validation dataset statistics: pd.DataFrame, the statistics over the train dataset schema: pd.DataFrame, the detected schema of the dataset config: the configuration for the step Returns: the transformed train, test and validation datasets as pd.DataFrames \"\"\" schema_dict = { k : v [ 0 ] for k , v in schema . to_dict () . items ()} # Exclude columns feature_set = set ( train_dataset . columns ) - set ( config . exclude_columns ) for feature , feature_type in schema_dict . items (): if feature_type != \"int64\" and feature_type != \"float64\" : feature_set . remove ( feature ) logger . warning ( f \" { feature } column is a not numeric, thus it is excluded \" f \"from the standard scaling.\" ) transform_feature_set = feature_set - set ( config . ignore_columns ) # Transform the datasets scaler = StandardScaler () scaler . mean_ = statistics [ \"mean\" ][ transform_feature_set ] scaler . scale_ = statistics [ \"std\" ][ transform_feature_set ] train_dataset [ list ( transform_feature_set )] = scaler . transform ( train_dataset [ transform_feature_set ] ) test_dataset [ list ( transform_feature_set )] = scaler . transform ( test_dataset [ transform_feature_set ] ) validation_dataset [ list ( transform_feature_set )] = scaler . transform ( validation_dataset [ transform_feature_set ] ) return train_dataset , test_dataset , validation_dataset","title":"SklearnStandardScaler"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.steps.sklearn_standard_scaler.SklearnStandardScalerConfig","text":"Config class for the sklearn standard scaler ignore_columns: a list of column names which should not be scaled exclude_columns: a list of column names to be excluded from the dataset Source code in zenml/integrations/sklearn/steps/sklearn_standard_scaler.py class SklearnStandardScalerConfig ( BasePreprocessorConfig ): \"\"\"Config class for the sklearn standard scaler ignore_columns: a list of column names which should not be scaled exclude_columns: a list of column names to be excluded from the dataset \"\"\" ignore_columns : List [ str ] = [] exclude_columns : List [ str ] = []","title":"SklearnStandardScalerConfig"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow","text":"","title":"tensorflow"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow.TensorflowIntegration","text":"Definition of Tensorflow integration for ZenML. Source code in zenml/integrations/tensorflow/__init__.py class TensorflowIntegration ( Integration ): \"\"\"Definition of Tensorflow integration for ZenML.\"\"\" NAME = TENSORFLOW REQUIREMENTS = [ \"tensorflow\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.tensorflow import materializers # noqa","title":"TensorflowIntegration"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow.TensorflowIntegration.activate","text":"Activates the integration. Source code in zenml/integrations/tensorflow/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.tensorflow import materializers # noqa","title":"activate()"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow.materializers","text":"","title":"materializers"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow.materializers.keras_materializer","text":"","title":"keras_materializer"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow.materializers.keras_materializer.KerasMaterializer","text":"Materializer to read/write Keras models. Source code in zenml/integrations/tensorflow/materializers/keras_materializer.py class KerasMaterializer ( BaseMaterializer ): \"\"\"Materializer to read/write Keras models.\"\"\" ASSOCIATED_TYPES = [ keras . Model ] ASSOCIATED_ARTIFACT_TYPES = [ ModelArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> keras . Model : \"\"\"Reads and returns a Keras model. Returns: A tf.keras.Model model. \"\"\" super () . handle_input ( data_type ) return keras . models . load_model ( self . artifact . uri ) def handle_return ( self , model : keras . Model ) -> None : \"\"\"Writes a keras model. Args: model: A tf.keras.Model model. \"\"\" super () . handle_return ( model ) model . save ( self . artifact . uri ) handle_input ( self , data_type ) Reads and returns a Keras model. Returns: Type Description Model A tf.keras.Model model. Source code in zenml/integrations/tensorflow/materializers/keras_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> keras . Model : \"\"\"Reads and returns a Keras model. Returns: A tf.keras.Model model. \"\"\" super () . handle_input ( data_type ) return keras . models . load_model ( self . artifact . uri ) handle_return ( self , model ) Writes a keras model. Parameters: Name Type Description Default model Model A tf.keras.Model model. required Source code in zenml/integrations/tensorflow/materializers/keras_materializer.py def handle_return ( self , model : keras . Model ) -> None : \"\"\"Writes a keras model. Args: model: A tf.keras.Model model. \"\"\" super () . handle_return ( model ) model . save ( self . artifact . uri )","title":"KerasMaterializer"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow.materializers.tf_dataset_materializer","text":"","title":"tf_dataset_materializer"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow.materializers.tf_dataset_materializer.TensorflowDatasetMaterializer","text":"Materializer to read data to and from tf.data.Dataset. Source code in zenml/integrations/tensorflow/materializers/tf_dataset_materializer.py class TensorflowDatasetMaterializer ( BaseMaterializer ): \"\"\"Materializer to read data to and from tf.data.Dataset.\"\"\" ASSOCIATED_TYPES = [ tf . data . Dataset ] ASSOCIATED_ARTIFACT_TYPES = [ DataArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads data into tf.data.Dataset\"\"\" super () . handle_input ( data_type ) path = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) return tf . data . experimental . load ( path ) def handle_return ( self , dataset : tf . data . Dataset ) -> None : \"\"\"Persists a tf.data.Dataset object.\"\"\" super () . handle_return ( dataset ) path = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) tf . data . experimental . save ( dataset , path , compression = None , shard_func = None ) handle_input ( self , data_type ) Reads data into tf.data.Dataset Source code in zenml/integrations/tensorflow/materializers/tf_dataset_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads data into tf.data.Dataset\"\"\" super () . handle_input ( data_type ) path = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) return tf . data . experimental . load ( path ) handle_return ( self , dataset ) Persists a tf.data.Dataset object. Source code in zenml/integrations/tensorflow/materializers/tf_dataset_materializer.py def handle_return ( self , dataset : tf . data . Dataset ) -> None : \"\"\"Persists a tf.data.Dataset object.\"\"\" super () . handle_return ( dataset ) path = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) tf . data . experimental . save ( dataset , path , compression = None , shard_func = None )","title":"TensorflowDatasetMaterializer"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow.steps","text":"","title":"steps"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow.steps.tensorflow_trainer","text":"","title":"tensorflow_trainer"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow.steps.tensorflow_trainer.TensorflowBinaryClassifier","text":"Simple step implementation which creates a simple tensorflow feedforward neural network and trains it on a given pd.DataFrame dataset Source code in zenml/integrations/tensorflow/steps/tensorflow_trainer.py class TensorflowBinaryClassifier ( BaseTrainerStep ): \"\"\"Simple step implementation which creates a simple tensorflow feedforward neural network and trains it on a given pd.DataFrame dataset \"\"\" def entrypoint ( # type: ignore[override] self , train_dataset : pd . DataFrame , validation_dataset : pd . DataFrame , config : TensorflowBinaryClassifierConfig , ) -> tf . keras . Model : \"\"\"Main entrypoint for the tensorflow trainer Args: train_dataset: pd.DataFrame, the training dataset validation_dataset: pd.DataFrame, the validation dataset config: the configuration of the step Returns: the trained tf.keras.Model \"\"\" model = tf . keras . Sequential () model . add ( tf . keras . layers . InputLayer ( input_shape = config . input_shape )) model . add ( tf . keras . layers . Flatten ()) last_layer = config . layers . pop () for i , layer in enumerate ( config . layers ): model . add ( tf . keras . layers . Dense ( layer , activation = \"relu\" )) model . add ( tf . keras . layers . Dense ( last_layer , activation = \"sigmoid\" )) model . compile ( optimizer = tf . keras . optimizers . Adam ( config . learning_rate ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = config . metrics , ) train_target = train_dataset . pop ( config . target_column ) validation_target = validation_dataset . pop ( config . target_column ) model . fit ( x = train_dataset , y = train_target , validation_data = ( validation_dataset , validation_target ), batch_size = config . batch_size , epochs = config . epochs , ) model . summary () return model CONFIG_CLASS ( BaseTrainerConfig ) pydantic-model Config class for the tensorflow trainer target_column: the name of the label column layers: the number of units in the fully connected layers input_shape: the shape of the input learning_rate: the learning rate metrics: the list of metrics to be computed epochs: the number of epochs batch_size: the size of the batch Source code in zenml/integrations/tensorflow/steps/tensorflow_trainer.py class TensorflowBinaryClassifierConfig ( BaseTrainerConfig ): \"\"\"Config class for the tensorflow trainer target_column: the name of the label column layers: the number of units in the fully connected layers input_shape: the shape of the input learning_rate: the learning rate metrics: the list of metrics to be computed epochs: the number of epochs batch_size: the size of the batch \"\"\" target_column : str layers : List [ int ] = [ 256 , 64 , 1 ] input_shape : Tuple [ int ] = ( 8 ,) learning_rate : float = 0.001 metrics : List [ str ] = [ \"accuracy\" ] epochs : int = 50 batch_size : int = 8 entrypoint ( self , train_dataset , validation_dataset , config ) Main entrypoint for the tensorflow trainer Parameters: Name Type Description Default train_dataset DataFrame pd.DataFrame, the training dataset required validation_dataset DataFrame pd.DataFrame, the validation dataset required config TensorflowBinaryClassifierConfig the configuration of the step required Returns: Type Description Model the trained tf.keras.Model Source code in zenml/integrations/tensorflow/steps/tensorflow_trainer.py def entrypoint ( # type: ignore[override] self , train_dataset : pd . DataFrame , validation_dataset : pd . DataFrame , config : TensorflowBinaryClassifierConfig , ) -> tf . keras . Model : \"\"\"Main entrypoint for the tensorflow trainer Args: train_dataset: pd.DataFrame, the training dataset validation_dataset: pd.DataFrame, the validation dataset config: the configuration of the step Returns: the trained tf.keras.Model \"\"\" model = tf . keras . Sequential () model . add ( tf . keras . layers . InputLayer ( input_shape = config . input_shape )) model . add ( tf . keras . layers . Flatten ()) last_layer = config . layers . pop () for i , layer in enumerate ( config . layers ): model . add ( tf . keras . layers . Dense ( layer , activation = \"relu\" )) model . add ( tf . keras . layers . Dense ( last_layer , activation = \"sigmoid\" )) model . compile ( optimizer = tf . keras . optimizers . Adam ( config . learning_rate ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = config . metrics , ) train_target = train_dataset . pop ( config . target_column ) validation_target = validation_dataset . pop ( config . target_column ) model . fit ( x = train_dataset , y = train_target , validation_data = ( validation_dataset , validation_target ), batch_size = config . batch_size , epochs = config . epochs , ) model . summary () return model","title":"TensorflowBinaryClassifier"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow.steps.tensorflow_trainer.TensorflowBinaryClassifierConfig","text":"Config class for the tensorflow trainer target_column: the name of the label column layers: the number of units in the fully connected layers input_shape: the shape of the input learning_rate: the learning rate metrics: the list of metrics to be computed epochs: the number of epochs batch_size: the size of the batch Source code in zenml/integrations/tensorflow/steps/tensorflow_trainer.py class TensorflowBinaryClassifierConfig ( BaseTrainerConfig ): \"\"\"Config class for the tensorflow trainer target_column: the name of the label column layers: the number of units in the fully connected layers input_shape: the shape of the input learning_rate: the learning rate metrics: the list of metrics to be computed epochs: the number of epochs batch_size: the size of the batch \"\"\" target_column : str layers : List [ int ] = [ 256 , 64 , 1 ] input_shape : Tuple [ int ] = ( 8 ,) learning_rate : float = 0.001 metrics : List [ str ] = [ \"accuracy\" ] epochs : int = 50 batch_size : int = 8","title":"TensorflowBinaryClassifierConfig"},{"location":"api_docs/integrations/#zenml.integrations.utils","text":"","title":"utils"},{"location":"api_docs/integrations/#zenml.integrations.utils.get_integration_for_module","text":"Gets the integration class for a module inside an integration. If the module given by module_name is not part of a ZenML integration, this method will return None . If it is part of a ZenML integration, it will return the integration class found inside the integration init file. Source code in zenml/integrations/utils.py def get_integration_for_module ( module_name : str ) -> Optional [ Type [ Integration ]]: \"\"\"Gets the integration class for a module inside an integration. If the module given by `module_name` is not part of a ZenML integration, this method will return `None`. If it is part of a ZenML integration, it will return the integration class found inside the integration __init__ file. \"\"\" integration_prefix = \"zenml.integrations.\" if not module_name . startswith ( integration_prefix ): return None integration_module_name = \".\" . join ( module_name . split ( \".\" , 3 )[: 3 ]) try : integration_module = sys . modules [ integration_module_name ] except KeyError : integration_module = importlib . import_module ( integration_module_name ) for name , member in inspect . getmembers ( integration_module ): if ( member is not Integration and isinstance ( member , IntegrationMeta ) and issubclass ( member , Integration ) ): return cast ( Type [ Integration ], member ) return None","title":"get_integration_for_module()"},{"location":"api_docs/integrations/#zenml.integrations.utils.get_requirements_for_module","text":"Gets requirements for a module inside an integration. If the module given by module_name is not part of a ZenML integration, this method will return an empty list. If it is part of a ZenML integration, it will return the list of requirements specified inside the integration class found inside the integration init file. Source code in zenml/integrations/utils.py def get_requirements_for_module ( module_name : str ) -> List [ str ]: \"\"\"Gets requirements for a module inside an integration. If the module given by `module_name` is not part of a ZenML integration, this method will return an empty list. If it is part of a ZenML integration, it will return the list of requirements specified inside the integration class found inside the integration __init__ file. \"\"\" integration = get_integration_for_module ( module_name ) return integration . REQUIREMENTS if integration else []","title":"get_requirements_for_module()"},{"location":"api_docs/io/","text":"Io zenml.io special The io module handles file operations for the ZenML package. It offers a standard interface for reading, writing and manipulating files and directories. It is heavily influenced and inspired by the io module of tfx . fileio append_file ( file_path , file_contents ) Appends file_contents to file. Parameters: Name Type Description Default file_path str Local path in filesystem. required file_contents str Contents of file. required Source code in zenml/io/fileio.py def append_file ( file_path : str , file_contents : str ) -> None : \"\"\"Appends file_contents to file. Args: file_path: Local path in filesystem. file_contents: Contents of file. \"\"\" # with file_io.FileIO(file_path, mode='a') as f: # f.write(file_contents) raise NotImplementedError convert_to_str ( path ) Converts a PathType to a str using UTF-8. Source code in zenml/io/fileio.py def convert_to_str ( path : PathType ) -> str : \"\"\"Converts a PathType to a str using UTF-8.\"\"\" if isinstance ( path , str ): return path else : return path . decode ( \"utf-8\" ) copy ( src , dst , overwrite = False ) Copy a file from the source to the destination. Source code in zenml/io/fileio.py def copy ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Copy a file from the source to the destination.\"\"\" src_fs = _get_filesystem ( src ) dst_fs = _get_filesystem ( dst ) if src_fs is dst_fs : src_fs . copy ( src , dst , overwrite = overwrite ) else : if not overwrite and file_exists ( dst ): raise FileExistsError ( f \"Destination file ' { convert_to_str ( dst ) } ' already exists \" f \"and `overwrite` is false.\" ) contents = open ( src , mode = \"rb\" ) . read () open ( dst , mode = \"wb\" ) . write ( contents ) copy_dir ( source_dir , destination_dir , overwrite = False ) Copies dir from source to destination. Parameters: Name Type Description Default source_dir str Path to copy from. required destination_dir str Path to copy to. required overwrite bool Boolean. If false, function throws an error before overwrite. False Source code in zenml/io/fileio.py def copy_dir ( source_dir : str , destination_dir : str , overwrite : bool = False ) -> None : \"\"\"Copies dir from source to destination. Args: source_dir: Path to copy from. destination_dir: Path to copy to. overwrite: Boolean. If false, function throws an error before overwrite. \"\"\" for source_file in list_dir ( source_dir ): source_file_path = Path ( source_file ) destination_name = os . path . join ( destination_dir , source_file_path . name ) if is_dir ( source_file ): copy_dir ( source_file , destination_name , overwrite ) else : create_dir_recursive_if_not_exists ( str ( Path ( destination_name ) . parent ) ) copy ( str ( source_file_path ), str ( destination_name ), overwrite ) create_dir_if_not_exists ( dir_path ) Creates directory if it does not exist. Parameters: Name Type Description Default dir_path(str) Local path in filesystem. required Source code in zenml/io/fileio.py def create_dir_if_not_exists ( dir_path : str ) -> None : \"\"\"Creates directory if it does not exist. Args: dir_path(str): Local path in filesystem. \"\"\" if not is_dir ( dir_path ): mkdir ( dir_path ) create_dir_recursive_if_not_exists ( dir_path ) Creates directory recursively if it does not exist. Parameters: Name Type Description Default dir_path str Local path in filesystem. required Source code in zenml/io/fileio.py def create_dir_recursive_if_not_exists ( dir_path : str ) -> None : \"\"\"Creates directory recursively if it does not exist. Args: dir_path: Local path in filesystem. \"\"\" if not is_dir ( dir_path ): make_dirs ( dir_path ) create_file_if_not_exists ( file_path , file_contents = ' {} ' ) Creates file if it does not exist. Parameters: Name Type Description Default file_path str Local path in filesystem. required file_contents str Contents of file. '{}' Source code in zenml/io/fileio.py def create_file_if_not_exists ( file_path : str , file_contents : str = \" {} \" ) -> None : \"\"\"Creates file if it does not exist. Args: file_path: Local path in filesystem. file_contents: Contents of file. \"\"\" # if not fileio.exists(file_path): # fileio.(file_path, file_contents) full_path = Path ( file_path ) create_dir_recursive_if_not_exists ( str ( full_path . parent )) with open ( str ( full_path ), \"w\" ) as f : f . write ( file_contents ) file_exists ( path ) Returns True if the given path exists. Source code in zenml/io/fileio.py def file_exists ( path : PathType ) -> bool : \"\"\"Returns `True` if the given path exists.\"\"\" return _get_filesystem ( path ) . exists ( path ) find_files ( dir_path , pattern ) Find files in a directory that match pattern. Parameters: Name Type Description Default dir_path Union[bytes, str] Path to directory. required pattern str pattern like *.png. required Yields: Type Description Iterable[str] All matching filenames if found, else None. Source code in zenml/io/fileio.py def find_files ( dir_path : PathType , pattern : str ) -> Iterable [ str ]: # TODO [ENG-189]: correct docstring since 'None' is never returned \"\"\"Find files in a directory that match pattern. Args: dir_path: Path to directory. pattern: pattern like *.png. Yields: All matching filenames if found, else None. \"\"\" for root , dirs , files in walk ( dir_path ): for basename in files : if fnmatch . fnmatch ( convert_to_str ( basename ), pattern ): filename = os . path . join ( convert_to_str ( root ), convert_to_str ( basename ) ) yield filename get_grandparent ( dir_path ) Get grandparent of dir. Parameters: Name Type Description Default dir_path str Path to directory. required Returns: Type Description str The input paths parents parent. Source code in zenml/io/fileio.py def get_grandparent ( dir_path : str ) -> str : \"\"\"Get grandparent of dir. Args: dir_path: Path to directory. Returns: The input paths parents parent. \"\"\" return Path ( dir_path ) . parent . parent . stem get_parent ( dir_path ) Get parent of dir. Parameters: Name Type Description Default dir_path(str) Path to directory. required Returns: Type Description str Parent (stem) of the dir as a string. Source code in zenml/io/fileio.py def get_parent ( dir_path : str ) -> str : \"\"\"Get parent of dir. Args: dir_path(str): Path to directory. Returns: Parent (stem) of the dir as a string. \"\"\" return Path ( dir_path ) . parent . stem glob ( pattern ) Return the paths that match a glob pattern. Source code in zenml/io/fileio.py def glob ( pattern : PathType ) -> List [ PathType ]: \"\"\"Return the paths that match a glob pattern.\"\"\" return _get_filesystem ( pattern ) . glob ( pattern ) is_dir ( path ) Returns whether the given path points to a directory. Source code in zenml/io/fileio.py def is_dir ( path : PathType ) -> bool : \"\"\"Returns whether the given path points to a directory.\"\"\" return _get_filesystem ( path ) . isdir ( path ) is_remote ( path ) Returns True if path exists remotely. Parameters: Name Type Description Default path str Any path as a string. required Returns: Type Description bool True if remote path, else False. Source code in zenml/io/fileio.py def is_remote ( path : str ) -> bool : \"\"\"Returns True if path exists remotely. Args: path: Any path as a string. Returns: True if remote path, else False. \"\"\" return any ( path . startswith ( prefix ) for prefix in REMOTE_FS_PREFIX ) is_root ( path ) Returns true if path has no parent in local filesystem. Parameters: Name Type Description Default path str Local path in filesystem. required Returns: Type Description bool True if root, else False. Source code in zenml/io/fileio.py def is_root ( path : str ) -> bool : \"\"\"Returns true if path has no parent in local filesystem. Args: path: Local path in filesystem. Returns: True if root, else False. \"\"\" return Path ( path ) . parent == Path ( path ) list_dir ( dir_path , only_file_names = False ) Returns a list of files under dir. Parameters: Name Type Description Default dir_path str Path in filesystem. required only_file_names bool Returns only file names if True. False Returns: Type Description List[str] List of full qualified paths. Source code in zenml/io/fileio.py def list_dir ( dir_path : str , only_file_names : bool = False ) -> List [ str ]: \"\"\"Returns a list of files under dir. Args: dir_path: Path in filesystem. only_file_names: Returns only file names if True. Returns: List of full qualified paths. \"\"\" try : return [ os . path . join ( dir_path , convert_to_str ( f )) if not only_file_names else convert_to_str ( f ) for f in _get_filesystem ( dir_path ) . listdir ( dir_path ) ] except IOError : logger . debug ( f \"Dir { dir_path } not found.\" ) return [] make_dirs ( path ) Make a directory at the given path, recursively creating parents. Source code in zenml/io/fileio.py def make_dirs ( path : PathType ) -> None : \"\"\"Make a directory at the given path, recursively creating parents.\"\"\" _get_filesystem ( path ) . makedirs ( path ) mkdir ( path ) Make a directory at the given path; parent directory must exist. Source code in zenml/io/fileio.py def mkdir ( path : PathType ) -> None : \"\"\"Make a directory at the given path; parent directory must exist.\"\"\" _get_filesystem ( path ) . mkdir ( path ) move ( source , destination , overwrite = False ) Moves dir or file from source to destination. Can be used to rename. Parameters: Name Type Description Default source str Local path to copy from. required destination str Local path to copy to. required overwrite bool boolean, if false, then throws an error before overwrite. False Source code in zenml/io/fileio.py def move ( source : str , destination : str , overwrite : bool = False ) -> None : \"\"\"Moves dir or file from source to destination. Can be used to rename. Args: source: Local path to copy from. destination: Local path to copy to. overwrite: boolean, if false, then throws an error before overwrite. \"\"\" rename ( source , destination , overwrite ) open ( path , mode = 'r' ) Open a file at the given path. Source code in zenml/io/fileio.py def open ( path : PathType , mode : str = \"r\" ) -> Any : # noqa \"\"\"Open a file at the given path.\"\"\" return _get_filesystem ( path ) . open ( path , mode = mode ) remove ( path ) Remove the file at the given path. Dangerous operation. Source code in zenml/io/fileio.py def remove ( path : PathType ) -> None : \"\"\"Remove the file at the given path. Dangerous operation.\"\"\" if not file_exists ( path ): raise FileNotFoundError ( f \" { convert_to_str ( path ) } does not exist!\" ) _get_filesystem ( path ) . remove ( path ) rename ( src , dst , overwrite = False ) Rename source file to destination file. Parameters: Name Type Description Default src Union[bytes, str] The path of the file to rename. required dst Union[bytes, str] The path to rename the source file to. required overwrite bool If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. False Exceptions: Type Description FileExistsError If a file already exists at the destination and overwrite is not set to True . Source code in zenml/io/fileio.py def rename ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Rename source file to destination file. Args: src: The path of the file to rename. dst: The path to rename the source file to. overwrite: If a file already exists at the destination, this method will overwrite it if overwrite=`True` and raise a FileExistsError otherwise. Raises: FileExistsError: If a file already exists at the destination and overwrite is not set to `True`. \"\"\" src_fs = _get_filesystem ( src ) dst_fs = _get_filesystem ( dst ) if src_fs is dst_fs : src_fs . rename ( src , dst , overwrite = overwrite ) else : raise NotImplementedError ( f \"Renaming from { convert_to_str ( src ) } to { convert_to_str ( dst ) } \" f \"using different filesystems plugins is currently not supported.\" ) resolve_relative_path ( path ) Takes relative path and resolves it absolutely. Parameters: Name Type Description Default path str Local path in filesystem. required Returns: Type Description str Resolved path. Source code in zenml/io/fileio.py def resolve_relative_path ( path : str ) -> str : \"\"\"Takes relative path and resolves it absolutely. Args: path: Local path in filesystem. Returns: Resolved path. \"\"\" if is_remote ( path ): return path return str ( Path ( path ) . resolve ()) rm_dir ( dir_path ) Deletes dir recursively. Dangerous operation. Parameters: Name Type Description Default dir_path str Dir to delete. required Source code in zenml/io/fileio.py def rm_dir ( dir_path : str ) -> None : \"\"\"Deletes dir recursively. Dangerous operation. Args: dir_path: Dir to delete. \"\"\" _get_filesystem ( dir_path ) . rmtree ( dir_path ) stat ( path ) Return the stat descriptor for a given file path. Source code in zenml/io/fileio.py def stat ( path : PathType ) -> Any : \"\"\"Return the stat descriptor for a given file path.\"\"\" return _get_filesystem ( path ) . stat ( path ) walk ( top , topdown = True , onerror = None ) Return an iterator that walks the contents of the given directory. Parameters: Name Type Description Default top Union[bytes, str] Path of directory to walk. required topdown bool Whether to walk directories topdown or bottom-up. True onerror Optional[Callable[..., NoneType]] Callable that gets called if an error occurs. None Returns: Type Description Iterable[Tuple[Union[bytes, str], List[Union[bytes, str]], List[Union[bytes, str]]]] An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. Source code in zenml/io/fileio.py def walk ( top : PathType , topdown : bool = True , onerror : Optional [ Callable [ ... , None ]] = None , ) -> Iterable [ Tuple [ PathType , List [ PathType ], List [ PathType ]]]: \"\"\"Return an iterator that walks the contents of the given directory. Args: top: Path of directory to walk. topdown: Whether to walk directories topdown or bottom-up. onerror: Callable that gets called if an error occurs. Returns: An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. \"\"\" return _get_filesystem ( top ) . walk ( top , topdown = topdown , onerror = onerror ) fileio_registry Filesystem registry managing filesystem plugins. FileIORegistry Registry of pluggable filesystem implementations used in TFX components. Source code in zenml/io/fileio_registry.py class FileIORegistry : \"\"\"Registry of pluggable filesystem implementations used in TFX components.\"\"\" def __init__ ( self ) -> None : self . _filesystems : Dict [ PathType , Type [ Filesystem ]] = {} self . _registration_lock = threading . Lock () def register ( self , filesystem_cls : Type [ Filesystem ]) -> None : \"\"\"Register a filesystem implementation. Args: filesystem_cls: Subclass of `tfx.dsl.io.filesystem.Filesystem`. \"\"\" with self . _registration_lock : for scheme in filesystem_cls . SUPPORTED_SCHEMES : current_preferred = self . _filesystems . get ( scheme ) if current_preferred is not None : # TODO: [LOW] Decide what to do here. Do we overwrite, # give out a warning or do we fail? pass self . _filesystems [ scheme ] = filesystem_cls def get_filesystem_for_scheme ( self , scheme : PathType ) -> Type [ Filesystem ]: \"\"\"Get filesystem plugin for given scheme string.\"\"\" if isinstance ( scheme , bytes ): scheme = scheme . decode ( \"utf-8\" ) if scheme not in self . _filesystems : raise Exception ( f \"No filesystems were found for the scheme: \" f \" { scheme } . Please make sure that you are using \" f \"the right path and the all the necessary \" f \"integrations are properly installed.\" ) return self . _filesystems [ scheme ] def get_filesystem_for_path ( self , path : PathType ) -> Type [ Filesystem ]: \"\"\"Get filesystem plugin for given path.\"\"\" # Assume local path by default, but extract filesystem prefix if available. if isinstance ( path , str ): path_bytes = path . encode ( \"utf-8\" ) elif isinstance ( path , bytes ): path_bytes = path else : raise ValueError ( \"Invalid path type: %r .\" % path ) result = re . match ( b \"^([a-z0-9]+://)\" , path_bytes ) if result : scheme = result . group ( 1 ) . decode ( \"utf-8\" ) else : scheme = \"\" return self . get_filesystem_for_scheme ( scheme ) get_filesystem_for_path ( self , path ) Get filesystem plugin for given path. Source code in zenml/io/fileio_registry.py def get_filesystem_for_path ( self , path : PathType ) -> Type [ Filesystem ]: \"\"\"Get filesystem plugin for given path.\"\"\" # Assume local path by default, but extract filesystem prefix if available. if isinstance ( path , str ): path_bytes = path . encode ( \"utf-8\" ) elif isinstance ( path , bytes ): path_bytes = path else : raise ValueError ( \"Invalid path type: %r .\" % path ) result = re . match ( b \"^([a-z0-9]+://)\" , path_bytes ) if result : scheme = result . group ( 1 ) . decode ( \"utf-8\" ) else : scheme = \"\" return self . get_filesystem_for_scheme ( scheme ) get_filesystem_for_scheme ( self , scheme ) Get filesystem plugin for given scheme string. Source code in zenml/io/fileio_registry.py def get_filesystem_for_scheme ( self , scheme : PathType ) -> Type [ Filesystem ]: \"\"\"Get filesystem plugin for given scheme string.\"\"\" if isinstance ( scheme , bytes ): scheme = scheme . decode ( \"utf-8\" ) if scheme not in self . _filesystems : raise Exception ( f \"No filesystems were found for the scheme: \" f \" { scheme } . Please make sure that you are using \" f \"the right path and the all the necessary \" f \"integrations are properly installed.\" ) return self . _filesystems [ scheme ] register ( self , filesystem_cls ) Register a filesystem implementation. Parameters: Name Type Description Default filesystem_cls Type[tfx.dsl.io.filesystem.Filesystem] Subclass of tfx.dsl.io.filesystem.Filesystem . required Source code in zenml/io/fileio_registry.py def register ( self , filesystem_cls : Type [ Filesystem ]) -> None : \"\"\"Register a filesystem implementation. Args: filesystem_cls: Subclass of `tfx.dsl.io.filesystem.Filesystem`. \"\"\" with self . _registration_lock : for scheme in filesystem_cls . SUPPORTED_SCHEMES : current_preferred = self . _filesystems . get ( scheme ) if current_preferred is not None : # TODO: [LOW] Decide what to do here. Do we overwrite, # give out a warning or do we fail? pass self . _filesystems [ scheme ] = filesystem_cls filesystem FileSystemMeta ( type ) Metaclass which is responsible for registering the defined filesystem in the default fileio registry. Source code in zenml/io/filesystem.py class FileSystemMeta ( type ): \"\"\"Metaclass which is responsible for registering the defined filesystem in the default fileio registry.\"\"\" def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"FileSystemMeta\" : \"\"\"Creates the filesystem class and registers it\"\"\" cls = cast ( Type [ \"Filesystem\" ], super () . __new__ ( mcs , name , bases , dct )) if name != \"Filesystem\" : assert cls . SUPPORTED_SCHEMES , ( \"You should specify a list of SUPPORTED_SCHEMES when creating \" \"a filesystem\" ) default_fileio_registry . register ( cls ) return cls __new__ ( mcs , name , bases , dct ) special staticmethod Creates the filesystem class and registers it Source code in zenml/io/filesystem.py def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"FileSystemMeta\" : \"\"\"Creates the filesystem class and registers it\"\"\" cls = cast ( Type [ \"Filesystem\" ], super () . __new__ ( mcs , name , bases , dct )) if name != \"Filesystem\" : assert cls . SUPPORTED_SCHEMES , ( \"You should specify a list of SUPPORTED_SCHEMES when creating \" \"a filesystem\" ) default_fileio_registry . register ( cls ) return cls Filesystem ( Filesystem ) Abstract Filesystem class. Source code in zenml/io/filesystem.py class Filesystem ( BaseFileSystem , metaclass = FileSystemMeta ): \"\"\"Abstract Filesystem class.\"\"\" NotFoundError ( OSError ) Auxiliary not found error Source code in zenml/io/filesystem.py class NotFoundError ( IOError ): \"\"\"Auxiliary not found error\"\"\" utils create_tarfile ( source_dir , output_filename = 'zipped.tar.gz' , exclude_function = None ) Create a compressed representation of source_dir. Parameters: Name Type Description Default source_dir str Path to source dir. required output_filename str Name of outputted gz. 'zipped.tar.gz' exclude_function Optional[Callable[[tarfile.TarInfo], Union[tarfile.TarInfo, NoneType]]] Function that determines whether to exclude file. None Source code in zenml/io/utils.py def create_tarfile ( source_dir : str , output_filename : str = \"zipped.tar.gz\" , exclude_function : Optional [ Callable [[ tarfile . TarInfo ], Optional [ tarfile . TarInfo ]] ] = None , ) -> None : \"\"\"Create a compressed representation of source_dir. Args: source_dir: Path to source dir. output_filename: Name of outputted gz. exclude_function: Function that determines whether to exclude file. \"\"\" if exclude_function is None : # default is to exclude the .zenml directory def exclude_function ( tarinfo : tarfile . TarInfo , ) -> Optional [ tarfile . TarInfo ]: \"\"\"Exclude files from tar. Args: tarinfo: Any Returns: tarinfo required for exclude. \"\"\" filename = tarinfo . name if \".zenml/\" in filename or \"venv/\" in filename : return None else : return tarinfo with tarfile . open ( output_filename , \"w:gz\" ) as tar : tar . add ( source_dir , arcname = \"\" , filter = exclude_function ) extract_tarfile ( source_tar , output_dir ) Extracts all files in a compressed tar file to output_dir. Parameters: Name Type Description Default source_tar str Path to a tar compressed file. required output_dir str Directory where to extract. required Source code in zenml/io/utils.py def extract_tarfile ( source_tar : str , output_dir : str ) -> None : \"\"\"Extracts all files in a compressed tar file to output_dir. Args: source_tar: Path to a tar compressed file. output_dir: Directory where to extract. \"\"\" if is_remote ( source_tar ): raise NotImplementedError ( \"Use local tars for now.\" ) with tarfile . open ( source_tar , \"r:gz\" ) as tar : tar . extractall ( output_dir ) get_global_config_directory () Returns the global config directory for ZenML. Source code in zenml/io/utils.py def get_global_config_directory () -> str : \"\"\"Returns the global config directory for ZenML.\"\"\" return click . get_app_dir ( APP_NAME ) is_gcs_path ( path ) Returns True if path is on Google Cloud Storage. Parameters: Name Type Description Default path str Any path as a string. required Returns: Type Description bool True if gcs path, else False. Source code in zenml/io/utils.py def is_gcs_path ( path : str ) -> bool : \"\"\"Returns True if path is on Google Cloud Storage. Args: path: Any path as a string. Returns: True if gcs path, else False. \"\"\" return path . startswith ( \"gs://\" ) read_file_contents_as_string ( file_path ) Reads contents of file. Parameters: Name Type Description Default file_path str Path to file. required Source code in zenml/io/utils.py def read_file_contents_as_string ( file_path : str ) -> str : \"\"\"Reads contents of file. Args: file_path: Path to file. \"\"\" if not file_exists ( file_path ): raise FileNotFoundError ( f \" { file_path } does not exist!\" ) return open ( file_path ) . read () # type: ignore[no-any-return] write_file_contents_as_string ( file_path , content ) Writes contents of file. Parameters: Name Type Description Default file_path str Path to file. required content str Contents of file. required Source code in zenml/io/utils.py def write_file_contents_as_string ( file_path : str , content : str ) -> None : \"\"\"Writes contents of file. Args: file_path: Path to file. content: Contents of file. \"\"\" with open ( file_path , \"w\" ) as f : f . write ( content )","title":"Io"},{"location":"api_docs/io/#io","text":"","title":"Io"},{"location":"api_docs/io/#zenml.io","text":"The io module handles file operations for the ZenML package. It offers a standard interface for reading, writing and manipulating files and directories. It is heavily influenced and inspired by the io module of tfx .","title":"io"},{"location":"api_docs/io/#zenml.io.fileio","text":"","title":"fileio"},{"location":"api_docs/io/#zenml.io.fileio.append_file","text":"Appends file_contents to file. Parameters: Name Type Description Default file_path str Local path in filesystem. required file_contents str Contents of file. required Source code in zenml/io/fileio.py def append_file ( file_path : str , file_contents : str ) -> None : \"\"\"Appends file_contents to file. Args: file_path: Local path in filesystem. file_contents: Contents of file. \"\"\" # with file_io.FileIO(file_path, mode='a') as f: # f.write(file_contents) raise NotImplementedError","title":"append_file()"},{"location":"api_docs/io/#zenml.io.fileio.convert_to_str","text":"Converts a PathType to a str using UTF-8. Source code in zenml/io/fileio.py def convert_to_str ( path : PathType ) -> str : \"\"\"Converts a PathType to a str using UTF-8.\"\"\" if isinstance ( path , str ): return path else : return path . decode ( \"utf-8\" )","title":"convert_to_str()"},{"location":"api_docs/io/#zenml.io.fileio.copy","text":"Copy a file from the source to the destination. Source code in zenml/io/fileio.py def copy ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Copy a file from the source to the destination.\"\"\" src_fs = _get_filesystem ( src ) dst_fs = _get_filesystem ( dst ) if src_fs is dst_fs : src_fs . copy ( src , dst , overwrite = overwrite ) else : if not overwrite and file_exists ( dst ): raise FileExistsError ( f \"Destination file ' { convert_to_str ( dst ) } ' already exists \" f \"and `overwrite` is false.\" ) contents = open ( src , mode = \"rb\" ) . read () open ( dst , mode = \"wb\" ) . write ( contents )","title":"copy()"},{"location":"api_docs/io/#zenml.io.fileio.copy_dir","text":"Copies dir from source to destination. Parameters: Name Type Description Default source_dir str Path to copy from. required destination_dir str Path to copy to. required overwrite bool Boolean. If false, function throws an error before overwrite. False Source code in zenml/io/fileio.py def copy_dir ( source_dir : str , destination_dir : str , overwrite : bool = False ) -> None : \"\"\"Copies dir from source to destination. Args: source_dir: Path to copy from. destination_dir: Path to copy to. overwrite: Boolean. If false, function throws an error before overwrite. \"\"\" for source_file in list_dir ( source_dir ): source_file_path = Path ( source_file ) destination_name = os . path . join ( destination_dir , source_file_path . name ) if is_dir ( source_file ): copy_dir ( source_file , destination_name , overwrite ) else : create_dir_recursive_if_not_exists ( str ( Path ( destination_name ) . parent ) ) copy ( str ( source_file_path ), str ( destination_name ), overwrite )","title":"copy_dir()"},{"location":"api_docs/io/#zenml.io.fileio.create_dir_if_not_exists","text":"Creates directory if it does not exist. Parameters: Name Type Description Default dir_path(str) Local path in filesystem. required Source code in zenml/io/fileio.py def create_dir_if_not_exists ( dir_path : str ) -> None : \"\"\"Creates directory if it does not exist. Args: dir_path(str): Local path in filesystem. \"\"\" if not is_dir ( dir_path ): mkdir ( dir_path )","title":"create_dir_if_not_exists()"},{"location":"api_docs/io/#zenml.io.fileio.create_dir_recursive_if_not_exists","text":"Creates directory recursively if it does not exist. Parameters: Name Type Description Default dir_path str Local path in filesystem. required Source code in zenml/io/fileio.py def create_dir_recursive_if_not_exists ( dir_path : str ) -> None : \"\"\"Creates directory recursively if it does not exist. Args: dir_path: Local path in filesystem. \"\"\" if not is_dir ( dir_path ): make_dirs ( dir_path )","title":"create_dir_recursive_if_not_exists()"},{"location":"api_docs/io/#zenml.io.fileio.create_file_if_not_exists","text":"Creates file if it does not exist. Parameters: Name Type Description Default file_path str Local path in filesystem. required file_contents str Contents of file. '{}' Source code in zenml/io/fileio.py def create_file_if_not_exists ( file_path : str , file_contents : str = \" {} \" ) -> None : \"\"\"Creates file if it does not exist. Args: file_path: Local path in filesystem. file_contents: Contents of file. \"\"\" # if not fileio.exists(file_path): # fileio.(file_path, file_contents) full_path = Path ( file_path ) create_dir_recursive_if_not_exists ( str ( full_path . parent )) with open ( str ( full_path ), \"w\" ) as f : f . write ( file_contents )","title":"create_file_if_not_exists()"},{"location":"api_docs/io/#zenml.io.fileio.file_exists","text":"Returns True if the given path exists. Source code in zenml/io/fileio.py def file_exists ( path : PathType ) -> bool : \"\"\"Returns `True` if the given path exists.\"\"\" return _get_filesystem ( path ) . exists ( path )","title":"file_exists()"},{"location":"api_docs/io/#zenml.io.fileio.find_files","text":"Find files in a directory that match pattern. Parameters: Name Type Description Default dir_path Union[bytes, str] Path to directory. required pattern str pattern like *.png. required Yields: Type Description Iterable[str] All matching filenames if found, else None. Source code in zenml/io/fileio.py def find_files ( dir_path : PathType , pattern : str ) -> Iterable [ str ]: # TODO [ENG-189]: correct docstring since 'None' is never returned \"\"\"Find files in a directory that match pattern. Args: dir_path: Path to directory. pattern: pattern like *.png. Yields: All matching filenames if found, else None. \"\"\" for root , dirs , files in walk ( dir_path ): for basename in files : if fnmatch . fnmatch ( convert_to_str ( basename ), pattern ): filename = os . path . join ( convert_to_str ( root ), convert_to_str ( basename ) ) yield filename","title":"find_files()"},{"location":"api_docs/io/#zenml.io.fileio.get_grandparent","text":"Get grandparent of dir. Parameters: Name Type Description Default dir_path str Path to directory. required Returns: Type Description str The input paths parents parent. Source code in zenml/io/fileio.py def get_grandparent ( dir_path : str ) -> str : \"\"\"Get grandparent of dir. Args: dir_path: Path to directory. Returns: The input paths parents parent. \"\"\" return Path ( dir_path ) . parent . parent . stem","title":"get_grandparent()"},{"location":"api_docs/io/#zenml.io.fileio.get_parent","text":"Get parent of dir. Parameters: Name Type Description Default dir_path(str) Path to directory. required Returns: Type Description str Parent (stem) of the dir as a string. Source code in zenml/io/fileio.py def get_parent ( dir_path : str ) -> str : \"\"\"Get parent of dir. Args: dir_path(str): Path to directory. Returns: Parent (stem) of the dir as a string. \"\"\" return Path ( dir_path ) . parent . stem","title":"get_parent()"},{"location":"api_docs/io/#zenml.io.fileio.glob","text":"Return the paths that match a glob pattern. Source code in zenml/io/fileio.py def glob ( pattern : PathType ) -> List [ PathType ]: \"\"\"Return the paths that match a glob pattern.\"\"\" return _get_filesystem ( pattern ) . glob ( pattern )","title":"glob()"},{"location":"api_docs/io/#zenml.io.fileio.is_dir","text":"Returns whether the given path points to a directory. Source code in zenml/io/fileio.py def is_dir ( path : PathType ) -> bool : \"\"\"Returns whether the given path points to a directory.\"\"\" return _get_filesystem ( path ) . isdir ( path )","title":"is_dir()"},{"location":"api_docs/io/#zenml.io.fileio.is_remote","text":"Returns True if path exists remotely. Parameters: Name Type Description Default path str Any path as a string. required Returns: Type Description bool True if remote path, else False. Source code in zenml/io/fileio.py def is_remote ( path : str ) -> bool : \"\"\"Returns True if path exists remotely. Args: path: Any path as a string. Returns: True if remote path, else False. \"\"\" return any ( path . startswith ( prefix ) for prefix in REMOTE_FS_PREFIX )","title":"is_remote()"},{"location":"api_docs/io/#zenml.io.fileio.is_root","text":"Returns true if path has no parent in local filesystem. Parameters: Name Type Description Default path str Local path in filesystem. required Returns: Type Description bool True if root, else False. Source code in zenml/io/fileio.py def is_root ( path : str ) -> bool : \"\"\"Returns true if path has no parent in local filesystem. Args: path: Local path in filesystem. Returns: True if root, else False. \"\"\" return Path ( path ) . parent == Path ( path )","title":"is_root()"},{"location":"api_docs/io/#zenml.io.fileio.list_dir","text":"Returns a list of files under dir. Parameters: Name Type Description Default dir_path str Path in filesystem. required only_file_names bool Returns only file names if True. False Returns: Type Description List[str] List of full qualified paths. Source code in zenml/io/fileio.py def list_dir ( dir_path : str , only_file_names : bool = False ) -> List [ str ]: \"\"\"Returns a list of files under dir. Args: dir_path: Path in filesystem. only_file_names: Returns only file names if True. Returns: List of full qualified paths. \"\"\" try : return [ os . path . join ( dir_path , convert_to_str ( f )) if not only_file_names else convert_to_str ( f ) for f in _get_filesystem ( dir_path ) . listdir ( dir_path ) ] except IOError : logger . debug ( f \"Dir { dir_path } not found.\" ) return []","title":"list_dir()"},{"location":"api_docs/io/#zenml.io.fileio.make_dirs","text":"Make a directory at the given path, recursively creating parents. Source code in zenml/io/fileio.py def make_dirs ( path : PathType ) -> None : \"\"\"Make a directory at the given path, recursively creating parents.\"\"\" _get_filesystem ( path ) . makedirs ( path )","title":"make_dirs()"},{"location":"api_docs/io/#zenml.io.fileio.mkdir","text":"Make a directory at the given path; parent directory must exist. Source code in zenml/io/fileio.py def mkdir ( path : PathType ) -> None : \"\"\"Make a directory at the given path; parent directory must exist.\"\"\" _get_filesystem ( path ) . mkdir ( path )","title":"mkdir()"},{"location":"api_docs/io/#zenml.io.fileio.move","text":"Moves dir or file from source to destination. Can be used to rename. Parameters: Name Type Description Default source str Local path to copy from. required destination str Local path to copy to. required overwrite bool boolean, if false, then throws an error before overwrite. False Source code in zenml/io/fileio.py def move ( source : str , destination : str , overwrite : bool = False ) -> None : \"\"\"Moves dir or file from source to destination. Can be used to rename. Args: source: Local path to copy from. destination: Local path to copy to. overwrite: boolean, if false, then throws an error before overwrite. \"\"\" rename ( source , destination , overwrite )","title":"move()"},{"location":"api_docs/io/#zenml.io.fileio.open","text":"Open a file at the given path. Source code in zenml/io/fileio.py def open ( path : PathType , mode : str = \"r\" ) -> Any : # noqa \"\"\"Open a file at the given path.\"\"\" return _get_filesystem ( path ) . open ( path , mode = mode )","title":"open()"},{"location":"api_docs/io/#zenml.io.fileio.remove","text":"Remove the file at the given path. Dangerous operation. Source code in zenml/io/fileio.py def remove ( path : PathType ) -> None : \"\"\"Remove the file at the given path. Dangerous operation.\"\"\" if not file_exists ( path ): raise FileNotFoundError ( f \" { convert_to_str ( path ) } does not exist!\" ) _get_filesystem ( path ) . remove ( path )","title":"remove()"},{"location":"api_docs/io/#zenml.io.fileio.rename","text":"Rename source file to destination file. Parameters: Name Type Description Default src Union[bytes, str] The path of the file to rename. required dst Union[bytes, str] The path to rename the source file to. required overwrite bool If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. False Exceptions: Type Description FileExistsError If a file already exists at the destination and overwrite is not set to True . Source code in zenml/io/fileio.py def rename ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Rename source file to destination file. Args: src: The path of the file to rename. dst: The path to rename the source file to. overwrite: If a file already exists at the destination, this method will overwrite it if overwrite=`True` and raise a FileExistsError otherwise. Raises: FileExistsError: If a file already exists at the destination and overwrite is not set to `True`. \"\"\" src_fs = _get_filesystem ( src ) dst_fs = _get_filesystem ( dst ) if src_fs is dst_fs : src_fs . rename ( src , dst , overwrite = overwrite ) else : raise NotImplementedError ( f \"Renaming from { convert_to_str ( src ) } to { convert_to_str ( dst ) } \" f \"using different filesystems plugins is currently not supported.\" )","title":"rename()"},{"location":"api_docs/io/#zenml.io.fileio.resolve_relative_path","text":"Takes relative path and resolves it absolutely. Parameters: Name Type Description Default path str Local path in filesystem. required Returns: Type Description str Resolved path. Source code in zenml/io/fileio.py def resolve_relative_path ( path : str ) -> str : \"\"\"Takes relative path and resolves it absolutely. Args: path: Local path in filesystem. Returns: Resolved path. \"\"\" if is_remote ( path ): return path return str ( Path ( path ) . resolve ())","title":"resolve_relative_path()"},{"location":"api_docs/io/#zenml.io.fileio.rm_dir","text":"Deletes dir recursively. Dangerous operation. Parameters: Name Type Description Default dir_path str Dir to delete. required Source code in zenml/io/fileio.py def rm_dir ( dir_path : str ) -> None : \"\"\"Deletes dir recursively. Dangerous operation. Args: dir_path: Dir to delete. \"\"\" _get_filesystem ( dir_path ) . rmtree ( dir_path )","title":"rm_dir()"},{"location":"api_docs/io/#zenml.io.fileio.stat","text":"Return the stat descriptor for a given file path. Source code in zenml/io/fileio.py def stat ( path : PathType ) -> Any : \"\"\"Return the stat descriptor for a given file path.\"\"\" return _get_filesystem ( path ) . stat ( path )","title":"stat()"},{"location":"api_docs/io/#zenml.io.fileio.walk","text":"Return an iterator that walks the contents of the given directory. Parameters: Name Type Description Default top Union[bytes, str] Path of directory to walk. required topdown bool Whether to walk directories topdown or bottom-up. True onerror Optional[Callable[..., NoneType]] Callable that gets called if an error occurs. None Returns: Type Description Iterable[Tuple[Union[bytes, str], List[Union[bytes, str]], List[Union[bytes, str]]]] An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. Source code in zenml/io/fileio.py def walk ( top : PathType , topdown : bool = True , onerror : Optional [ Callable [ ... , None ]] = None , ) -> Iterable [ Tuple [ PathType , List [ PathType ], List [ PathType ]]]: \"\"\"Return an iterator that walks the contents of the given directory. Args: top: Path of directory to walk. topdown: Whether to walk directories topdown or bottom-up. onerror: Callable that gets called if an error occurs. Returns: An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. \"\"\" return _get_filesystem ( top ) . walk ( top , topdown = topdown , onerror = onerror )","title":"walk()"},{"location":"api_docs/io/#zenml.io.fileio_registry","text":"Filesystem registry managing filesystem plugins.","title":"fileio_registry"},{"location":"api_docs/io/#zenml.io.fileio_registry.FileIORegistry","text":"Registry of pluggable filesystem implementations used in TFX components. Source code in zenml/io/fileio_registry.py class FileIORegistry : \"\"\"Registry of pluggable filesystem implementations used in TFX components.\"\"\" def __init__ ( self ) -> None : self . _filesystems : Dict [ PathType , Type [ Filesystem ]] = {} self . _registration_lock = threading . Lock () def register ( self , filesystem_cls : Type [ Filesystem ]) -> None : \"\"\"Register a filesystem implementation. Args: filesystem_cls: Subclass of `tfx.dsl.io.filesystem.Filesystem`. \"\"\" with self . _registration_lock : for scheme in filesystem_cls . SUPPORTED_SCHEMES : current_preferred = self . _filesystems . get ( scheme ) if current_preferred is not None : # TODO: [LOW] Decide what to do here. Do we overwrite, # give out a warning or do we fail? pass self . _filesystems [ scheme ] = filesystem_cls def get_filesystem_for_scheme ( self , scheme : PathType ) -> Type [ Filesystem ]: \"\"\"Get filesystem plugin for given scheme string.\"\"\" if isinstance ( scheme , bytes ): scheme = scheme . decode ( \"utf-8\" ) if scheme not in self . _filesystems : raise Exception ( f \"No filesystems were found for the scheme: \" f \" { scheme } . Please make sure that you are using \" f \"the right path and the all the necessary \" f \"integrations are properly installed.\" ) return self . _filesystems [ scheme ] def get_filesystem_for_path ( self , path : PathType ) -> Type [ Filesystem ]: \"\"\"Get filesystem plugin for given path.\"\"\" # Assume local path by default, but extract filesystem prefix if available. if isinstance ( path , str ): path_bytes = path . encode ( \"utf-8\" ) elif isinstance ( path , bytes ): path_bytes = path else : raise ValueError ( \"Invalid path type: %r .\" % path ) result = re . match ( b \"^([a-z0-9]+://)\" , path_bytes ) if result : scheme = result . group ( 1 ) . decode ( \"utf-8\" ) else : scheme = \"\" return self . get_filesystem_for_scheme ( scheme )","title":"FileIORegistry"},{"location":"api_docs/io/#zenml.io.fileio_registry.FileIORegistry.get_filesystem_for_path","text":"Get filesystem plugin for given path. Source code in zenml/io/fileio_registry.py def get_filesystem_for_path ( self , path : PathType ) -> Type [ Filesystem ]: \"\"\"Get filesystem plugin for given path.\"\"\" # Assume local path by default, but extract filesystem prefix if available. if isinstance ( path , str ): path_bytes = path . encode ( \"utf-8\" ) elif isinstance ( path , bytes ): path_bytes = path else : raise ValueError ( \"Invalid path type: %r .\" % path ) result = re . match ( b \"^([a-z0-9]+://)\" , path_bytes ) if result : scheme = result . group ( 1 ) . decode ( \"utf-8\" ) else : scheme = \"\" return self . get_filesystem_for_scheme ( scheme )","title":"get_filesystem_for_path()"},{"location":"api_docs/io/#zenml.io.fileio_registry.FileIORegistry.get_filesystem_for_scheme","text":"Get filesystem plugin for given scheme string. Source code in zenml/io/fileio_registry.py def get_filesystem_for_scheme ( self , scheme : PathType ) -> Type [ Filesystem ]: \"\"\"Get filesystem plugin for given scheme string.\"\"\" if isinstance ( scheme , bytes ): scheme = scheme . decode ( \"utf-8\" ) if scheme not in self . _filesystems : raise Exception ( f \"No filesystems were found for the scheme: \" f \" { scheme } . Please make sure that you are using \" f \"the right path and the all the necessary \" f \"integrations are properly installed.\" ) return self . _filesystems [ scheme ]","title":"get_filesystem_for_scheme()"},{"location":"api_docs/io/#zenml.io.fileio_registry.FileIORegistry.register","text":"Register a filesystem implementation. Parameters: Name Type Description Default filesystem_cls Type[tfx.dsl.io.filesystem.Filesystem] Subclass of tfx.dsl.io.filesystem.Filesystem . required Source code in zenml/io/fileio_registry.py def register ( self , filesystem_cls : Type [ Filesystem ]) -> None : \"\"\"Register a filesystem implementation. Args: filesystem_cls: Subclass of `tfx.dsl.io.filesystem.Filesystem`. \"\"\" with self . _registration_lock : for scheme in filesystem_cls . SUPPORTED_SCHEMES : current_preferred = self . _filesystems . get ( scheme ) if current_preferred is not None : # TODO: [LOW] Decide what to do here. Do we overwrite, # give out a warning or do we fail? pass self . _filesystems [ scheme ] = filesystem_cls","title":"register()"},{"location":"api_docs/io/#zenml.io.filesystem","text":"","title":"filesystem"},{"location":"api_docs/io/#zenml.io.filesystem.FileSystemMeta","text":"Metaclass which is responsible for registering the defined filesystem in the default fileio registry. Source code in zenml/io/filesystem.py class FileSystemMeta ( type ): \"\"\"Metaclass which is responsible for registering the defined filesystem in the default fileio registry.\"\"\" def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"FileSystemMeta\" : \"\"\"Creates the filesystem class and registers it\"\"\" cls = cast ( Type [ \"Filesystem\" ], super () . __new__ ( mcs , name , bases , dct )) if name != \"Filesystem\" : assert cls . SUPPORTED_SCHEMES , ( \"You should specify a list of SUPPORTED_SCHEMES when creating \" \"a filesystem\" ) default_fileio_registry . register ( cls ) return cls","title":"FileSystemMeta"},{"location":"api_docs/io/#zenml.io.filesystem.FileSystemMeta.__new__","text":"Creates the filesystem class and registers it Source code in zenml/io/filesystem.py def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"FileSystemMeta\" : \"\"\"Creates the filesystem class and registers it\"\"\" cls = cast ( Type [ \"Filesystem\" ], super () . __new__ ( mcs , name , bases , dct )) if name != \"Filesystem\" : assert cls . SUPPORTED_SCHEMES , ( \"You should specify a list of SUPPORTED_SCHEMES when creating \" \"a filesystem\" ) default_fileio_registry . register ( cls ) return cls","title":"__new__()"},{"location":"api_docs/io/#zenml.io.filesystem.Filesystem","text":"Abstract Filesystem class. Source code in zenml/io/filesystem.py class Filesystem ( BaseFileSystem , metaclass = FileSystemMeta ): \"\"\"Abstract Filesystem class.\"\"\"","title":"Filesystem"},{"location":"api_docs/io/#zenml.io.filesystem.NotFoundError","text":"Auxiliary not found error Source code in zenml/io/filesystem.py class NotFoundError ( IOError ): \"\"\"Auxiliary not found error\"\"\"","title":"NotFoundError"},{"location":"api_docs/io/#zenml.io.utils","text":"","title":"utils"},{"location":"api_docs/io/#zenml.io.utils.create_tarfile","text":"Create a compressed representation of source_dir. Parameters: Name Type Description Default source_dir str Path to source dir. required output_filename str Name of outputted gz. 'zipped.tar.gz' exclude_function Optional[Callable[[tarfile.TarInfo], Union[tarfile.TarInfo, NoneType]]] Function that determines whether to exclude file. None Source code in zenml/io/utils.py def create_tarfile ( source_dir : str , output_filename : str = \"zipped.tar.gz\" , exclude_function : Optional [ Callable [[ tarfile . TarInfo ], Optional [ tarfile . TarInfo ]] ] = None , ) -> None : \"\"\"Create a compressed representation of source_dir. Args: source_dir: Path to source dir. output_filename: Name of outputted gz. exclude_function: Function that determines whether to exclude file. \"\"\" if exclude_function is None : # default is to exclude the .zenml directory def exclude_function ( tarinfo : tarfile . TarInfo , ) -> Optional [ tarfile . TarInfo ]: \"\"\"Exclude files from tar. Args: tarinfo: Any Returns: tarinfo required for exclude. \"\"\" filename = tarinfo . name if \".zenml/\" in filename or \"venv/\" in filename : return None else : return tarinfo with tarfile . open ( output_filename , \"w:gz\" ) as tar : tar . add ( source_dir , arcname = \"\" , filter = exclude_function )","title":"create_tarfile()"},{"location":"api_docs/io/#zenml.io.utils.extract_tarfile","text":"Extracts all files in a compressed tar file to output_dir. Parameters: Name Type Description Default source_tar str Path to a tar compressed file. required output_dir str Directory where to extract. required Source code in zenml/io/utils.py def extract_tarfile ( source_tar : str , output_dir : str ) -> None : \"\"\"Extracts all files in a compressed tar file to output_dir. Args: source_tar: Path to a tar compressed file. output_dir: Directory where to extract. \"\"\" if is_remote ( source_tar ): raise NotImplementedError ( \"Use local tars for now.\" ) with tarfile . open ( source_tar , \"r:gz\" ) as tar : tar . extractall ( output_dir )","title":"extract_tarfile()"},{"location":"api_docs/io/#zenml.io.utils.get_global_config_directory","text":"Returns the global config directory for ZenML. Source code in zenml/io/utils.py def get_global_config_directory () -> str : \"\"\"Returns the global config directory for ZenML.\"\"\" return click . get_app_dir ( APP_NAME )","title":"get_global_config_directory()"},{"location":"api_docs/io/#zenml.io.utils.is_gcs_path","text":"Returns True if path is on Google Cloud Storage. Parameters: Name Type Description Default path str Any path as a string. required Returns: Type Description bool True if gcs path, else False. Source code in zenml/io/utils.py def is_gcs_path ( path : str ) -> bool : \"\"\"Returns True if path is on Google Cloud Storage. Args: path: Any path as a string. Returns: True if gcs path, else False. \"\"\" return path . startswith ( \"gs://\" )","title":"is_gcs_path()"},{"location":"api_docs/io/#zenml.io.utils.read_file_contents_as_string","text":"Reads contents of file. Parameters: Name Type Description Default file_path str Path to file. required Source code in zenml/io/utils.py def read_file_contents_as_string ( file_path : str ) -> str : \"\"\"Reads contents of file. Args: file_path: Path to file. \"\"\" if not file_exists ( file_path ): raise FileNotFoundError ( f \" { file_path } does not exist!\" ) return open ( file_path ) . read () # type: ignore[no-any-return]","title":"read_file_contents_as_string()"},{"location":"api_docs/io/#zenml.io.utils.write_file_contents_as_string","text":"Writes contents of file. Parameters: Name Type Description Default file_path str Path to file. required content str Contents of file. required Source code in zenml/io/utils.py def write_file_contents_as_string ( file_path : str , content : str ) -> None : \"\"\"Writes contents of file. Args: file_path: Path to file. content: Contents of file. \"\"\" with open ( file_path , \"w\" ) as f : f . write ( content )","title":"write_file_contents_as_string()"},{"location":"api_docs/logger/","text":"Logger zenml.logger CustomFormatter ( Formatter ) Formats logs according to custom specifications. Source code in zenml/logger.py class CustomFormatter ( logging . Formatter ): \"\"\"Formats logs according to custom specifications.\"\"\" grey : str = \" \\x1b [38;21m\" pink : str = \" \\x1b [35m\" green : str = \" \\x1b [32m\" yellow : str = \" \\x1b [33;21m\" red : str = \" \\x1b [31;21m\" bold_red : str = \" \\x1b [31;1m\" purple : str = \" \\x1b [1;35m\" reset : str = \" \\x1b [0m\" format_template : str = ( \" %(asctime)s - %(name)s - %(levelname)s - %(message)s (%(\" \"filename)s: %(lineno)d )\" if LoggingLevels [ ZENML_LOGGING_VERBOSITY ] == LoggingLevels . DEBUG else \" %(message)s \" ) COLORS : Dict [ LoggingLevels , str ] = { LoggingLevels . DEBUG : grey , LoggingLevels . INFO : purple , LoggingLevels . WARN : yellow , LoggingLevels . ERROR : red , LoggingLevels . CRITICAL : bold_red , } def format ( self , record : logging . LogRecord ) -> str : \"\"\"Converts a log record to a (colored) string Args: record: LogRecord generated by the code. Returns: A string formatted according to specifications. \"\"\" log_fmt = ( self . COLORS [ LoggingLevels [ ZENML_LOGGING_VERBOSITY ]] + self . format_template + self . reset ) formatter = logging . Formatter ( log_fmt ) formatted_message = formatter . format ( record ) quoted_groups = re . findall ( \"`([^`]*)`\" , formatted_message ) for quoted in quoted_groups : formatted_message = formatted_message . replace ( \"`\" + quoted + \"`\" , \"`\" + self . reset + self . yellow + quoted + \"`\" + self . COLORS . get ( LoggingLevels [ ZENML_LOGGING_VERBOSITY ]), ) return formatted_message format ( self , record ) Converts a log record to a (colored) string Parameters: Name Type Description Default record LogRecord LogRecord generated by the code. required Returns: Type Description str A string formatted according to specifications. Source code in zenml/logger.py def format ( self , record : logging . LogRecord ) -> str : \"\"\"Converts a log record to a (colored) string Args: record: LogRecord generated by the code. Returns: A string formatted according to specifications. \"\"\" log_fmt = ( self . COLORS [ LoggingLevels [ ZENML_LOGGING_VERBOSITY ]] + self . format_template + self . reset ) formatter = logging . Formatter ( log_fmt ) formatted_message = formatter . format ( record ) quoted_groups = re . findall ( \"`([^`]*)`\" , formatted_message ) for quoted in quoted_groups : formatted_message = formatted_message . replace ( \"`\" + quoted + \"`\" , \"`\" + self . reset + self . yellow + quoted + \"`\" + self . COLORS . get ( LoggingLevels [ ZENML_LOGGING_VERBOSITY ]), ) return formatted_message get_console_handler () Get console handler for logging. Source code in zenml/logger.py def get_console_handler () -> Any : \"\"\"Get console handler for logging.\"\"\" console_handler = logging . StreamHandler ( sys . stdout ) console_handler . setFormatter ( CustomFormatter ()) return console_handler get_file_handler () Return a file handler for logging. Source code in zenml/logger.py def get_file_handler () -> Any : \"\"\"Return a file handler for logging.\"\"\" file_handler = TimedRotatingFileHandler ( LOG_FILE , when = \"midnight\" ) file_handler . setFormatter ( CustomFormatter ()) return file_handler get_logger ( logger_name ) Main function to get logger name,. Parameters: Name Type Description Default logger_name str Name of logger to initialize. required Returns: Type Description Logger A logger object. Source code in zenml/logger.py def get_logger ( logger_name : str ) -> logging . Logger : \"\"\"Main function to get logger name,. Args: logger_name: Name of logger to initialize. Returns: A logger object. \"\"\" logger = logging . getLogger ( logger_name ) logger . setLevel ( get_logging_level () . value ) logger . addHandler ( get_console_handler ()) # TODO [ENG-130]: Add a file handler for persistent handling # logger.addHandler(get_file_handler()) # with this pattern, it's rarely necessary to propagate the error up to # parent logger . propagate = False return logger get_logging_level () Get logging level from the env variable. Source code in zenml/logger.py def get_logging_level () -> LoggingLevels : \"\"\"Get logging level from the env variable.\"\"\" verbosity = ZENML_LOGGING_VERBOSITY . upper () if verbosity not in LoggingLevels . __members__ : raise KeyError ( f \"Verbosity must be one of { list ( LoggingLevels . __members__ . keys ()) } \" ) return LoggingLevels [ verbosity ] init_logging () Initialize logging with default levels. Source code in zenml/logger.py def init_logging () -> None : \"\"\"Initialize logging with default levels.\"\"\" # Mute tensorflow cuda warnings os . environ [ \"TF_CPP_MIN_LOG_LEVEL\" ] = \"3\" set_root_verbosity () # Mute apache_beam muted_logger_names = [ \"apache_beam\" , \"rdbms_metadata_access_object\" , \"apache_beam.io.gcp.bigquery\" , \"backoff\" , \"segment\" , ] for logger_name in muted_logger_names : logging . getLogger ( logger_name ) . setLevel ( logging . WARNING ) logging . getLogger ( logger_name ) . disabled = True # set absl logging absl_logging . set_verbosity ( ABSL_LOGGING_VERBOSITY ) set_root_verbosity () Set the root verbosity. Source code in zenml/logger.py def set_root_verbosity () -> None : \"\"\"Set the root verbosity.\"\"\" level = get_logging_level () if level != LoggingLevels . NOTSET : logging . basicConfig ( level = level . value ) get_logger ( __name__ ) . debug ( f \"Logging set to level: \" f \" { logging . getLevelName ( level . value ) } \" ) else : logging . disable ( sys . maxsize ) logging . getLogger () . disabled = True get_logger ( __name__ ) . debug ( \"Logging NOTSET\" )","title":"Logger"},{"location":"api_docs/logger/#logger","text":"","title":"Logger"},{"location":"api_docs/logger/#zenml.logger","text":"","title":"logger"},{"location":"api_docs/logger/#zenml.logger.CustomFormatter","text":"Formats logs according to custom specifications. Source code in zenml/logger.py class CustomFormatter ( logging . Formatter ): \"\"\"Formats logs according to custom specifications.\"\"\" grey : str = \" \\x1b [38;21m\" pink : str = \" \\x1b [35m\" green : str = \" \\x1b [32m\" yellow : str = \" \\x1b [33;21m\" red : str = \" \\x1b [31;21m\" bold_red : str = \" \\x1b [31;1m\" purple : str = \" \\x1b [1;35m\" reset : str = \" \\x1b [0m\" format_template : str = ( \" %(asctime)s - %(name)s - %(levelname)s - %(message)s (%(\" \"filename)s: %(lineno)d )\" if LoggingLevels [ ZENML_LOGGING_VERBOSITY ] == LoggingLevels . DEBUG else \" %(message)s \" ) COLORS : Dict [ LoggingLevels , str ] = { LoggingLevels . DEBUG : grey , LoggingLevels . INFO : purple , LoggingLevels . WARN : yellow , LoggingLevels . ERROR : red , LoggingLevels . CRITICAL : bold_red , } def format ( self , record : logging . LogRecord ) -> str : \"\"\"Converts a log record to a (colored) string Args: record: LogRecord generated by the code. Returns: A string formatted according to specifications. \"\"\" log_fmt = ( self . COLORS [ LoggingLevels [ ZENML_LOGGING_VERBOSITY ]] + self . format_template + self . reset ) formatter = logging . Formatter ( log_fmt ) formatted_message = formatter . format ( record ) quoted_groups = re . findall ( \"`([^`]*)`\" , formatted_message ) for quoted in quoted_groups : formatted_message = formatted_message . replace ( \"`\" + quoted + \"`\" , \"`\" + self . reset + self . yellow + quoted + \"`\" + self . COLORS . get ( LoggingLevels [ ZENML_LOGGING_VERBOSITY ]), ) return formatted_message","title":"CustomFormatter"},{"location":"api_docs/logger/#zenml.logger.CustomFormatter.format","text":"Converts a log record to a (colored) string Parameters: Name Type Description Default record LogRecord LogRecord generated by the code. required Returns: Type Description str A string formatted according to specifications. Source code in zenml/logger.py def format ( self , record : logging . LogRecord ) -> str : \"\"\"Converts a log record to a (colored) string Args: record: LogRecord generated by the code. Returns: A string formatted according to specifications. \"\"\" log_fmt = ( self . COLORS [ LoggingLevels [ ZENML_LOGGING_VERBOSITY ]] + self . format_template + self . reset ) formatter = logging . Formatter ( log_fmt ) formatted_message = formatter . format ( record ) quoted_groups = re . findall ( \"`([^`]*)`\" , formatted_message ) for quoted in quoted_groups : formatted_message = formatted_message . replace ( \"`\" + quoted + \"`\" , \"`\" + self . reset + self . yellow + quoted + \"`\" + self . COLORS . get ( LoggingLevels [ ZENML_LOGGING_VERBOSITY ]), ) return formatted_message","title":"format()"},{"location":"api_docs/logger/#zenml.logger.get_console_handler","text":"Get console handler for logging. Source code in zenml/logger.py def get_console_handler () -> Any : \"\"\"Get console handler for logging.\"\"\" console_handler = logging . StreamHandler ( sys . stdout ) console_handler . setFormatter ( CustomFormatter ()) return console_handler","title":"get_console_handler()"},{"location":"api_docs/logger/#zenml.logger.get_file_handler","text":"Return a file handler for logging. Source code in zenml/logger.py def get_file_handler () -> Any : \"\"\"Return a file handler for logging.\"\"\" file_handler = TimedRotatingFileHandler ( LOG_FILE , when = \"midnight\" ) file_handler . setFormatter ( CustomFormatter ()) return file_handler","title":"get_file_handler()"},{"location":"api_docs/logger/#zenml.logger.get_logger","text":"Main function to get logger name,. Parameters: Name Type Description Default logger_name str Name of logger to initialize. required Returns: Type Description Logger A logger object. Source code in zenml/logger.py def get_logger ( logger_name : str ) -> logging . Logger : \"\"\"Main function to get logger name,. Args: logger_name: Name of logger to initialize. Returns: A logger object. \"\"\" logger = logging . getLogger ( logger_name ) logger . setLevel ( get_logging_level () . value ) logger . addHandler ( get_console_handler ()) # TODO [ENG-130]: Add a file handler for persistent handling # logger.addHandler(get_file_handler()) # with this pattern, it's rarely necessary to propagate the error up to # parent logger . propagate = False return logger","title":"get_logger()"},{"location":"api_docs/logger/#zenml.logger.get_logging_level","text":"Get logging level from the env variable. Source code in zenml/logger.py def get_logging_level () -> LoggingLevels : \"\"\"Get logging level from the env variable.\"\"\" verbosity = ZENML_LOGGING_VERBOSITY . upper () if verbosity not in LoggingLevels . __members__ : raise KeyError ( f \"Verbosity must be one of { list ( LoggingLevels . __members__ . keys ()) } \" ) return LoggingLevels [ verbosity ]","title":"get_logging_level()"},{"location":"api_docs/logger/#zenml.logger.init_logging","text":"Initialize logging with default levels. Source code in zenml/logger.py def init_logging () -> None : \"\"\"Initialize logging with default levels.\"\"\" # Mute tensorflow cuda warnings os . environ [ \"TF_CPP_MIN_LOG_LEVEL\" ] = \"3\" set_root_verbosity () # Mute apache_beam muted_logger_names = [ \"apache_beam\" , \"rdbms_metadata_access_object\" , \"apache_beam.io.gcp.bigquery\" , \"backoff\" , \"segment\" , ] for logger_name in muted_logger_names : logging . getLogger ( logger_name ) . setLevel ( logging . WARNING ) logging . getLogger ( logger_name ) . disabled = True # set absl logging absl_logging . set_verbosity ( ABSL_LOGGING_VERBOSITY )","title":"init_logging()"},{"location":"api_docs/logger/#zenml.logger.set_root_verbosity","text":"Set the root verbosity. Source code in zenml/logger.py def set_root_verbosity () -> None : \"\"\"Set the root verbosity.\"\"\" level = get_logging_level () if level != LoggingLevels . NOTSET : logging . basicConfig ( level = level . value ) get_logger ( __name__ ) . debug ( f \"Logging set to level: \" f \" { logging . getLevelName ( level . value ) } \" ) else : logging . disable ( sys . maxsize ) logging . getLogger () . disabled = True get_logger ( __name__ ) . debug ( \"Logging NOTSET\" )","title":"set_root_verbosity()"},{"location":"api_docs/materializers/","text":"Materializers zenml.materializers special Materializers are used to convert a ZenML artifact into a specific format. They are most often used to handle the input or output of ZenML steps, and can be extended by building on the BaseMaterializer class. base_materializer BaseMaterializer Base Materializer to realize artifact data. Source code in zenml/materializers/base_materializer.py class BaseMaterializer ( metaclass = BaseMaterializerMeta ): \"\"\"Base Materializer to realize artifact data.\"\"\" ASSOCIATED_ARTIFACT_TYPES : ClassVar [ List [ Type [ \"BaseArtifact\" ]]] = [] ASSOCIATED_TYPES : ClassVar [ List [ Type [ Any ]]] = [] def __init__ ( self , artifact : \"BaseArtifact\" ): \"\"\"Initializes a materializer with the given artifact.\"\"\" self . artifact = artifact def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Write logic here to handle input of the step function. Args: data_type: What type the input should be materialized as. Returns: Any object that is to be passed into the relevant artifact in the step. \"\"\" # TODO [ENG-140]: Add type checking for materializer handle_input # if data_type not in self.ASSOCIATED_TYPES: # raise ValueError( # f\"Data type {data_type} not supported by materializer \" # f\"{self.__name__}. Supported types: {self.ASSOCIATED_TYPES}\" # ) def handle_return ( self , data : Any ) -> None : \"\"\"Write logic here to handle return of the step function. Args: Any object that is specified as an input artifact of the step. \"\"\" # TODO [ENG-141]: Put proper type checking # if data_type not in self.ASSOCIATED_TYPES: # raise ValueError( # f\"Data type {data_type} not supported by materializer \" # f\"{self.__class__.__name__}. Supported types: \" # f\"{self.ASSOCIATED_TYPES}\" # ) __init__ ( self , artifact ) special Initializes a materializer with the given artifact. Source code in zenml/materializers/base_materializer.py def __init__ ( self , artifact : \"BaseArtifact\" ): \"\"\"Initializes a materializer with the given artifact.\"\"\" self . artifact = artifact handle_input ( self , data_type ) Write logic here to handle input of the step function. Parameters: Name Type Description Default data_type Type[Any] What type the input should be materialized as. required Returns: Type Description Any Any object that is to be passed into the relevant artifact in the step. Source code in zenml/materializers/base_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Write logic here to handle input of the step function. Args: data_type: What type the input should be materialized as. Returns: Any object that is to be passed into the relevant artifact in the step. \"\"\" # TODO [ENG-140]: Add type checking for materializer handle_input # if data_type not in self.ASSOCIATED_TYPES: # raise ValueError( # f\"Data type {data_type} not supported by materializer \" # f\"{self.__name__}. Supported types: {self.ASSOCIATED_TYPES}\" # ) handle_return ( self , data ) Write logic here to handle return of the step function. Source code in zenml/materializers/base_materializer.py def handle_return ( self , data : Any ) -> None : \"\"\"Write logic here to handle return of the step function. Args: Any object that is specified as an input artifact of the step. \"\"\" # TODO [ENG-141]: Put proper type checking # if data_type not in self.ASSOCIATED_TYPES: # raise ValueError( # f\"Data type {data_type} not supported by materializer \" # f\"{self.__class__.__name__}. Supported types: \" # f\"{self.ASSOCIATED_TYPES}\" # ) BaseMaterializerMeta ( type ) Metaclass responsible for registering different BaseMaterializer subclasses for reading/writing artifacts. Source code in zenml/materializers/base_materializer.py class BaseMaterializerMeta ( type ): \"\"\"Metaclass responsible for registering different BaseMaterializer subclasses for reading/writing artifacts.\"\"\" def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BaseMaterializerMeta\" : \"\"\"Creates a Materializer class and registers it at the `MaterializerRegistry`.\"\"\" cls = cast ( Type [ \"BaseMaterializer\" ], super () . __new__ ( mcs , name , bases , dct ) ) if name != \"BaseMaterializer\" : assert cls . ASSOCIATED_TYPES , ( \"You should specify a list of ASSOCIATED_TYPES when creating a \" \"Materializer!\" ) for associated_type in cls . ASSOCIATED_TYPES : default_materializer_registry . register_materializer_type ( associated_type , cls ) if cls . ASSOCIATED_ARTIFACT_TYPES : type_registry . register_integration ( associated_type , cls . ASSOCIATED_ARTIFACT_TYPES ) else : from zenml.artifacts.base_artifact import BaseArtifact type_registry . register_integration ( associated_type , [ BaseArtifact ] ) return cls __new__ ( mcs , name , bases , dct ) special staticmethod Creates a Materializer class and registers it at the MaterializerRegistry . Source code in zenml/materializers/base_materializer.py def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BaseMaterializerMeta\" : \"\"\"Creates a Materializer class and registers it at the `MaterializerRegistry`.\"\"\" cls = cast ( Type [ \"BaseMaterializer\" ], super () . __new__ ( mcs , name , bases , dct ) ) if name != \"BaseMaterializer\" : assert cls . ASSOCIATED_TYPES , ( \"You should specify a list of ASSOCIATED_TYPES when creating a \" \"Materializer!\" ) for associated_type in cls . ASSOCIATED_TYPES : default_materializer_registry . register_materializer_type ( associated_type , cls ) if cls . ASSOCIATED_ARTIFACT_TYPES : type_registry . register_integration ( associated_type , cls . ASSOCIATED_ARTIFACT_TYPES ) else : from zenml.artifacts.base_artifact import BaseArtifact type_registry . register_integration ( associated_type , [ BaseArtifact ] ) return cls beam_materializer BeamMaterializer ( BaseMaterializer ) Materializer to read data to and from beam. Source code in zenml/materializers/beam_materializer.py class BeamMaterializer ( BaseMaterializer ): \"\"\"Materializer to read data to and from beam.\"\"\" ASSOCIATED_TYPES = [ beam . PCollection ] ASSOCIATED_ARTIFACT_TYPES = [ DataArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads all files inside the artifact directory and materializes them as a beam compatible output.\"\"\" # TODO [ENG-138]: Implement beam reading super () . handle_input ( data_type ) def handle_return ( self , pipeline : beam . Pipeline ) -> None : \"\"\"Appends a beam.io.WriteToParquet at the end of a beam pipeline and therefore persists the results. Args: pipeline: A beam.pipeline object. \"\"\" # TODO [ENG-139]: Implement beam writing super () . handle_return ( pipeline ) pipeline | beam . ParDo () pipeline . run () # pipeline | beam.io.WriteToParquet(self.artifact.uri) # pipeline.run() handle_input ( self , data_type ) Reads all files inside the artifact directory and materializes them as a beam compatible output. Source code in zenml/materializers/beam_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads all files inside the artifact directory and materializes them as a beam compatible output.\"\"\" # TODO [ENG-138]: Implement beam reading super () . handle_input ( data_type ) handle_return ( self , pipeline ) Appends a beam.io.WriteToParquet at the end of a beam pipeline and therefore persists the results. Parameters: Name Type Description Default pipeline Pipeline A beam.pipeline object. required Source code in zenml/materializers/beam_materializer.py def handle_return ( self , pipeline : beam . Pipeline ) -> None : \"\"\"Appends a beam.io.WriteToParquet at the end of a beam pipeline and therefore persists the results. Args: pipeline: A beam.pipeline object. \"\"\" # TODO [ENG-139]: Implement beam writing super () . handle_return ( pipeline ) pipeline | beam . ParDo () pipeline . run () # pipeline | beam.io.WriteToParquet(self.artifact.uri) # pipeline.run() built_in_materializer BuiltInMaterializer ( BaseMaterializer ) Read/Write JSON files. Source code in zenml/materializers/built_in_materializer.py class BuiltInMaterializer ( BaseMaterializer ): \"\"\"Read/Write JSON files.\"\"\" # TODO [ENG-322]: consider adding typing.Dict and typing.List # since these are the 'correct' way to annotate these types. ASSOCIATED_ARTIFACT_TYPES = [ DataArtifact , DataAnalysisArtifact , ] ASSOCIATED_TYPES = [ int , str , bytes , dict , float , list , tuple , bool , ] def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads basic primitive types from json.\"\"\" super () . handle_input ( data_type ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) contents = yaml_utils . read_json ( filepath ) if type ( contents ) != data_type : # TODO [ENG-142]: Raise error or try to coerce logger . debug ( f \"Contents { contents } was type { type ( contents ) } but expected \" f \" { data_type } \" ) return contents def handle_return ( self , data : Any ) -> None : \"\"\"Handles basic built-in types and stores them as json\"\"\" super () . handle_return ( data ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) yaml_utils . write_json ( filepath , data ) handle_input ( self , data_type ) Reads basic primitive types from json. Source code in zenml/materializers/built_in_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads basic primitive types from json.\"\"\" super () . handle_input ( data_type ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) contents = yaml_utils . read_json ( filepath ) if type ( contents ) != data_type : # TODO [ENG-142]: Raise error or try to coerce logger . debug ( f \"Contents { contents } was type { type ( contents ) } but expected \" f \" { data_type } \" ) return contents handle_return ( self , data ) Handles basic built-in types and stores them as json Source code in zenml/materializers/built_in_materializer.py def handle_return ( self , data : Any ) -> None : \"\"\"Handles basic built-in types and stores them as json\"\"\" super () . handle_return ( data ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) yaml_utils . write_json ( filepath , data ) default_materializer_registry MaterializerRegistry Matches a python type to a default materializer. Source code in zenml/materializers/default_materializer_registry.py class MaterializerRegistry : \"\"\"Matches a python type to a default materializer.\"\"\" def __init__ ( self ) -> None : self . materializer_types : Dict [ Type [ Any ], Type [ \"BaseMaterializer\" ]] = {} def register_materializer_type ( self , key : Type [ Any ], type_ : Type [ \"BaseMaterializer\" ] ) -> None : \"\"\"Registers a new materializer. Args: key: Indicates the type of an object. type_: A BaseMaterializer subclass. \"\"\" if key not in self . materializer_types : self . materializer_types [ key ] = type_ logger . debug ( f \"Registered materializer { type_ } for { key } \" ) else : logger . debug ( f \" { key } already registered with \" f \" { self . materializer_types [ key ] } . Cannot register { type_ } .\" ) def register_and_overwrite_type ( self , key : Type [ Any ], type_ : Type [ \"BaseMaterializer\" ] ) -> None : \"\"\"Registers a new materializer and also overwrites a default if set. Args: key: Indicates the type of an object. type_: A BaseMaterializer subclass. \"\"\" self . materializer_types [ key ] = type_ logger . debug ( f \"Registered materializer { type_ } for { key } \" ) def __getitem__ ( self , key : Type [ Any ]) -> Type [ \"BaseMaterializer\" ]: \"\"\"Get a single materializers based on the key. Args: key: Indicates the type of an object. Returns: `BaseMaterializer` subclass that was registered for this key. \"\"\" if key in self . materializer_types : return self . materializer_types [ key ] else : raise KeyError ( f \"Type { key } does not have a default `Materializer`! Please \" f \"specify your own `Materializer`.\" ) def get_materializer_types ( self , ) -> Dict [ Type [ Any ], Type [ \"BaseMaterializer\" ]]: \"\"\"Get all registered materializer types.\"\"\" return self . materializer_types def is_registered ( self , key : Type [ Any ]) -> bool : \"\"\"Returns if a materializer class is registered for the given type.\"\"\" return key in self . materializer_types __getitem__ ( self , key ) special Get a single materializers based on the key. Parameters: Name Type Description Default key Type[Any] Indicates the type of an object. required Returns: Type Description Type[BaseMaterializer] BaseMaterializer subclass that was registered for this key. Source code in zenml/materializers/default_materializer_registry.py def __getitem__ ( self , key : Type [ Any ]) -> Type [ \"BaseMaterializer\" ]: \"\"\"Get a single materializers based on the key. Args: key: Indicates the type of an object. Returns: `BaseMaterializer` subclass that was registered for this key. \"\"\" if key in self . materializer_types : return self . materializer_types [ key ] else : raise KeyError ( f \"Type { key } does not have a default `Materializer`! Please \" f \"specify your own `Materializer`.\" ) get_materializer_types ( self ) Get all registered materializer types. Source code in zenml/materializers/default_materializer_registry.py def get_materializer_types ( self , ) -> Dict [ Type [ Any ], Type [ \"BaseMaterializer\" ]]: \"\"\"Get all registered materializer types.\"\"\" return self . materializer_types is_registered ( self , key ) Returns if a materializer class is registered for the given type. Source code in zenml/materializers/default_materializer_registry.py def is_registered ( self , key : Type [ Any ]) -> bool : \"\"\"Returns if a materializer class is registered for the given type.\"\"\" return key in self . materializer_types register_and_overwrite_type ( self , key , type_ ) Registers a new materializer and also overwrites a default if set. Parameters: Name Type Description Default key Type[Any] Indicates the type of an object. required type_ Type[BaseMaterializer] A BaseMaterializer subclass. required Source code in zenml/materializers/default_materializer_registry.py def register_and_overwrite_type ( self , key : Type [ Any ], type_ : Type [ \"BaseMaterializer\" ] ) -> None : \"\"\"Registers a new materializer and also overwrites a default if set. Args: key: Indicates the type of an object. type_: A BaseMaterializer subclass. \"\"\" self . materializer_types [ key ] = type_ logger . debug ( f \"Registered materializer { type_ } for { key } \" ) register_materializer_type ( self , key , type_ ) Registers a new materializer. Parameters: Name Type Description Default key Type[Any] Indicates the type of an object. required type_ Type[BaseMaterializer] A BaseMaterializer subclass. required Source code in zenml/materializers/default_materializer_registry.py def register_materializer_type ( self , key : Type [ Any ], type_ : Type [ \"BaseMaterializer\" ] ) -> None : \"\"\"Registers a new materializer. Args: key: Indicates the type of an object. type_: A BaseMaterializer subclass. \"\"\" if key not in self . materializer_types : self . materializer_types [ key ] = type_ logger . debug ( f \"Registered materializer { type_ } for { key } \" ) else : logger . debug ( f \" { key } already registered with \" f \" { self . materializer_types [ key ] } . Cannot register { type_ } .\" ) numpy_materializer NumpyMaterializer ( BaseMaterializer ) Materializer to read data to and from pandas. Source code in zenml/materializers/numpy_materializer.py class NumpyMaterializer ( BaseMaterializer ): \"\"\"Materializer to read data to and from pandas.\"\"\" ASSOCIATED_TYPES = [ np . ndarray ] ASSOCIATED_ARTIFACT_TYPES = [ DataArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> np . ndarray : \"\"\"Reads numpy array from parquet file.\"\"\" super () . handle_input ( data_type ) shape_dict = yaml_utils . read_json ( os . path . join ( self . artifact . uri , SHAPE_FILENAME ) ) shape_tuple = tuple ( shape_dict . values ()) with fileio . open ( os . path . join ( self . artifact . uri , DATA_FILENAME ), \"rb\" ) as f : input_stream = pa . input_stream ( f ) data = pq . read_table ( input_stream ) vals = getattr ( data . to_pandas (), DATA_VAR ) . values return np . reshape ( vals , shape_tuple ) def handle_return ( self , arr : np . ndarray ) -> None : \"\"\"Writes a np.ndarray to the artifact store as a parquet file. Args: arr: The numpy array to write. \"\"\" super () . handle_return ( arr ) yaml_utils . write_json ( os . path . join ( self . artifact . uri , SHAPE_FILENAME ), { str ( i ): x for i , x in enumerate ( arr . shape )}, ) pa_table = pa . table ({ DATA_VAR : arr . flatten ()}) with fileio . open ( os . path . join ( self . artifact . uri , DATA_FILENAME ), \"wb\" ) as f : stream = pa . output_stream ( f ) pq . write_table ( pa_table , stream ) handle_input ( self , data_type ) Reads numpy array from parquet file. Source code in zenml/materializers/numpy_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> np . ndarray : \"\"\"Reads numpy array from parquet file.\"\"\" super () . handle_input ( data_type ) shape_dict = yaml_utils . read_json ( os . path . join ( self . artifact . uri , SHAPE_FILENAME ) ) shape_tuple = tuple ( shape_dict . values ()) with fileio . open ( os . path . join ( self . artifact . uri , DATA_FILENAME ), \"rb\" ) as f : input_stream = pa . input_stream ( f ) data = pq . read_table ( input_stream ) vals = getattr ( data . to_pandas (), DATA_VAR ) . values return np . reshape ( vals , shape_tuple ) handle_return ( self , arr ) Writes a np.ndarray to the artifact store as a parquet file. Parameters: Name Type Description Default arr ndarray The numpy array to write. required Source code in zenml/materializers/numpy_materializer.py def handle_return ( self , arr : np . ndarray ) -> None : \"\"\"Writes a np.ndarray to the artifact store as a parquet file. Args: arr: The numpy array to write. \"\"\" super () . handle_return ( arr ) yaml_utils . write_json ( os . path . join ( self . artifact . uri , SHAPE_FILENAME ), { str ( i ): x for i , x in enumerate ( arr . shape )}, ) pa_table = pa . table ({ DATA_VAR : arr . flatten ()}) with fileio . open ( os . path . join ( self . artifact . uri , DATA_FILENAME ), \"wb\" ) as f : stream = pa . output_stream ( f ) pq . write_table ( pa_table , stream ) pandas_materializer PandasMaterializer ( BaseMaterializer ) Materializer to read data to and from pandas. Source code in zenml/materializers/pandas_materializer.py class PandasMaterializer ( BaseMaterializer ): \"\"\"Materializer to read data to and from pandas.\"\"\" ASSOCIATED_TYPES = [ pd . DataFrame ] ASSOCIATED_ARTIFACT_TYPES = [ DataArtifact , StatisticsArtifact , SchemaArtifact , ] def handle_input ( self , data_type : Type [ Any ]) -> pd . DataFrame : \"\"\"Reads pd.Dataframe from a parquet file.\"\"\" super () . handle_input ( data_type ) return pd . read_parquet ( os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) ) def handle_return ( self , df : pd . DataFrame ) -> None : \"\"\"Writes a pandas dataframe to the specified filename. Args: df: The pandas dataframe to write. \"\"\" super () . handle_return ( df ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) df . to_parquet ( filepath , compression = COMPRESSION_TYPE ) handle_input ( self , data_type ) Reads pd.Dataframe from a parquet file. Source code in zenml/materializers/pandas_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> pd . DataFrame : \"\"\"Reads pd.Dataframe from a parquet file.\"\"\" super () . handle_input ( data_type ) return pd . read_parquet ( os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) ) handle_return ( self , df ) Writes a pandas dataframe to the specified filename. Parameters: Name Type Description Default df DataFrame The pandas dataframe to write. required Source code in zenml/materializers/pandas_materializer.py def handle_return ( self , df : pd . DataFrame ) -> None : \"\"\"Writes a pandas dataframe to the specified filename. Args: df: The pandas dataframe to write. \"\"\" super () . handle_return ( df ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) df . to_parquet ( filepath , compression = COMPRESSION_TYPE )","title":"Materializers"},{"location":"api_docs/materializers/#materializers","text":"","title":"Materializers"},{"location":"api_docs/materializers/#zenml.materializers","text":"Materializers are used to convert a ZenML artifact into a specific format. They are most often used to handle the input or output of ZenML steps, and can be extended by building on the BaseMaterializer class.","title":"materializers"},{"location":"api_docs/materializers/#zenml.materializers.base_materializer","text":"","title":"base_materializer"},{"location":"api_docs/materializers/#zenml.materializers.base_materializer.BaseMaterializer","text":"Base Materializer to realize artifact data. Source code in zenml/materializers/base_materializer.py class BaseMaterializer ( metaclass = BaseMaterializerMeta ): \"\"\"Base Materializer to realize artifact data.\"\"\" ASSOCIATED_ARTIFACT_TYPES : ClassVar [ List [ Type [ \"BaseArtifact\" ]]] = [] ASSOCIATED_TYPES : ClassVar [ List [ Type [ Any ]]] = [] def __init__ ( self , artifact : \"BaseArtifact\" ): \"\"\"Initializes a materializer with the given artifact.\"\"\" self . artifact = artifact def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Write logic here to handle input of the step function. Args: data_type: What type the input should be materialized as. Returns: Any object that is to be passed into the relevant artifact in the step. \"\"\" # TODO [ENG-140]: Add type checking for materializer handle_input # if data_type not in self.ASSOCIATED_TYPES: # raise ValueError( # f\"Data type {data_type} not supported by materializer \" # f\"{self.__name__}. Supported types: {self.ASSOCIATED_TYPES}\" # ) def handle_return ( self , data : Any ) -> None : \"\"\"Write logic here to handle return of the step function. Args: Any object that is specified as an input artifact of the step. \"\"\" # TODO [ENG-141]: Put proper type checking # if data_type not in self.ASSOCIATED_TYPES: # raise ValueError( # f\"Data type {data_type} not supported by materializer \" # f\"{self.__class__.__name__}. Supported types: \" # f\"{self.ASSOCIATED_TYPES}\" # )","title":"BaseMaterializer"},{"location":"api_docs/materializers/#zenml.materializers.base_materializer.BaseMaterializer.__init__","text":"Initializes a materializer with the given artifact. Source code in zenml/materializers/base_materializer.py def __init__ ( self , artifact : \"BaseArtifact\" ): \"\"\"Initializes a materializer with the given artifact.\"\"\" self . artifact = artifact","title":"__init__()"},{"location":"api_docs/materializers/#zenml.materializers.base_materializer.BaseMaterializer.handle_input","text":"Write logic here to handle input of the step function. Parameters: Name Type Description Default data_type Type[Any] What type the input should be materialized as. required Returns: Type Description Any Any object that is to be passed into the relevant artifact in the step. Source code in zenml/materializers/base_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Write logic here to handle input of the step function. Args: data_type: What type the input should be materialized as. Returns: Any object that is to be passed into the relevant artifact in the step. \"\"\" # TODO [ENG-140]: Add type checking for materializer handle_input # if data_type not in self.ASSOCIATED_TYPES: # raise ValueError( # f\"Data type {data_type} not supported by materializer \" # f\"{self.__name__}. Supported types: {self.ASSOCIATED_TYPES}\" # )","title":"handle_input()"},{"location":"api_docs/materializers/#zenml.materializers.base_materializer.BaseMaterializer.handle_return","text":"Write logic here to handle return of the step function. Source code in zenml/materializers/base_materializer.py def handle_return ( self , data : Any ) -> None : \"\"\"Write logic here to handle return of the step function. Args: Any object that is specified as an input artifact of the step. \"\"\" # TODO [ENG-141]: Put proper type checking # if data_type not in self.ASSOCIATED_TYPES: # raise ValueError( # f\"Data type {data_type} not supported by materializer \" # f\"{self.__class__.__name__}. Supported types: \" # f\"{self.ASSOCIATED_TYPES}\" # )","title":"handle_return()"},{"location":"api_docs/materializers/#zenml.materializers.base_materializer.BaseMaterializerMeta","text":"Metaclass responsible for registering different BaseMaterializer subclasses for reading/writing artifacts. Source code in zenml/materializers/base_materializer.py class BaseMaterializerMeta ( type ): \"\"\"Metaclass responsible for registering different BaseMaterializer subclasses for reading/writing artifacts.\"\"\" def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BaseMaterializerMeta\" : \"\"\"Creates a Materializer class and registers it at the `MaterializerRegistry`.\"\"\" cls = cast ( Type [ \"BaseMaterializer\" ], super () . __new__ ( mcs , name , bases , dct ) ) if name != \"BaseMaterializer\" : assert cls . ASSOCIATED_TYPES , ( \"You should specify a list of ASSOCIATED_TYPES when creating a \" \"Materializer!\" ) for associated_type in cls . ASSOCIATED_TYPES : default_materializer_registry . register_materializer_type ( associated_type , cls ) if cls . ASSOCIATED_ARTIFACT_TYPES : type_registry . register_integration ( associated_type , cls . ASSOCIATED_ARTIFACT_TYPES ) else : from zenml.artifacts.base_artifact import BaseArtifact type_registry . register_integration ( associated_type , [ BaseArtifact ] ) return cls","title":"BaseMaterializerMeta"},{"location":"api_docs/materializers/#zenml.materializers.base_materializer.BaseMaterializerMeta.__new__","text":"Creates a Materializer class and registers it at the MaterializerRegistry . Source code in zenml/materializers/base_materializer.py def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BaseMaterializerMeta\" : \"\"\"Creates a Materializer class and registers it at the `MaterializerRegistry`.\"\"\" cls = cast ( Type [ \"BaseMaterializer\" ], super () . __new__ ( mcs , name , bases , dct ) ) if name != \"BaseMaterializer\" : assert cls . ASSOCIATED_TYPES , ( \"You should specify a list of ASSOCIATED_TYPES when creating a \" \"Materializer!\" ) for associated_type in cls . ASSOCIATED_TYPES : default_materializer_registry . register_materializer_type ( associated_type , cls ) if cls . ASSOCIATED_ARTIFACT_TYPES : type_registry . register_integration ( associated_type , cls . ASSOCIATED_ARTIFACT_TYPES ) else : from zenml.artifacts.base_artifact import BaseArtifact type_registry . register_integration ( associated_type , [ BaseArtifact ] ) return cls","title":"__new__()"},{"location":"api_docs/materializers/#zenml.materializers.beam_materializer","text":"","title":"beam_materializer"},{"location":"api_docs/materializers/#zenml.materializers.beam_materializer.BeamMaterializer","text":"Materializer to read data to and from beam. Source code in zenml/materializers/beam_materializer.py class BeamMaterializer ( BaseMaterializer ): \"\"\"Materializer to read data to and from beam.\"\"\" ASSOCIATED_TYPES = [ beam . PCollection ] ASSOCIATED_ARTIFACT_TYPES = [ DataArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads all files inside the artifact directory and materializes them as a beam compatible output.\"\"\" # TODO [ENG-138]: Implement beam reading super () . handle_input ( data_type ) def handle_return ( self , pipeline : beam . Pipeline ) -> None : \"\"\"Appends a beam.io.WriteToParquet at the end of a beam pipeline and therefore persists the results. Args: pipeline: A beam.pipeline object. \"\"\" # TODO [ENG-139]: Implement beam writing super () . handle_return ( pipeline ) pipeline | beam . ParDo () pipeline . run () # pipeline | beam.io.WriteToParquet(self.artifact.uri) # pipeline.run()","title":"BeamMaterializer"},{"location":"api_docs/materializers/#zenml.materializers.beam_materializer.BeamMaterializer.handle_input","text":"Reads all files inside the artifact directory and materializes them as a beam compatible output. Source code in zenml/materializers/beam_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads all files inside the artifact directory and materializes them as a beam compatible output.\"\"\" # TODO [ENG-138]: Implement beam reading super () . handle_input ( data_type )","title":"handle_input()"},{"location":"api_docs/materializers/#zenml.materializers.beam_materializer.BeamMaterializer.handle_return","text":"Appends a beam.io.WriteToParquet at the end of a beam pipeline and therefore persists the results. Parameters: Name Type Description Default pipeline Pipeline A beam.pipeline object. required Source code in zenml/materializers/beam_materializer.py def handle_return ( self , pipeline : beam . Pipeline ) -> None : \"\"\"Appends a beam.io.WriteToParquet at the end of a beam pipeline and therefore persists the results. Args: pipeline: A beam.pipeline object. \"\"\" # TODO [ENG-139]: Implement beam writing super () . handle_return ( pipeline ) pipeline | beam . ParDo () pipeline . run () # pipeline | beam.io.WriteToParquet(self.artifact.uri) # pipeline.run()","title":"handle_return()"},{"location":"api_docs/materializers/#zenml.materializers.built_in_materializer","text":"","title":"built_in_materializer"},{"location":"api_docs/materializers/#zenml.materializers.built_in_materializer.BuiltInMaterializer","text":"Read/Write JSON files. Source code in zenml/materializers/built_in_materializer.py class BuiltInMaterializer ( BaseMaterializer ): \"\"\"Read/Write JSON files.\"\"\" # TODO [ENG-322]: consider adding typing.Dict and typing.List # since these are the 'correct' way to annotate these types. ASSOCIATED_ARTIFACT_TYPES = [ DataArtifact , DataAnalysisArtifact , ] ASSOCIATED_TYPES = [ int , str , bytes , dict , float , list , tuple , bool , ] def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads basic primitive types from json.\"\"\" super () . handle_input ( data_type ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) contents = yaml_utils . read_json ( filepath ) if type ( contents ) != data_type : # TODO [ENG-142]: Raise error or try to coerce logger . debug ( f \"Contents { contents } was type { type ( contents ) } but expected \" f \" { data_type } \" ) return contents def handle_return ( self , data : Any ) -> None : \"\"\"Handles basic built-in types and stores them as json\"\"\" super () . handle_return ( data ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) yaml_utils . write_json ( filepath , data )","title":"BuiltInMaterializer"},{"location":"api_docs/materializers/#zenml.materializers.built_in_materializer.BuiltInMaterializer.handle_input","text":"Reads basic primitive types from json. Source code in zenml/materializers/built_in_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads basic primitive types from json.\"\"\" super () . handle_input ( data_type ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) contents = yaml_utils . read_json ( filepath ) if type ( contents ) != data_type : # TODO [ENG-142]: Raise error or try to coerce logger . debug ( f \"Contents { contents } was type { type ( contents ) } but expected \" f \" { data_type } \" ) return contents","title":"handle_input()"},{"location":"api_docs/materializers/#zenml.materializers.built_in_materializer.BuiltInMaterializer.handle_return","text":"Handles basic built-in types and stores them as json Source code in zenml/materializers/built_in_materializer.py def handle_return ( self , data : Any ) -> None : \"\"\"Handles basic built-in types and stores them as json\"\"\" super () . handle_return ( data ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) yaml_utils . write_json ( filepath , data )","title":"handle_return()"},{"location":"api_docs/materializers/#zenml.materializers.default_materializer_registry","text":"","title":"default_materializer_registry"},{"location":"api_docs/materializers/#zenml.materializers.default_materializer_registry.MaterializerRegistry","text":"Matches a python type to a default materializer. Source code in zenml/materializers/default_materializer_registry.py class MaterializerRegistry : \"\"\"Matches a python type to a default materializer.\"\"\" def __init__ ( self ) -> None : self . materializer_types : Dict [ Type [ Any ], Type [ \"BaseMaterializer\" ]] = {} def register_materializer_type ( self , key : Type [ Any ], type_ : Type [ \"BaseMaterializer\" ] ) -> None : \"\"\"Registers a new materializer. Args: key: Indicates the type of an object. type_: A BaseMaterializer subclass. \"\"\" if key not in self . materializer_types : self . materializer_types [ key ] = type_ logger . debug ( f \"Registered materializer { type_ } for { key } \" ) else : logger . debug ( f \" { key } already registered with \" f \" { self . materializer_types [ key ] } . Cannot register { type_ } .\" ) def register_and_overwrite_type ( self , key : Type [ Any ], type_ : Type [ \"BaseMaterializer\" ] ) -> None : \"\"\"Registers a new materializer and also overwrites a default if set. Args: key: Indicates the type of an object. type_: A BaseMaterializer subclass. \"\"\" self . materializer_types [ key ] = type_ logger . debug ( f \"Registered materializer { type_ } for { key } \" ) def __getitem__ ( self , key : Type [ Any ]) -> Type [ \"BaseMaterializer\" ]: \"\"\"Get a single materializers based on the key. Args: key: Indicates the type of an object. Returns: `BaseMaterializer` subclass that was registered for this key. \"\"\" if key in self . materializer_types : return self . materializer_types [ key ] else : raise KeyError ( f \"Type { key } does not have a default `Materializer`! Please \" f \"specify your own `Materializer`.\" ) def get_materializer_types ( self , ) -> Dict [ Type [ Any ], Type [ \"BaseMaterializer\" ]]: \"\"\"Get all registered materializer types.\"\"\" return self . materializer_types def is_registered ( self , key : Type [ Any ]) -> bool : \"\"\"Returns if a materializer class is registered for the given type.\"\"\" return key in self . materializer_types","title":"MaterializerRegistry"},{"location":"api_docs/materializers/#zenml.materializers.default_materializer_registry.MaterializerRegistry.__getitem__","text":"Get a single materializers based on the key. Parameters: Name Type Description Default key Type[Any] Indicates the type of an object. required Returns: Type Description Type[BaseMaterializer] BaseMaterializer subclass that was registered for this key. Source code in zenml/materializers/default_materializer_registry.py def __getitem__ ( self , key : Type [ Any ]) -> Type [ \"BaseMaterializer\" ]: \"\"\"Get a single materializers based on the key. Args: key: Indicates the type of an object. Returns: `BaseMaterializer` subclass that was registered for this key. \"\"\" if key in self . materializer_types : return self . materializer_types [ key ] else : raise KeyError ( f \"Type { key } does not have a default `Materializer`! Please \" f \"specify your own `Materializer`.\" )","title":"__getitem__()"},{"location":"api_docs/materializers/#zenml.materializers.default_materializer_registry.MaterializerRegistry.get_materializer_types","text":"Get all registered materializer types. Source code in zenml/materializers/default_materializer_registry.py def get_materializer_types ( self , ) -> Dict [ Type [ Any ], Type [ \"BaseMaterializer\" ]]: \"\"\"Get all registered materializer types.\"\"\" return self . materializer_types","title":"get_materializer_types()"},{"location":"api_docs/materializers/#zenml.materializers.default_materializer_registry.MaterializerRegistry.is_registered","text":"Returns if a materializer class is registered for the given type. Source code in zenml/materializers/default_materializer_registry.py def is_registered ( self , key : Type [ Any ]) -> bool : \"\"\"Returns if a materializer class is registered for the given type.\"\"\" return key in self . materializer_types","title":"is_registered()"},{"location":"api_docs/materializers/#zenml.materializers.default_materializer_registry.MaterializerRegistry.register_and_overwrite_type","text":"Registers a new materializer and also overwrites a default if set. Parameters: Name Type Description Default key Type[Any] Indicates the type of an object. required type_ Type[BaseMaterializer] A BaseMaterializer subclass. required Source code in zenml/materializers/default_materializer_registry.py def register_and_overwrite_type ( self , key : Type [ Any ], type_ : Type [ \"BaseMaterializer\" ] ) -> None : \"\"\"Registers a new materializer and also overwrites a default if set. Args: key: Indicates the type of an object. type_: A BaseMaterializer subclass. \"\"\" self . materializer_types [ key ] = type_ logger . debug ( f \"Registered materializer { type_ } for { key } \" )","title":"register_and_overwrite_type()"},{"location":"api_docs/materializers/#zenml.materializers.default_materializer_registry.MaterializerRegistry.register_materializer_type","text":"Registers a new materializer. Parameters: Name Type Description Default key Type[Any] Indicates the type of an object. required type_ Type[BaseMaterializer] A BaseMaterializer subclass. required Source code in zenml/materializers/default_materializer_registry.py def register_materializer_type ( self , key : Type [ Any ], type_ : Type [ \"BaseMaterializer\" ] ) -> None : \"\"\"Registers a new materializer. Args: key: Indicates the type of an object. type_: A BaseMaterializer subclass. \"\"\" if key not in self . materializer_types : self . materializer_types [ key ] = type_ logger . debug ( f \"Registered materializer { type_ } for { key } \" ) else : logger . debug ( f \" { key } already registered with \" f \" { self . materializer_types [ key ] } . Cannot register { type_ } .\" )","title":"register_materializer_type()"},{"location":"api_docs/materializers/#zenml.materializers.numpy_materializer","text":"","title":"numpy_materializer"},{"location":"api_docs/materializers/#zenml.materializers.numpy_materializer.NumpyMaterializer","text":"Materializer to read data to and from pandas. Source code in zenml/materializers/numpy_materializer.py class NumpyMaterializer ( BaseMaterializer ): \"\"\"Materializer to read data to and from pandas.\"\"\" ASSOCIATED_TYPES = [ np . ndarray ] ASSOCIATED_ARTIFACT_TYPES = [ DataArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> np . ndarray : \"\"\"Reads numpy array from parquet file.\"\"\" super () . handle_input ( data_type ) shape_dict = yaml_utils . read_json ( os . path . join ( self . artifact . uri , SHAPE_FILENAME ) ) shape_tuple = tuple ( shape_dict . values ()) with fileio . open ( os . path . join ( self . artifact . uri , DATA_FILENAME ), \"rb\" ) as f : input_stream = pa . input_stream ( f ) data = pq . read_table ( input_stream ) vals = getattr ( data . to_pandas (), DATA_VAR ) . values return np . reshape ( vals , shape_tuple ) def handle_return ( self , arr : np . ndarray ) -> None : \"\"\"Writes a np.ndarray to the artifact store as a parquet file. Args: arr: The numpy array to write. \"\"\" super () . handle_return ( arr ) yaml_utils . write_json ( os . path . join ( self . artifact . uri , SHAPE_FILENAME ), { str ( i ): x for i , x in enumerate ( arr . shape )}, ) pa_table = pa . table ({ DATA_VAR : arr . flatten ()}) with fileio . open ( os . path . join ( self . artifact . uri , DATA_FILENAME ), \"wb\" ) as f : stream = pa . output_stream ( f ) pq . write_table ( pa_table , stream )","title":"NumpyMaterializer"},{"location":"api_docs/materializers/#zenml.materializers.numpy_materializer.NumpyMaterializer.handle_input","text":"Reads numpy array from parquet file. Source code in zenml/materializers/numpy_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> np . ndarray : \"\"\"Reads numpy array from parquet file.\"\"\" super () . handle_input ( data_type ) shape_dict = yaml_utils . read_json ( os . path . join ( self . artifact . uri , SHAPE_FILENAME ) ) shape_tuple = tuple ( shape_dict . values ()) with fileio . open ( os . path . join ( self . artifact . uri , DATA_FILENAME ), \"rb\" ) as f : input_stream = pa . input_stream ( f ) data = pq . read_table ( input_stream ) vals = getattr ( data . to_pandas (), DATA_VAR ) . values return np . reshape ( vals , shape_tuple )","title":"handle_input()"},{"location":"api_docs/materializers/#zenml.materializers.numpy_materializer.NumpyMaterializer.handle_return","text":"Writes a np.ndarray to the artifact store as a parquet file. Parameters: Name Type Description Default arr ndarray The numpy array to write. required Source code in zenml/materializers/numpy_materializer.py def handle_return ( self , arr : np . ndarray ) -> None : \"\"\"Writes a np.ndarray to the artifact store as a parquet file. Args: arr: The numpy array to write. \"\"\" super () . handle_return ( arr ) yaml_utils . write_json ( os . path . join ( self . artifact . uri , SHAPE_FILENAME ), { str ( i ): x for i , x in enumerate ( arr . shape )}, ) pa_table = pa . table ({ DATA_VAR : arr . flatten ()}) with fileio . open ( os . path . join ( self . artifact . uri , DATA_FILENAME ), \"wb\" ) as f : stream = pa . output_stream ( f ) pq . write_table ( pa_table , stream )","title":"handle_return()"},{"location":"api_docs/materializers/#zenml.materializers.pandas_materializer","text":"","title":"pandas_materializer"},{"location":"api_docs/materializers/#zenml.materializers.pandas_materializer.PandasMaterializer","text":"Materializer to read data to and from pandas. Source code in zenml/materializers/pandas_materializer.py class PandasMaterializer ( BaseMaterializer ): \"\"\"Materializer to read data to and from pandas.\"\"\" ASSOCIATED_TYPES = [ pd . DataFrame ] ASSOCIATED_ARTIFACT_TYPES = [ DataArtifact , StatisticsArtifact , SchemaArtifact , ] def handle_input ( self , data_type : Type [ Any ]) -> pd . DataFrame : \"\"\"Reads pd.Dataframe from a parquet file.\"\"\" super () . handle_input ( data_type ) return pd . read_parquet ( os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) ) def handle_return ( self , df : pd . DataFrame ) -> None : \"\"\"Writes a pandas dataframe to the specified filename. Args: df: The pandas dataframe to write. \"\"\" super () . handle_return ( df ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) df . to_parquet ( filepath , compression = COMPRESSION_TYPE )","title":"PandasMaterializer"},{"location":"api_docs/materializers/#zenml.materializers.pandas_materializer.PandasMaterializer.handle_input","text":"Reads pd.Dataframe from a parquet file. Source code in zenml/materializers/pandas_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> pd . DataFrame : \"\"\"Reads pd.Dataframe from a parquet file.\"\"\" super () . handle_input ( data_type ) return pd . read_parquet ( os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) )","title":"handle_input()"},{"location":"api_docs/materializers/#zenml.materializers.pandas_materializer.PandasMaterializer.handle_return","text":"Writes a pandas dataframe to the specified filename. Parameters: Name Type Description Default df DataFrame The pandas dataframe to write. required Source code in zenml/materializers/pandas_materializer.py def handle_return ( self , df : pd . DataFrame ) -> None : \"\"\"Writes a pandas dataframe to the specified filename. Args: df: The pandas dataframe to write. \"\"\" super () . handle_return ( df ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) df . to_parquet ( filepath , compression = COMPRESSION_TYPE )","title":"handle_return()"},{"location":"api_docs/metadata_stores/","text":"Metadata Stores zenml.metadata_stores special The configuration of each pipeline, step, backend, and produced artifacts are all tracked within the metadata store. The metadata store is an SQL database, and can be sqlite or mysql . Metadata are the pieces of information tracked about the pipelines, experiments and configurations that you are running with ZenML. Metadata are stored inside the metadata store. base_metadata_store BaseMetadataStore ( StackComponent , ABC ) pydantic-model Base class for all ZenML metadata stores. Source code in zenml/metadata_stores/base_metadata_store.py class BaseMetadataStore ( StackComponent , ABC ): \"\"\"Base class for all ZenML metadata stores.\"\"\" @property def type ( self ) -> StackComponentType : \"\"\"The component type.\"\"\" return StackComponentType . METADATA_STORE @property @abstractmethod def flavor ( self ) -> MetadataStoreFlavor : \"\"\"The metadata store flavor.\"\"\" @property def store ( self ) -> metadata_store . MetadataStore : \"\"\"General property that hooks into TFX metadata store.\"\"\" # TODO [ENG-133]: this always gets recreated, is this intended? return metadata_store . MetadataStore ( self . get_tfx_metadata_config (), enable_upgrade_migration = True ) @abstractmethod def get_tfx_metadata_config ( self , ) -> Union [ metadata_store_pb2 . ConnectionConfig , metadata_store_pb2 . MetadataStoreClientConfig , ]: \"\"\"Return tfx metadata config.\"\"\" raise NotImplementedError @property def step_type_mapping ( self ) -> Dict [ int , str ]: \"\"\"Maps type_id's to step names.\"\"\" return { type_ . id : type_ . name for type_ in self . store . get_execution_types () } def _check_if_executions_belong_to_pipeline ( self , executions : List [ proto . Execution ], pipeline : PipelineView , ) -> bool : \"\"\"Returns `True` if the executions are associated with the pipeline context.\"\"\" for execution in executions : associated_contexts = self . store . get_contexts_by_execution ( execution . id ) for context in associated_contexts : if context . id == pipeline . _id : # noqa return True return False def _get_step_view_from_execution ( self , execution : proto . Execution ) -> StepView : \"\"\"Get original StepView from an execution. Args: execution: proto.Execution object from mlmd store. Returns: Original `StepView` derived from the proto.Execution. \"\"\" impl_name = self . step_type_mapping [ execution . type_id ] . split ( \".\" )[ - 1 ] step_name_property = execution . custom_properties . get ( INTERNAL_EXECUTION_PARAMETER_PREFIX + PARAM_PIPELINE_PARAMETER_NAME , None , ) if step_name_property : step_name = json . loads ( step_name_property . string_value ) else : raise KeyError ( f \"Step name missing for execution with ID { execution . id } . \" f \"This error probably occurs because you're using ZenML \" f \"version 0.5.4 or newer but your metadata store contains \" f \"data from previous versions.\" ) step_parameters = {} for k , v in execution . custom_properties . items (): if not k . startswith ( INTERNAL_EXECUTION_PARAMETER_PREFIX ): try : step_parameters [ k ] = json . loads ( v . string_value ) except JSONDecodeError : # this means there is a property in there that is neither # an internal one or one created by zenml. Therefore, we can # ignore it pass # TODO [ENG-222]: This is a lot of querying to the metadata store. We # should refactor and make it nicer. Probably it makes more sense # to first get `executions_ids_for_current_run` and then filter on # `event.execution_id in execution_ids_for_current_run`. # Core logic here is that we get the event of this particular execution # id that gives us the artifacts of this execution. We then go through # all `input` artifacts of this execution and get all events related to # that artifact. This in turn gives us other events for which this # artifact was an `output` artifact. Then we simply need to sort by # time to get the most recent execution (i.e. step) that produced that # particular artifact. events_for_execution = self . store . get_events_by_execution_ids ( [ execution . id ] ) parents_step_ids = set () for current_event in events_for_execution : if current_event . type == current_event . INPUT : # this means the artifact is an input artifact events_for_input_artifact = [ e for e in self . store . get_events_by_artifact_ids ( [ current_event . artifact_id ] ) # should be output type and should NOT be the same id as # the execution we are querying and it should be BEFORE # the time of the current event. if e . type == e . OUTPUT and e . execution_id != current_event . execution_id and e . milliseconds_since_epoch < current_event . milliseconds_since_epoch ] # sort by time events_for_input_artifact . sort ( key = lambda x : x . milliseconds_since_epoch # type: ignore[no-any-return] # noqa ) # take the latest one and add execution to the parents. parents_step_ids . add ( events_for_input_artifact [ - 1 ] . execution_id ) return StepView ( id_ = execution . id , parents_step_ids = list ( parents_step_ids ), entrypoint_name = impl_name , name = step_name , parameters = step_parameters , metadata_store = self , ) def get_pipelines ( self ) -> List [ PipelineView ]: \"\"\"Returns a list of all pipelines stored in this metadata store.\"\"\" pipelines = [] for pipeline_context in self . store . get_contexts_by_type ( PIPELINE_CONTEXT_TYPE_NAME ): pipeline = PipelineView ( id_ = pipeline_context . id , name = pipeline_context . name , metadata_store = self , ) pipelines . append ( pipeline ) logger . debug ( \"Fetched %d pipelines.\" , len ( pipelines )) return pipelines def get_pipeline ( self , pipeline_name : str ) -> Optional [ PipelineView ]: \"\"\"Returns a pipeline for the given name.\"\"\" pipeline_context = self . store . get_context_by_type_and_name ( PIPELINE_CONTEXT_TYPE_NAME , pipeline_name ) if pipeline_context : logger . debug ( \"Fetched pipeline with name ' %s '\" , pipeline_name ) return PipelineView ( id_ = pipeline_context . id , name = pipeline_context . name , metadata_store = self , ) else : logger . info ( \"No pipelines found for name ' %s '\" , pipeline_name ) return None def get_pipeline_runs ( self , pipeline : PipelineView ) -> Dict [ str , PipelineRunView ]: \"\"\"Gets all runs for the given pipeline.\"\"\" all_pipeline_runs = self . store . get_contexts_by_type ( PIPELINE_RUN_CONTEXT_TYPE_NAME ) runs : Dict [ str , PipelineRunView ] = OrderedDict () for run in all_pipeline_runs : executions = self . store . get_executions_by_context ( run . id ) if self . _check_if_executions_belong_to_pipeline ( executions , pipeline ): run_view = PipelineRunView ( id_ = run . id , name = run . name , executions = executions , metadata_store = self , ) runs [ run . name ] = run_view logger . debug ( \"Fetched %d pipeline runs for pipeline named ' %s '.\" , len ( runs ), pipeline . name , ) return runs def get_pipeline_run ( self , pipeline : PipelineView , run_name : str ) -> Optional [ PipelineRunView ]: \"\"\"Gets a specific run for the given pipeline.\"\"\" run = self . store . get_context_by_type_and_name ( PIPELINE_RUN_CONTEXT_TYPE_NAME , run_name ) if not run : # No context found for the given run name return None executions = self . store . get_executions_by_context ( run . id ) if self . _check_if_executions_belong_to_pipeline ( executions , pipeline ): logger . debug ( \"Fetched pipeline run with name ' %s '\" , run_name ) return PipelineRunView ( id_ = run . id , name = run . name , executions = executions , metadata_store = self , ) logger . info ( \"No pipeline run found for name ' %s '\" , run_name ) return None def get_pipeline_run_steps ( self , pipeline_run : PipelineRunView ) -> Dict [ str , StepView ]: \"\"\"Gets all steps for the given pipeline run.\"\"\" steps : Dict [ str , StepView ] = OrderedDict () # reverse the executions as they get returned in reverse chronological # order from the metadata store for execution in reversed ( pipeline_run . _executions ): # noqa step = self . _get_step_view_from_execution ( execution ) steps [ step . name ] = step logger . debug ( \"Fetched %d steps for pipeline run ' %s '.\" , len ( steps ), pipeline_run . name , ) return steps def get_step_by_id ( self , step_id : int ) -> StepView : \"\"\"Gets a `StepView` by its ID\"\"\" execution = self . store . get_executions_by_id ([ step_id ])[ 0 ] return self . _get_step_view_from_execution ( execution ) def get_step_status ( self , step : StepView ) -> ExecutionStatus : \"\"\"Gets the execution status of a single step.\"\"\" proto = self . store . get_executions_by_id ([ step . _id ])[ 0 ] # noqa state = proto . last_known_state if state == proto . COMPLETE : return ExecutionStatus . COMPLETED elif state == proto . RUNNING : return ExecutionStatus . RUNNING elif state == proto . CACHED : return ExecutionStatus . CACHED else : return ExecutionStatus . FAILED def get_step_artifacts ( self , step : StepView ) -> Tuple [ Dict [ str , ArtifactView ], Dict [ str , ArtifactView ]]: \"\"\"Returns input and output artifacts for the given step. Args: step: The step for which to get the artifacts. Returns: A tuple (inputs, outputs) where inputs and outputs are both Dicts mapping artifact names to the input and output artifacts respectively. \"\"\" # maps artifact types to their string representation artifact_type_mapping = { type_ . id : type_ . name for type_ in self . store . get_artifact_types () } events = self . store . get_events_by_execution_ids ([ step . _id ]) # noqa artifacts = self . store . get_artifacts_by_id ( [ event . artifact_id for event in events ] ) inputs : Dict [ str , ArtifactView ] = {} outputs : Dict [ str , ArtifactView ] = {} # sort them according to artifact_id's so that the zip works. events . sort ( key = lambda x : x . artifact_id ) artifacts . sort ( key = lambda x : x . id ) for event_proto , artifact_proto in zip ( events , artifacts ): artifact_type = artifact_type_mapping [ artifact_proto . type_id ] artifact_name = event_proto . path . steps [ 0 ] . key materializer = artifact_proto . properties [ MATERIALIZER_PROPERTY_KEY ] . string_value data_type = artifact_proto . properties [ DATATYPE_PROPERTY_KEY ] . string_value parent_step_id = step . id if event_proto . type == event_proto . INPUT : # In the case that this is an input event, we actually need # to resolve it via its parents outputs. for parent in step . parent_steps : for a in parent . outputs . values (): if artifact_proto . id == a . id : parent_step_id = parent . id artifact = ArtifactView ( id_ = event_proto . artifact_id , type_ = artifact_type , uri = artifact_proto . uri , materializer = materializer , data_type = data_type , metadata_store = self , parent_step_id = parent_step_id , ) if event_proto . type == event_proto . INPUT : inputs [ artifact_name ] = artifact elif event_proto . type == event_proto . OUTPUT : outputs [ artifact_name ] = artifact logger . debug ( \"Fetched %d inputs and %d outputs for step ' %s '.\" , len ( inputs ), len ( outputs ), step . entrypoint_name , ) return inputs , outputs def get_producer_step_from_artifact ( self , artifact : ArtifactView ) -> StepView : \"\"\"Returns original StepView from an ArtifactView. Args: artifact: ArtifactView to be queried. Returns: Original StepView that produced the artifact. \"\"\" executions_ids = set ( event . execution_id for event in self . store . get_events_by_artifact_ids ([ artifact . id ]) if event . type == event . OUTPUT ) execution = self . store . get_executions_by_id ( executions_ids )[ 0 ] return self . _get_step_view_from_execution ( execution ) flavor : MetadataStoreFlavor property readonly The metadata store flavor. step_type_mapping : Dict [ int , str ] property readonly Maps type_id's to step names. store : MetadataStore property readonly General property that hooks into TFX metadata store. type : StackComponentType property readonly The component type. get_pipeline ( self , pipeline_name ) Returns a pipeline for the given name. Source code in zenml/metadata_stores/base_metadata_store.py def get_pipeline ( self , pipeline_name : str ) -> Optional [ PipelineView ]: \"\"\"Returns a pipeline for the given name.\"\"\" pipeline_context = self . store . get_context_by_type_and_name ( PIPELINE_CONTEXT_TYPE_NAME , pipeline_name ) if pipeline_context : logger . debug ( \"Fetched pipeline with name ' %s '\" , pipeline_name ) return PipelineView ( id_ = pipeline_context . id , name = pipeline_context . name , metadata_store = self , ) else : logger . info ( \"No pipelines found for name ' %s '\" , pipeline_name ) return None get_pipeline_run ( self , pipeline , run_name ) Gets a specific run for the given pipeline. Source code in zenml/metadata_stores/base_metadata_store.py def get_pipeline_run ( self , pipeline : PipelineView , run_name : str ) -> Optional [ PipelineRunView ]: \"\"\"Gets a specific run for the given pipeline.\"\"\" run = self . store . get_context_by_type_and_name ( PIPELINE_RUN_CONTEXT_TYPE_NAME , run_name ) if not run : # No context found for the given run name return None executions = self . store . get_executions_by_context ( run . id ) if self . _check_if_executions_belong_to_pipeline ( executions , pipeline ): logger . debug ( \"Fetched pipeline run with name ' %s '\" , run_name ) return PipelineRunView ( id_ = run . id , name = run . name , executions = executions , metadata_store = self , ) logger . info ( \"No pipeline run found for name ' %s '\" , run_name ) return None get_pipeline_run_steps ( self , pipeline_run ) Gets all steps for the given pipeline run. Source code in zenml/metadata_stores/base_metadata_store.py def get_pipeline_run_steps ( self , pipeline_run : PipelineRunView ) -> Dict [ str , StepView ]: \"\"\"Gets all steps for the given pipeline run.\"\"\" steps : Dict [ str , StepView ] = OrderedDict () # reverse the executions as they get returned in reverse chronological # order from the metadata store for execution in reversed ( pipeline_run . _executions ): # noqa step = self . _get_step_view_from_execution ( execution ) steps [ step . name ] = step logger . debug ( \"Fetched %d steps for pipeline run ' %s '.\" , len ( steps ), pipeline_run . name , ) return steps get_pipeline_runs ( self , pipeline ) Gets all runs for the given pipeline. Source code in zenml/metadata_stores/base_metadata_store.py def get_pipeline_runs ( self , pipeline : PipelineView ) -> Dict [ str , PipelineRunView ]: \"\"\"Gets all runs for the given pipeline.\"\"\" all_pipeline_runs = self . store . get_contexts_by_type ( PIPELINE_RUN_CONTEXT_TYPE_NAME ) runs : Dict [ str , PipelineRunView ] = OrderedDict () for run in all_pipeline_runs : executions = self . store . get_executions_by_context ( run . id ) if self . _check_if_executions_belong_to_pipeline ( executions , pipeline ): run_view = PipelineRunView ( id_ = run . id , name = run . name , executions = executions , metadata_store = self , ) runs [ run . name ] = run_view logger . debug ( \"Fetched %d pipeline runs for pipeline named ' %s '.\" , len ( runs ), pipeline . name , ) return runs get_pipelines ( self ) Returns a list of all pipelines stored in this metadata store. Source code in zenml/metadata_stores/base_metadata_store.py def get_pipelines ( self ) -> List [ PipelineView ]: \"\"\"Returns a list of all pipelines stored in this metadata store.\"\"\" pipelines = [] for pipeline_context in self . store . get_contexts_by_type ( PIPELINE_CONTEXT_TYPE_NAME ): pipeline = PipelineView ( id_ = pipeline_context . id , name = pipeline_context . name , metadata_store = self , ) pipelines . append ( pipeline ) logger . debug ( \"Fetched %d pipelines.\" , len ( pipelines )) return pipelines get_producer_step_from_artifact ( self , artifact ) Returns original StepView from an ArtifactView. Parameters: Name Type Description Default artifact ArtifactView ArtifactView to be queried. required Returns: Type Description StepView Original StepView that produced the artifact. Source code in zenml/metadata_stores/base_metadata_store.py def get_producer_step_from_artifact ( self , artifact : ArtifactView ) -> StepView : \"\"\"Returns original StepView from an ArtifactView. Args: artifact: ArtifactView to be queried. Returns: Original StepView that produced the artifact. \"\"\" executions_ids = set ( event . execution_id for event in self . store . get_events_by_artifact_ids ([ artifact . id ]) if event . type == event . OUTPUT ) execution = self . store . get_executions_by_id ( executions_ids )[ 0 ] return self . _get_step_view_from_execution ( execution ) get_step_artifacts ( self , step ) Returns input and output artifacts for the given step. Parameters: Name Type Description Default step StepView The step for which to get the artifacts. required Returns: Type Description Tuple[Dict[str, zenml.post_execution.artifact.ArtifactView], Dict[str, zenml.post_execution.artifact.ArtifactView]] A tuple (inputs, outputs) where inputs and outputs are both Dicts mapping artifact names to the input and output artifacts respectively. Source code in zenml/metadata_stores/base_metadata_store.py def get_step_artifacts ( self , step : StepView ) -> Tuple [ Dict [ str , ArtifactView ], Dict [ str , ArtifactView ]]: \"\"\"Returns input and output artifacts for the given step. Args: step: The step for which to get the artifacts. Returns: A tuple (inputs, outputs) where inputs and outputs are both Dicts mapping artifact names to the input and output artifacts respectively. \"\"\" # maps artifact types to their string representation artifact_type_mapping = { type_ . id : type_ . name for type_ in self . store . get_artifact_types () } events = self . store . get_events_by_execution_ids ([ step . _id ]) # noqa artifacts = self . store . get_artifacts_by_id ( [ event . artifact_id for event in events ] ) inputs : Dict [ str , ArtifactView ] = {} outputs : Dict [ str , ArtifactView ] = {} # sort them according to artifact_id's so that the zip works. events . sort ( key = lambda x : x . artifact_id ) artifacts . sort ( key = lambda x : x . id ) for event_proto , artifact_proto in zip ( events , artifacts ): artifact_type = artifact_type_mapping [ artifact_proto . type_id ] artifact_name = event_proto . path . steps [ 0 ] . key materializer = artifact_proto . properties [ MATERIALIZER_PROPERTY_KEY ] . string_value data_type = artifact_proto . properties [ DATATYPE_PROPERTY_KEY ] . string_value parent_step_id = step . id if event_proto . type == event_proto . INPUT : # In the case that this is an input event, we actually need # to resolve it via its parents outputs. for parent in step . parent_steps : for a in parent . outputs . values (): if artifact_proto . id == a . id : parent_step_id = parent . id artifact = ArtifactView ( id_ = event_proto . artifact_id , type_ = artifact_type , uri = artifact_proto . uri , materializer = materializer , data_type = data_type , metadata_store = self , parent_step_id = parent_step_id , ) if event_proto . type == event_proto . INPUT : inputs [ artifact_name ] = artifact elif event_proto . type == event_proto . OUTPUT : outputs [ artifact_name ] = artifact logger . debug ( \"Fetched %d inputs and %d outputs for step ' %s '.\" , len ( inputs ), len ( outputs ), step . entrypoint_name , ) return inputs , outputs get_step_by_id ( self , step_id ) Gets a StepView by its ID Source code in zenml/metadata_stores/base_metadata_store.py def get_step_by_id ( self , step_id : int ) -> StepView : \"\"\"Gets a `StepView` by its ID\"\"\" execution = self . store . get_executions_by_id ([ step_id ])[ 0 ] return self . _get_step_view_from_execution ( execution ) get_step_status ( self , step ) Gets the execution status of a single step. Source code in zenml/metadata_stores/base_metadata_store.py def get_step_status ( self , step : StepView ) -> ExecutionStatus : \"\"\"Gets the execution status of a single step.\"\"\" proto = self . store . get_executions_by_id ([ step . _id ])[ 0 ] # noqa state = proto . last_known_state if state == proto . COMPLETE : return ExecutionStatus . COMPLETED elif state == proto . RUNNING : return ExecutionStatus . RUNNING elif state == proto . CACHED : return ExecutionStatus . CACHED else : return ExecutionStatus . FAILED get_tfx_metadata_config ( self ) Return tfx metadata config. Source code in zenml/metadata_stores/base_metadata_store.py @abstractmethod def get_tfx_metadata_config ( self , ) -> Union [ metadata_store_pb2 . ConnectionConfig , metadata_store_pb2 . MetadataStoreClientConfig , ]: \"\"\"Return tfx metadata config.\"\"\" raise NotImplementedError mysql_metadata_store MySQLMetadataStore ( BaseMetadataStore ) pydantic-model MySQL backend for ZenML metadata store. Source code in zenml/metadata_stores/mysql_metadata_store.py class MySQLMetadataStore ( BaseMetadataStore ): \"\"\"MySQL backend for ZenML metadata store.\"\"\" host : str port : int database : str username : str password : str supports_local_execution = True supports_remote_execution = True @property def flavor ( self ) -> MetadataStoreFlavor : \"\"\"The metadata store flavor.\"\"\" return MetadataStoreFlavor . MYSQL def get_tfx_metadata_config ( self , ) -> Union [ metadata_store_pb2 . ConnectionConfig , metadata_store_pb2 . MetadataStoreClientConfig , ]: \"\"\"Return tfx metadata config for mysql metadata store.\"\"\" return metadata . mysql_metadata_connection_config ( host = self . host , port = self . port , database = self . database , username = self . username , password = self . password , ) flavor : MetadataStoreFlavor property readonly The metadata store flavor. get_tfx_metadata_config ( self ) Return tfx metadata config for mysql metadata store. Source code in zenml/metadata_stores/mysql_metadata_store.py def get_tfx_metadata_config ( self , ) -> Union [ metadata_store_pb2 . ConnectionConfig , metadata_store_pb2 . MetadataStoreClientConfig , ]: \"\"\"Return tfx metadata config for mysql metadata store.\"\"\" return metadata . mysql_metadata_connection_config ( host = self . host , port = self . port , database = self . database , username = self . username , password = self . password , ) sqlite_metadata_store SQLiteMetadataStore ( BaseMetadataStore ) pydantic-model SQLite backend for ZenML metadata store. Source code in zenml/metadata_stores/sqlite_metadata_store.py class SQLiteMetadataStore ( BaseMetadataStore ): \"\"\"SQLite backend for ZenML metadata store.\"\"\" uri : str supports_local_execution = True supports_remote_execution = False @property def flavor ( self ) -> MetadataStoreFlavor : \"\"\"The metadata store flavor.\"\"\" return MetadataStoreFlavor . SQLITE def get_tfx_metadata_config ( self , ) -> Union [ metadata_store_pb2 . ConnectionConfig , metadata_store_pb2 . MetadataStoreClientConfig , ]: \"\"\"Return tfx metadata config for sqlite metadata store.\"\"\" return metadata . sqlite_metadata_connection_config ( self . uri ) @validator ( \"uri\" ) def ensure_uri_is_local ( cls , uri : str ) -> str : \"\"\"Ensures that the metadata store uri is local.\"\"\" if fileio . is_remote ( uri ): raise ValueError ( f \"Uri ' { uri } ' specified for SQLiteMetadataStore is not a \" f \"local uri.\" ) return uri flavor : MetadataStoreFlavor property readonly The metadata store flavor. ensure_uri_is_local ( uri ) classmethod Ensures that the metadata store uri is local. Source code in zenml/metadata_stores/sqlite_metadata_store.py @validator ( \"uri\" ) def ensure_uri_is_local ( cls , uri : str ) -> str : \"\"\"Ensures that the metadata store uri is local.\"\"\" if fileio . is_remote ( uri ): raise ValueError ( f \"Uri ' { uri } ' specified for SQLiteMetadataStore is not a \" f \"local uri.\" ) return uri get_tfx_metadata_config ( self ) Return tfx metadata config for sqlite metadata store. Source code in zenml/metadata_stores/sqlite_metadata_store.py def get_tfx_metadata_config ( self , ) -> Union [ metadata_store_pb2 . ConnectionConfig , metadata_store_pb2 . MetadataStoreClientConfig , ]: \"\"\"Return tfx metadata config for sqlite metadata store.\"\"\" return metadata . sqlite_metadata_connection_config ( self . uri )","title":"Metadata Stores"},{"location":"api_docs/metadata_stores/#metadata-stores","text":"","title":"Metadata Stores"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores","text":"The configuration of each pipeline, step, backend, and produced artifacts are all tracked within the metadata store. The metadata store is an SQL database, and can be sqlite or mysql . Metadata are the pieces of information tracked about the pipelines, experiments and configurations that you are running with ZenML. Metadata are stored inside the metadata store.","title":"metadata_stores"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store","text":"","title":"base_metadata_store"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore","text":"Base class for all ZenML metadata stores. Source code in zenml/metadata_stores/base_metadata_store.py class BaseMetadataStore ( StackComponent , ABC ): \"\"\"Base class for all ZenML metadata stores.\"\"\" @property def type ( self ) -> StackComponentType : \"\"\"The component type.\"\"\" return StackComponentType . METADATA_STORE @property @abstractmethod def flavor ( self ) -> MetadataStoreFlavor : \"\"\"The metadata store flavor.\"\"\" @property def store ( self ) -> metadata_store . MetadataStore : \"\"\"General property that hooks into TFX metadata store.\"\"\" # TODO [ENG-133]: this always gets recreated, is this intended? return metadata_store . MetadataStore ( self . get_tfx_metadata_config (), enable_upgrade_migration = True ) @abstractmethod def get_tfx_metadata_config ( self , ) -> Union [ metadata_store_pb2 . ConnectionConfig , metadata_store_pb2 . MetadataStoreClientConfig , ]: \"\"\"Return tfx metadata config.\"\"\" raise NotImplementedError @property def step_type_mapping ( self ) -> Dict [ int , str ]: \"\"\"Maps type_id's to step names.\"\"\" return { type_ . id : type_ . name for type_ in self . store . get_execution_types () } def _check_if_executions_belong_to_pipeline ( self , executions : List [ proto . Execution ], pipeline : PipelineView , ) -> bool : \"\"\"Returns `True` if the executions are associated with the pipeline context.\"\"\" for execution in executions : associated_contexts = self . store . get_contexts_by_execution ( execution . id ) for context in associated_contexts : if context . id == pipeline . _id : # noqa return True return False def _get_step_view_from_execution ( self , execution : proto . Execution ) -> StepView : \"\"\"Get original StepView from an execution. Args: execution: proto.Execution object from mlmd store. Returns: Original `StepView` derived from the proto.Execution. \"\"\" impl_name = self . step_type_mapping [ execution . type_id ] . split ( \".\" )[ - 1 ] step_name_property = execution . custom_properties . get ( INTERNAL_EXECUTION_PARAMETER_PREFIX + PARAM_PIPELINE_PARAMETER_NAME , None , ) if step_name_property : step_name = json . loads ( step_name_property . string_value ) else : raise KeyError ( f \"Step name missing for execution with ID { execution . id } . \" f \"This error probably occurs because you're using ZenML \" f \"version 0.5.4 or newer but your metadata store contains \" f \"data from previous versions.\" ) step_parameters = {} for k , v in execution . custom_properties . items (): if not k . startswith ( INTERNAL_EXECUTION_PARAMETER_PREFIX ): try : step_parameters [ k ] = json . loads ( v . string_value ) except JSONDecodeError : # this means there is a property in there that is neither # an internal one or one created by zenml. Therefore, we can # ignore it pass # TODO [ENG-222]: This is a lot of querying to the metadata store. We # should refactor and make it nicer. Probably it makes more sense # to first get `executions_ids_for_current_run` and then filter on # `event.execution_id in execution_ids_for_current_run`. # Core logic here is that we get the event of this particular execution # id that gives us the artifacts of this execution. We then go through # all `input` artifacts of this execution and get all events related to # that artifact. This in turn gives us other events for which this # artifact was an `output` artifact. Then we simply need to sort by # time to get the most recent execution (i.e. step) that produced that # particular artifact. events_for_execution = self . store . get_events_by_execution_ids ( [ execution . id ] ) parents_step_ids = set () for current_event in events_for_execution : if current_event . type == current_event . INPUT : # this means the artifact is an input artifact events_for_input_artifact = [ e for e in self . store . get_events_by_artifact_ids ( [ current_event . artifact_id ] ) # should be output type and should NOT be the same id as # the execution we are querying and it should be BEFORE # the time of the current event. if e . type == e . OUTPUT and e . execution_id != current_event . execution_id and e . milliseconds_since_epoch < current_event . milliseconds_since_epoch ] # sort by time events_for_input_artifact . sort ( key = lambda x : x . milliseconds_since_epoch # type: ignore[no-any-return] # noqa ) # take the latest one and add execution to the parents. parents_step_ids . add ( events_for_input_artifact [ - 1 ] . execution_id ) return StepView ( id_ = execution . id , parents_step_ids = list ( parents_step_ids ), entrypoint_name = impl_name , name = step_name , parameters = step_parameters , metadata_store = self , ) def get_pipelines ( self ) -> List [ PipelineView ]: \"\"\"Returns a list of all pipelines stored in this metadata store.\"\"\" pipelines = [] for pipeline_context in self . store . get_contexts_by_type ( PIPELINE_CONTEXT_TYPE_NAME ): pipeline = PipelineView ( id_ = pipeline_context . id , name = pipeline_context . name , metadata_store = self , ) pipelines . append ( pipeline ) logger . debug ( \"Fetched %d pipelines.\" , len ( pipelines )) return pipelines def get_pipeline ( self , pipeline_name : str ) -> Optional [ PipelineView ]: \"\"\"Returns a pipeline for the given name.\"\"\" pipeline_context = self . store . get_context_by_type_and_name ( PIPELINE_CONTEXT_TYPE_NAME , pipeline_name ) if pipeline_context : logger . debug ( \"Fetched pipeline with name ' %s '\" , pipeline_name ) return PipelineView ( id_ = pipeline_context . id , name = pipeline_context . name , metadata_store = self , ) else : logger . info ( \"No pipelines found for name ' %s '\" , pipeline_name ) return None def get_pipeline_runs ( self , pipeline : PipelineView ) -> Dict [ str , PipelineRunView ]: \"\"\"Gets all runs for the given pipeline.\"\"\" all_pipeline_runs = self . store . get_contexts_by_type ( PIPELINE_RUN_CONTEXT_TYPE_NAME ) runs : Dict [ str , PipelineRunView ] = OrderedDict () for run in all_pipeline_runs : executions = self . store . get_executions_by_context ( run . id ) if self . _check_if_executions_belong_to_pipeline ( executions , pipeline ): run_view = PipelineRunView ( id_ = run . id , name = run . name , executions = executions , metadata_store = self , ) runs [ run . name ] = run_view logger . debug ( \"Fetched %d pipeline runs for pipeline named ' %s '.\" , len ( runs ), pipeline . name , ) return runs def get_pipeline_run ( self , pipeline : PipelineView , run_name : str ) -> Optional [ PipelineRunView ]: \"\"\"Gets a specific run for the given pipeline.\"\"\" run = self . store . get_context_by_type_and_name ( PIPELINE_RUN_CONTEXT_TYPE_NAME , run_name ) if not run : # No context found for the given run name return None executions = self . store . get_executions_by_context ( run . id ) if self . _check_if_executions_belong_to_pipeline ( executions , pipeline ): logger . debug ( \"Fetched pipeline run with name ' %s '\" , run_name ) return PipelineRunView ( id_ = run . id , name = run . name , executions = executions , metadata_store = self , ) logger . info ( \"No pipeline run found for name ' %s '\" , run_name ) return None def get_pipeline_run_steps ( self , pipeline_run : PipelineRunView ) -> Dict [ str , StepView ]: \"\"\"Gets all steps for the given pipeline run.\"\"\" steps : Dict [ str , StepView ] = OrderedDict () # reverse the executions as they get returned in reverse chronological # order from the metadata store for execution in reversed ( pipeline_run . _executions ): # noqa step = self . _get_step_view_from_execution ( execution ) steps [ step . name ] = step logger . debug ( \"Fetched %d steps for pipeline run ' %s '.\" , len ( steps ), pipeline_run . name , ) return steps def get_step_by_id ( self , step_id : int ) -> StepView : \"\"\"Gets a `StepView` by its ID\"\"\" execution = self . store . get_executions_by_id ([ step_id ])[ 0 ] return self . _get_step_view_from_execution ( execution ) def get_step_status ( self , step : StepView ) -> ExecutionStatus : \"\"\"Gets the execution status of a single step.\"\"\" proto = self . store . get_executions_by_id ([ step . _id ])[ 0 ] # noqa state = proto . last_known_state if state == proto . COMPLETE : return ExecutionStatus . COMPLETED elif state == proto . RUNNING : return ExecutionStatus . RUNNING elif state == proto . CACHED : return ExecutionStatus . CACHED else : return ExecutionStatus . FAILED def get_step_artifacts ( self , step : StepView ) -> Tuple [ Dict [ str , ArtifactView ], Dict [ str , ArtifactView ]]: \"\"\"Returns input and output artifacts for the given step. Args: step: The step for which to get the artifacts. Returns: A tuple (inputs, outputs) where inputs and outputs are both Dicts mapping artifact names to the input and output artifacts respectively. \"\"\" # maps artifact types to their string representation artifact_type_mapping = { type_ . id : type_ . name for type_ in self . store . get_artifact_types () } events = self . store . get_events_by_execution_ids ([ step . _id ]) # noqa artifacts = self . store . get_artifacts_by_id ( [ event . artifact_id for event in events ] ) inputs : Dict [ str , ArtifactView ] = {} outputs : Dict [ str , ArtifactView ] = {} # sort them according to artifact_id's so that the zip works. events . sort ( key = lambda x : x . artifact_id ) artifacts . sort ( key = lambda x : x . id ) for event_proto , artifact_proto in zip ( events , artifacts ): artifact_type = artifact_type_mapping [ artifact_proto . type_id ] artifact_name = event_proto . path . steps [ 0 ] . key materializer = artifact_proto . properties [ MATERIALIZER_PROPERTY_KEY ] . string_value data_type = artifact_proto . properties [ DATATYPE_PROPERTY_KEY ] . string_value parent_step_id = step . id if event_proto . type == event_proto . INPUT : # In the case that this is an input event, we actually need # to resolve it via its parents outputs. for parent in step . parent_steps : for a in parent . outputs . values (): if artifact_proto . id == a . id : parent_step_id = parent . id artifact = ArtifactView ( id_ = event_proto . artifact_id , type_ = artifact_type , uri = artifact_proto . uri , materializer = materializer , data_type = data_type , metadata_store = self , parent_step_id = parent_step_id , ) if event_proto . type == event_proto . INPUT : inputs [ artifact_name ] = artifact elif event_proto . type == event_proto . OUTPUT : outputs [ artifact_name ] = artifact logger . debug ( \"Fetched %d inputs and %d outputs for step ' %s '.\" , len ( inputs ), len ( outputs ), step . entrypoint_name , ) return inputs , outputs def get_producer_step_from_artifact ( self , artifact : ArtifactView ) -> StepView : \"\"\"Returns original StepView from an ArtifactView. Args: artifact: ArtifactView to be queried. Returns: Original StepView that produced the artifact. \"\"\" executions_ids = set ( event . execution_id for event in self . store . get_events_by_artifact_ids ([ artifact . id ]) if event . type == event . OUTPUT ) execution = self . store . get_executions_by_id ( executions_ids )[ 0 ] return self . _get_step_view_from_execution ( execution )","title":"BaseMetadataStore"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.flavor","text":"The metadata store flavor.","title":"flavor"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.step_type_mapping","text":"Maps type_id's to step names.","title":"step_type_mapping"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.store","text":"General property that hooks into TFX metadata store.","title":"store"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.type","text":"The component type.","title":"type"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.get_pipeline","text":"Returns a pipeline for the given name. Source code in zenml/metadata_stores/base_metadata_store.py def get_pipeline ( self , pipeline_name : str ) -> Optional [ PipelineView ]: \"\"\"Returns a pipeline for the given name.\"\"\" pipeline_context = self . store . get_context_by_type_and_name ( PIPELINE_CONTEXT_TYPE_NAME , pipeline_name ) if pipeline_context : logger . debug ( \"Fetched pipeline with name ' %s '\" , pipeline_name ) return PipelineView ( id_ = pipeline_context . id , name = pipeline_context . name , metadata_store = self , ) else : logger . info ( \"No pipelines found for name ' %s '\" , pipeline_name ) return None","title":"get_pipeline()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.get_pipeline_run","text":"Gets a specific run for the given pipeline. Source code in zenml/metadata_stores/base_metadata_store.py def get_pipeline_run ( self , pipeline : PipelineView , run_name : str ) -> Optional [ PipelineRunView ]: \"\"\"Gets a specific run for the given pipeline.\"\"\" run = self . store . get_context_by_type_and_name ( PIPELINE_RUN_CONTEXT_TYPE_NAME , run_name ) if not run : # No context found for the given run name return None executions = self . store . get_executions_by_context ( run . id ) if self . _check_if_executions_belong_to_pipeline ( executions , pipeline ): logger . debug ( \"Fetched pipeline run with name ' %s '\" , run_name ) return PipelineRunView ( id_ = run . id , name = run . name , executions = executions , metadata_store = self , ) logger . info ( \"No pipeline run found for name ' %s '\" , run_name ) return None","title":"get_pipeline_run()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.get_pipeline_run_steps","text":"Gets all steps for the given pipeline run. Source code in zenml/metadata_stores/base_metadata_store.py def get_pipeline_run_steps ( self , pipeline_run : PipelineRunView ) -> Dict [ str , StepView ]: \"\"\"Gets all steps for the given pipeline run.\"\"\" steps : Dict [ str , StepView ] = OrderedDict () # reverse the executions as they get returned in reverse chronological # order from the metadata store for execution in reversed ( pipeline_run . _executions ): # noqa step = self . _get_step_view_from_execution ( execution ) steps [ step . name ] = step logger . debug ( \"Fetched %d steps for pipeline run ' %s '.\" , len ( steps ), pipeline_run . name , ) return steps","title":"get_pipeline_run_steps()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.get_pipeline_runs","text":"Gets all runs for the given pipeline. Source code in zenml/metadata_stores/base_metadata_store.py def get_pipeline_runs ( self , pipeline : PipelineView ) -> Dict [ str , PipelineRunView ]: \"\"\"Gets all runs for the given pipeline.\"\"\" all_pipeline_runs = self . store . get_contexts_by_type ( PIPELINE_RUN_CONTEXT_TYPE_NAME ) runs : Dict [ str , PipelineRunView ] = OrderedDict () for run in all_pipeline_runs : executions = self . store . get_executions_by_context ( run . id ) if self . _check_if_executions_belong_to_pipeline ( executions , pipeline ): run_view = PipelineRunView ( id_ = run . id , name = run . name , executions = executions , metadata_store = self , ) runs [ run . name ] = run_view logger . debug ( \"Fetched %d pipeline runs for pipeline named ' %s '.\" , len ( runs ), pipeline . name , ) return runs","title":"get_pipeline_runs()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.get_pipelines","text":"Returns a list of all pipelines stored in this metadata store. Source code in zenml/metadata_stores/base_metadata_store.py def get_pipelines ( self ) -> List [ PipelineView ]: \"\"\"Returns a list of all pipelines stored in this metadata store.\"\"\" pipelines = [] for pipeline_context in self . store . get_contexts_by_type ( PIPELINE_CONTEXT_TYPE_NAME ): pipeline = PipelineView ( id_ = pipeline_context . id , name = pipeline_context . name , metadata_store = self , ) pipelines . append ( pipeline ) logger . debug ( \"Fetched %d pipelines.\" , len ( pipelines )) return pipelines","title":"get_pipelines()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.get_producer_step_from_artifact","text":"Returns original StepView from an ArtifactView. Parameters: Name Type Description Default artifact ArtifactView ArtifactView to be queried. required Returns: Type Description StepView Original StepView that produced the artifact. Source code in zenml/metadata_stores/base_metadata_store.py def get_producer_step_from_artifact ( self , artifact : ArtifactView ) -> StepView : \"\"\"Returns original StepView from an ArtifactView. Args: artifact: ArtifactView to be queried. Returns: Original StepView that produced the artifact. \"\"\" executions_ids = set ( event . execution_id for event in self . store . get_events_by_artifact_ids ([ artifact . id ]) if event . type == event . OUTPUT ) execution = self . store . get_executions_by_id ( executions_ids )[ 0 ] return self . _get_step_view_from_execution ( execution )","title":"get_producer_step_from_artifact()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.get_step_artifacts","text":"Returns input and output artifacts for the given step. Parameters: Name Type Description Default step StepView The step for which to get the artifacts. required Returns: Type Description Tuple[Dict[str, zenml.post_execution.artifact.ArtifactView], Dict[str, zenml.post_execution.artifact.ArtifactView]] A tuple (inputs, outputs) where inputs and outputs are both Dicts mapping artifact names to the input and output artifacts respectively. Source code in zenml/metadata_stores/base_metadata_store.py def get_step_artifacts ( self , step : StepView ) -> Tuple [ Dict [ str , ArtifactView ], Dict [ str , ArtifactView ]]: \"\"\"Returns input and output artifacts for the given step. Args: step: The step for which to get the artifacts. Returns: A tuple (inputs, outputs) where inputs and outputs are both Dicts mapping artifact names to the input and output artifacts respectively. \"\"\" # maps artifact types to their string representation artifact_type_mapping = { type_ . id : type_ . name for type_ in self . store . get_artifact_types () } events = self . store . get_events_by_execution_ids ([ step . _id ]) # noqa artifacts = self . store . get_artifacts_by_id ( [ event . artifact_id for event in events ] ) inputs : Dict [ str , ArtifactView ] = {} outputs : Dict [ str , ArtifactView ] = {} # sort them according to artifact_id's so that the zip works. events . sort ( key = lambda x : x . artifact_id ) artifacts . sort ( key = lambda x : x . id ) for event_proto , artifact_proto in zip ( events , artifacts ): artifact_type = artifact_type_mapping [ artifact_proto . type_id ] artifact_name = event_proto . path . steps [ 0 ] . key materializer = artifact_proto . properties [ MATERIALIZER_PROPERTY_KEY ] . string_value data_type = artifact_proto . properties [ DATATYPE_PROPERTY_KEY ] . string_value parent_step_id = step . id if event_proto . type == event_proto . INPUT : # In the case that this is an input event, we actually need # to resolve it via its parents outputs. for parent in step . parent_steps : for a in parent . outputs . values (): if artifact_proto . id == a . id : parent_step_id = parent . id artifact = ArtifactView ( id_ = event_proto . artifact_id , type_ = artifact_type , uri = artifact_proto . uri , materializer = materializer , data_type = data_type , metadata_store = self , parent_step_id = parent_step_id , ) if event_proto . type == event_proto . INPUT : inputs [ artifact_name ] = artifact elif event_proto . type == event_proto . OUTPUT : outputs [ artifact_name ] = artifact logger . debug ( \"Fetched %d inputs and %d outputs for step ' %s '.\" , len ( inputs ), len ( outputs ), step . entrypoint_name , ) return inputs , outputs","title":"get_step_artifacts()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.get_step_by_id","text":"Gets a StepView by its ID Source code in zenml/metadata_stores/base_metadata_store.py def get_step_by_id ( self , step_id : int ) -> StepView : \"\"\"Gets a `StepView` by its ID\"\"\" execution = self . store . get_executions_by_id ([ step_id ])[ 0 ] return self . _get_step_view_from_execution ( execution )","title":"get_step_by_id()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.get_step_status","text":"Gets the execution status of a single step. Source code in zenml/metadata_stores/base_metadata_store.py def get_step_status ( self , step : StepView ) -> ExecutionStatus : \"\"\"Gets the execution status of a single step.\"\"\" proto = self . store . get_executions_by_id ([ step . _id ])[ 0 ] # noqa state = proto . last_known_state if state == proto . COMPLETE : return ExecutionStatus . COMPLETED elif state == proto . RUNNING : return ExecutionStatus . RUNNING elif state == proto . CACHED : return ExecutionStatus . CACHED else : return ExecutionStatus . FAILED","title":"get_step_status()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.get_tfx_metadata_config","text":"Return tfx metadata config. Source code in zenml/metadata_stores/base_metadata_store.py @abstractmethod def get_tfx_metadata_config ( self , ) -> Union [ metadata_store_pb2 . ConnectionConfig , metadata_store_pb2 . MetadataStoreClientConfig , ]: \"\"\"Return tfx metadata config.\"\"\" raise NotImplementedError","title":"get_tfx_metadata_config()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.mysql_metadata_store","text":"","title":"mysql_metadata_store"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.mysql_metadata_store.MySQLMetadataStore","text":"MySQL backend for ZenML metadata store. Source code in zenml/metadata_stores/mysql_metadata_store.py class MySQLMetadataStore ( BaseMetadataStore ): \"\"\"MySQL backend for ZenML metadata store.\"\"\" host : str port : int database : str username : str password : str supports_local_execution = True supports_remote_execution = True @property def flavor ( self ) -> MetadataStoreFlavor : \"\"\"The metadata store flavor.\"\"\" return MetadataStoreFlavor . MYSQL def get_tfx_metadata_config ( self , ) -> Union [ metadata_store_pb2 . ConnectionConfig , metadata_store_pb2 . MetadataStoreClientConfig , ]: \"\"\"Return tfx metadata config for mysql metadata store.\"\"\" return metadata . mysql_metadata_connection_config ( host = self . host , port = self . port , database = self . database , username = self . username , password = self . password , )","title":"MySQLMetadataStore"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.mysql_metadata_store.MySQLMetadataStore.flavor","text":"The metadata store flavor.","title":"flavor"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.mysql_metadata_store.MySQLMetadataStore.get_tfx_metadata_config","text":"Return tfx metadata config for mysql metadata store. Source code in zenml/metadata_stores/mysql_metadata_store.py def get_tfx_metadata_config ( self , ) -> Union [ metadata_store_pb2 . ConnectionConfig , metadata_store_pb2 . MetadataStoreClientConfig , ]: \"\"\"Return tfx metadata config for mysql metadata store.\"\"\" return metadata . mysql_metadata_connection_config ( host = self . host , port = self . port , database = self . database , username = self . username , password = self . password , )","title":"get_tfx_metadata_config()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.sqlite_metadata_store","text":"","title":"sqlite_metadata_store"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.sqlite_metadata_store.SQLiteMetadataStore","text":"SQLite backend for ZenML metadata store. Source code in zenml/metadata_stores/sqlite_metadata_store.py class SQLiteMetadataStore ( BaseMetadataStore ): \"\"\"SQLite backend for ZenML metadata store.\"\"\" uri : str supports_local_execution = True supports_remote_execution = False @property def flavor ( self ) -> MetadataStoreFlavor : \"\"\"The metadata store flavor.\"\"\" return MetadataStoreFlavor . SQLITE def get_tfx_metadata_config ( self , ) -> Union [ metadata_store_pb2 . ConnectionConfig , metadata_store_pb2 . MetadataStoreClientConfig , ]: \"\"\"Return tfx metadata config for sqlite metadata store.\"\"\" return metadata . sqlite_metadata_connection_config ( self . uri ) @validator ( \"uri\" ) def ensure_uri_is_local ( cls , uri : str ) -> str : \"\"\"Ensures that the metadata store uri is local.\"\"\" if fileio . is_remote ( uri ): raise ValueError ( f \"Uri ' { uri } ' specified for SQLiteMetadataStore is not a \" f \"local uri.\" ) return uri","title":"SQLiteMetadataStore"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.sqlite_metadata_store.SQLiteMetadataStore.flavor","text":"The metadata store flavor.","title":"flavor"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.sqlite_metadata_store.SQLiteMetadataStore.ensure_uri_is_local","text":"Ensures that the metadata store uri is local. Source code in zenml/metadata_stores/sqlite_metadata_store.py @validator ( \"uri\" ) def ensure_uri_is_local ( cls , uri : str ) -> str : \"\"\"Ensures that the metadata store uri is local.\"\"\" if fileio . is_remote ( uri ): raise ValueError ( f \"Uri ' { uri } ' specified for SQLiteMetadataStore is not a \" f \"local uri.\" ) return uri","title":"ensure_uri_is_local()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.sqlite_metadata_store.SQLiteMetadataStore.get_tfx_metadata_config","text":"Return tfx metadata config for sqlite metadata store. Source code in zenml/metadata_stores/sqlite_metadata_store.py def get_tfx_metadata_config ( self , ) -> Union [ metadata_store_pb2 . ConnectionConfig , metadata_store_pb2 . MetadataStoreClientConfig , ]: \"\"\"Return tfx metadata config for sqlite metadata store.\"\"\" return metadata . sqlite_metadata_connection_config ( self . uri )","title":"get_tfx_metadata_config()"},{"location":"api_docs/orchestrators/","text":"Orchestrators zenml.orchestrators special An orchestrator is a special kind of backend that manages the running of each step of the pipeline. Orchestrators administer the actual pipeline runs. You can think of it as the 'root' of any pipeline job that you run during your experimentation. ZenML supports a local orchestrator out of the box which allows you to run your pipelines in a local environment. We also support using Apache Airflow as the orchestrator to handle the steps of your pipeline. base_orchestrator BaseOrchestrator ( StackComponent , ABC ) pydantic-model Base class for all ZenML orchestrators. Source code in zenml/orchestrators/base_orchestrator.py class BaseOrchestrator ( StackComponent , ABC ): \"\"\"Base class for all ZenML orchestrators.\"\"\" @property def type ( self ) -> StackComponentType : \"\"\"The component type.\"\"\" return StackComponentType . ORCHESTRATOR @property @abstractmethod def flavor ( self ) -> OrchestratorFlavor : \"\"\"The orchestrator flavor.\"\"\" @abstractmethod def run_pipeline ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , run_name : str ) -> Any : \"\"\"Runs a pipeline. Args: pipeline: The pipeline to run. stack: The stack on which the pipeline is run. run_name: Name of the pipeline run. \"\"\" flavor : OrchestratorFlavor property readonly The orchestrator flavor. type : StackComponentType property readonly The component type. run_pipeline ( self , pipeline , stack , run_name ) Runs a pipeline. Parameters: Name Type Description Default pipeline BasePipeline The pipeline to run. required stack Stack The stack on which the pipeline is run. required run_name str Name of the pipeline run. required Source code in zenml/orchestrators/base_orchestrator.py @abstractmethod def run_pipeline ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , run_name : str ) -> Any : \"\"\"Runs a pipeline. Args: pipeline: The pipeline to run. stack: The stack on which the pipeline is run. run_name: Name of the pipeline run. \"\"\" local special local_dag_runner Inspired by local dag runner implementation by Google at: https://github.com/tensorflow/tfx/blob/master/tfx/orchestration /local/local_dag_runner.py LocalDagRunner ( TfxRunner ) Local TFX DAG runner. Source code in zenml/orchestrators/local/local_dag_runner.py class LocalDagRunner ( tfx_runner . TfxRunner ): \"\"\"Local TFX DAG runner.\"\"\" def __init__ ( self ) -> None : \"\"\"Initializes LocalDagRunner as a TFX orchestrator.\"\"\" def run ( self , pipeline : tfx_pipeline . Pipeline , run_name : str = \"\" ) -> None : \"\"\"Runs given logical pipeline locally. Args: pipeline: Logical pipeline containing pipeline args and components. run_name: Name of the pipeline run. \"\"\" for component in pipeline . components : if isinstance ( component , base_component . BaseComponent ): component . _resolve_pip_dependencies ( pipeline . pipeline_info . pipeline_root ) c = compiler . Compiler () pipeline = c . compile ( pipeline ) # Substitute the runtime parameter to be a concrete run_id runtime_parameter_utils . substitute_runtime_parameter ( pipeline , { PIPELINE_RUN_ID_PARAMETER_NAME : run_name , }, ) deployment_config = runner_utils . extract_local_deployment_config ( pipeline ) connection_config = ( Repository () . active_stack . metadata_store . get_tfx_metadata_config () ) logger . debug ( f \"Using deployment config: \\n { deployment_config } \" ) logger . debug ( f \"Using connection config: \\n { connection_config } \" ) # Run each component. Note that the pipeline.components list is in # topological order. for node in pipeline . nodes : pipeline_node = node . pipeline_node node_id = pipeline_node . node_info . id executor_spec = runner_utils . extract_executor_spec ( deployment_config , node_id ) custom_driver_spec = runner_utils . extract_custom_driver_spec ( deployment_config , node_id ) component_launcher = launcher . Launcher ( pipeline_node = pipeline_node , mlmd_connection = metadata . Metadata ( connection_config ), pipeline_info = pipeline . pipeline_info , pipeline_runtime_spec = pipeline . runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) execute_step ( component_launcher ) __init__ ( self ) special Initializes LocalDagRunner as a TFX orchestrator. Source code in zenml/orchestrators/local/local_dag_runner.py def __init__ ( self ) -> None : \"\"\"Initializes LocalDagRunner as a TFX orchestrator.\"\"\" run ( self , pipeline , run_name = '' ) Runs given logical pipeline locally. Parameters: Name Type Description Default pipeline Pipeline Logical pipeline containing pipeline args and components. required run_name str Name of the pipeline run. '' Source code in zenml/orchestrators/local/local_dag_runner.py def run ( self , pipeline : tfx_pipeline . Pipeline , run_name : str = \"\" ) -> None : \"\"\"Runs given logical pipeline locally. Args: pipeline: Logical pipeline containing pipeline args and components. run_name: Name of the pipeline run. \"\"\" for component in pipeline . components : if isinstance ( component , base_component . BaseComponent ): component . _resolve_pip_dependencies ( pipeline . pipeline_info . pipeline_root ) c = compiler . Compiler () pipeline = c . compile ( pipeline ) # Substitute the runtime parameter to be a concrete run_id runtime_parameter_utils . substitute_runtime_parameter ( pipeline , { PIPELINE_RUN_ID_PARAMETER_NAME : run_name , }, ) deployment_config = runner_utils . extract_local_deployment_config ( pipeline ) connection_config = ( Repository () . active_stack . metadata_store . get_tfx_metadata_config () ) logger . debug ( f \"Using deployment config: \\n { deployment_config } \" ) logger . debug ( f \"Using connection config: \\n { connection_config } \" ) # Run each component. Note that the pipeline.components list is in # topological order. for node in pipeline . nodes : pipeline_node = node . pipeline_node node_id = pipeline_node . node_info . id executor_spec = runner_utils . extract_executor_spec ( deployment_config , node_id ) custom_driver_spec = runner_utils . extract_custom_driver_spec ( deployment_config , node_id ) component_launcher = launcher . Launcher ( pipeline_node = pipeline_node , mlmd_connection = metadata . Metadata ( connection_config ), pipeline_info = pipeline . pipeline_info , pipeline_runtime_spec = pipeline . runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) execute_step ( component_launcher ) local_orchestrator LocalOrchestrator ( BaseOrchestrator ) pydantic-model Orchestrator responsible for running pipelines locally. Source code in zenml/orchestrators/local/local_orchestrator.py class LocalOrchestrator ( BaseOrchestrator ): \"\"\"Orchestrator responsible for running pipelines locally.\"\"\" supports_local_execution = True supports_remote_execution = False @property def flavor ( self ) -> OrchestratorFlavor : \"\"\"The orchestrator flavor.\"\"\" return OrchestratorFlavor . LOCAL def run_pipeline ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , run_name : str ) -> Any : \"\"\"Runs a pipeline locally.\"\"\" tfx_pipeline = create_tfx_pipeline ( pipeline , stack = stack ) LocalDagRunner () . run ( tfx_pipeline , run_name ) flavor : OrchestratorFlavor property readonly The orchestrator flavor. run_pipeline ( self , pipeline , stack , run_name ) Runs a pipeline locally. Source code in zenml/orchestrators/local/local_orchestrator.py def run_pipeline ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , run_name : str ) -> Any : \"\"\"Runs a pipeline locally.\"\"\" tfx_pipeline = create_tfx_pipeline ( pipeline , stack = stack ) LocalDagRunner () . run ( tfx_pipeline , run_name ) utils create_tfx_pipeline ( zenml_pipeline , stack ) Creates a tfx pipeline from a ZenML pipeline. Source code in zenml/orchestrators/utils.py def create_tfx_pipeline ( zenml_pipeline : \"BasePipeline\" , stack : \"Stack\" ) -> tfx_pipeline . Pipeline : \"\"\"Creates a tfx pipeline from a ZenML pipeline.\"\"\" # Connect the inputs/outputs of all steps in the pipeline zenml_pipeline . connect ( ** zenml_pipeline . steps ) tfx_components = [ step . component for step in zenml_pipeline . steps . values ()] artifact_store = stack . artifact_store metadata_store = stack . metadata_store return tfx_pipeline . Pipeline ( pipeline_name = zenml_pipeline . name , components = tfx_components , # type: ignore[arg-type] pipeline_root = artifact_store . path , metadata_connection_config = metadata_store . get_tfx_metadata_config (), enable_cache = zenml_pipeline . enable_cache , ) execute_step ( tfx_launcher ) Executes a tfx component. Parameters: Name Type Description Default tfx_launcher Launcher A tfx launcher to execute the component. required Returns: Type Description Optional[tfx.orchestration.portable.data_types.ExecutionInfo] Optional execution info returned by the launcher. Source code in zenml/orchestrators/utils.py def execute_step ( tfx_launcher : launcher . Launcher , ) -> Optional [ data_types . ExecutionInfo ]: \"\"\"Executes a tfx component. Args: tfx_launcher: A tfx launcher to execute the component. Returns: Optional execution info returned by the launcher. \"\"\" step_name = tfx_launcher . _pipeline_node . node_info . id # type: ignore[attr-defined] # noqa start_time = time . time () logger . info ( f \"Step ` { step_name } ` has started.\" ) try : execution_info = tfx_launcher . launch () except RuntimeError as e : if \"execution has already succeeded\" in str ( e ): # Hacky workaround to catch the error that a pipeline run with # this name already exists. Raise an error with a more descriptive # message instead. raise DuplicateRunNameError () else : raise run_duration = time . time () - start_time logger . info ( \"Step ` %s ` has finished in %s .\" , step_name , string_utils . get_human_readable_time ( run_duration ), ) return execution_info","title":"Orchestrators"},{"location":"api_docs/orchestrators/#orchestrators","text":"","title":"Orchestrators"},{"location":"api_docs/orchestrators/#zenml.orchestrators","text":"An orchestrator is a special kind of backend that manages the running of each step of the pipeline. Orchestrators administer the actual pipeline runs. You can think of it as the 'root' of any pipeline job that you run during your experimentation. ZenML supports a local orchestrator out of the box which allows you to run your pipelines in a local environment. We also support using Apache Airflow as the orchestrator to handle the steps of your pipeline.","title":"orchestrators"},{"location":"api_docs/orchestrators/#zenml.orchestrators.base_orchestrator","text":"","title":"base_orchestrator"},{"location":"api_docs/orchestrators/#zenml.orchestrators.base_orchestrator.BaseOrchestrator","text":"Base class for all ZenML orchestrators. Source code in zenml/orchestrators/base_orchestrator.py class BaseOrchestrator ( StackComponent , ABC ): \"\"\"Base class for all ZenML orchestrators.\"\"\" @property def type ( self ) -> StackComponentType : \"\"\"The component type.\"\"\" return StackComponentType . ORCHESTRATOR @property @abstractmethod def flavor ( self ) -> OrchestratorFlavor : \"\"\"The orchestrator flavor.\"\"\" @abstractmethod def run_pipeline ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , run_name : str ) -> Any : \"\"\"Runs a pipeline. Args: pipeline: The pipeline to run. stack: The stack on which the pipeline is run. run_name: Name of the pipeline run. \"\"\"","title":"BaseOrchestrator"},{"location":"api_docs/orchestrators/#zenml.orchestrators.base_orchestrator.BaseOrchestrator.flavor","text":"The orchestrator flavor.","title":"flavor"},{"location":"api_docs/orchestrators/#zenml.orchestrators.base_orchestrator.BaseOrchestrator.type","text":"The component type.","title":"type"},{"location":"api_docs/orchestrators/#zenml.orchestrators.base_orchestrator.BaseOrchestrator.run_pipeline","text":"Runs a pipeline. Parameters: Name Type Description Default pipeline BasePipeline The pipeline to run. required stack Stack The stack on which the pipeline is run. required run_name str Name of the pipeline run. required Source code in zenml/orchestrators/base_orchestrator.py @abstractmethod def run_pipeline ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , run_name : str ) -> Any : \"\"\"Runs a pipeline. Args: pipeline: The pipeline to run. stack: The stack on which the pipeline is run. run_name: Name of the pipeline run. \"\"\"","title":"run_pipeline()"},{"location":"api_docs/orchestrators/#zenml.orchestrators.local","text":"","title":"local"},{"location":"api_docs/orchestrators/#zenml.orchestrators.local.local_dag_runner","text":"Inspired by local dag runner implementation by Google at: https://github.com/tensorflow/tfx/blob/master/tfx/orchestration /local/local_dag_runner.py","title":"local_dag_runner"},{"location":"api_docs/orchestrators/#zenml.orchestrators.local.local_dag_runner.LocalDagRunner","text":"Local TFX DAG runner. Source code in zenml/orchestrators/local/local_dag_runner.py class LocalDagRunner ( tfx_runner . TfxRunner ): \"\"\"Local TFX DAG runner.\"\"\" def __init__ ( self ) -> None : \"\"\"Initializes LocalDagRunner as a TFX orchestrator.\"\"\" def run ( self , pipeline : tfx_pipeline . Pipeline , run_name : str = \"\" ) -> None : \"\"\"Runs given logical pipeline locally. Args: pipeline: Logical pipeline containing pipeline args and components. run_name: Name of the pipeline run. \"\"\" for component in pipeline . components : if isinstance ( component , base_component . BaseComponent ): component . _resolve_pip_dependencies ( pipeline . pipeline_info . pipeline_root ) c = compiler . Compiler () pipeline = c . compile ( pipeline ) # Substitute the runtime parameter to be a concrete run_id runtime_parameter_utils . substitute_runtime_parameter ( pipeline , { PIPELINE_RUN_ID_PARAMETER_NAME : run_name , }, ) deployment_config = runner_utils . extract_local_deployment_config ( pipeline ) connection_config = ( Repository () . active_stack . metadata_store . get_tfx_metadata_config () ) logger . debug ( f \"Using deployment config: \\n { deployment_config } \" ) logger . debug ( f \"Using connection config: \\n { connection_config } \" ) # Run each component. Note that the pipeline.components list is in # topological order. for node in pipeline . nodes : pipeline_node = node . pipeline_node node_id = pipeline_node . node_info . id executor_spec = runner_utils . extract_executor_spec ( deployment_config , node_id ) custom_driver_spec = runner_utils . extract_custom_driver_spec ( deployment_config , node_id ) component_launcher = launcher . Launcher ( pipeline_node = pipeline_node , mlmd_connection = metadata . Metadata ( connection_config ), pipeline_info = pipeline . pipeline_info , pipeline_runtime_spec = pipeline . runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) execute_step ( component_launcher )","title":"LocalDagRunner"},{"location":"api_docs/orchestrators/#zenml.orchestrators.local.local_dag_runner.LocalDagRunner.__init__","text":"Initializes LocalDagRunner as a TFX orchestrator. Source code in zenml/orchestrators/local/local_dag_runner.py def __init__ ( self ) -> None : \"\"\"Initializes LocalDagRunner as a TFX orchestrator.\"\"\"","title":"__init__()"},{"location":"api_docs/orchestrators/#zenml.orchestrators.local.local_dag_runner.LocalDagRunner.run","text":"Runs given logical pipeline locally. Parameters: Name Type Description Default pipeline Pipeline Logical pipeline containing pipeline args and components. required run_name str Name of the pipeline run. '' Source code in zenml/orchestrators/local/local_dag_runner.py def run ( self , pipeline : tfx_pipeline . Pipeline , run_name : str = \"\" ) -> None : \"\"\"Runs given logical pipeline locally. Args: pipeline: Logical pipeline containing pipeline args and components. run_name: Name of the pipeline run. \"\"\" for component in pipeline . components : if isinstance ( component , base_component . BaseComponent ): component . _resolve_pip_dependencies ( pipeline . pipeline_info . pipeline_root ) c = compiler . Compiler () pipeline = c . compile ( pipeline ) # Substitute the runtime parameter to be a concrete run_id runtime_parameter_utils . substitute_runtime_parameter ( pipeline , { PIPELINE_RUN_ID_PARAMETER_NAME : run_name , }, ) deployment_config = runner_utils . extract_local_deployment_config ( pipeline ) connection_config = ( Repository () . active_stack . metadata_store . get_tfx_metadata_config () ) logger . debug ( f \"Using deployment config: \\n { deployment_config } \" ) logger . debug ( f \"Using connection config: \\n { connection_config } \" ) # Run each component. Note that the pipeline.components list is in # topological order. for node in pipeline . nodes : pipeline_node = node . pipeline_node node_id = pipeline_node . node_info . id executor_spec = runner_utils . extract_executor_spec ( deployment_config , node_id ) custom_driver_spec = runner_utils . extract_custom_driver_spec ( deployment_config , node_id ) component_launcher = launcher . Launcher ( pipeline_node = pipeline_node , mlmd_connection = metadata . Metadata ( connection_config ), pipeline_info = pipeline . pipeline_info , pipeline_runtime_spec = pipeline . runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) execute_step ( component_launcher )","title":"run()"},{"location":"api_docs/orchestrators/#zenml.orchestrators.local.local_orchestrator","text":"","title":"local_orchestrator"},{"location":"api_docs/orchestrators/#zenml.orchestrators.local.local_orchestrator.LocalOrchestrator","text":"Orchestrator responsible for running pipelines locally. Source code in zenml/orchestrators/local/local_orchestrator.py class LocalOrchestrator ( BaseOrchestrator ): \"\"\"Orchestrator responsible for running pipelines locally.\"\"\" supports_local_execution = True supports_remote_execution = False @property def flavor ( self ) -> OrchestratorFlavor : \"\"\"The orchestrator flavor.\"\"\" return OrchestratorFlavor . LOCAL def run_pipeline ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , run_name : str ) -> Any : \"\"\"Runs a pipeline locally.\"\"\" tfx_pipeline = create_tfx_pipeline ( pipeline , stack = stack ) LocalDagRunner () . run ( tfx_pipeline , run_name )","title":"LocalOrchestrator"},{"location":"api_docs/orchestrators/#zenml.orchestrators.local.local_orchestrator.LocalOrchestrator.flavor","text":"The orchestrator flavor.","title":"flavor"},{"location":"api_docs/orchestrators/#zenml.orchestrators.local.local_orchestrator.LocalOrchestrator.run_pipeline","text":"Runs a pipeline locally. Source code in zenml/orchestrators/local/local_orchestrator.py def run_pipeline ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , run_name : str ) -> Any : \"\"\"Runs a pipeline locally.\"\"\" tfx_pipeline = create_tfx_pipeline ( pipeline , stack = stack ) LocalDagRunner () . run ( tfx_pipeline , run_name )","title":"run_pipeline()"},{"location":"api_docs/orchestrators/#zenml.orchestrators.utils","text":"","title":"utils"},{"location":"api_docs/orchestrators/#zenml.orchestrators.utils.create_tfx_pipeline","text":"Creates a tfx pipeline from a ZenML pipeline. Source code in zenml/orchestrators/utils.py def create_tfx_pipeline ( zenml_pipeline : \"BasePipeline\" , stack : \"Stack\" ) -> tfx_pipeline . Pipeline : \"\"\"Creates a tfx pipeline from a ZenML pipeline.\"\"\" # Connect the inputs/outputs of all steps in the pipeline zenml_pipeline . connect ( ** zenml_pipeline . steps ) tfx_components = [ step . component for step in zenml_pipeline . steps . values ()] artifact_store = stack . artifact_store metadata_store = stack . metadata_store return tfx_pipeline . Pipeline ( pipeline_name = zenml_pipeline . name , components = tfx_components , # type: ignore[arg-type] pipeline_root = artifact_store . path , metadata_connection_config = metadata_store . get_tfx_metadata_config (), enable_cache = zenml_pipeline . enable_cache , )","title":"create_tfx_pipeline()"},{"location":"api_docs/orchestrators/#zenml.orchestrators.utils.execute_step","text":"Executes a tfx component. Parameters: Name Type Description Default tfx_launcher Launcher A tfx launcher to execute the component. required Returns: Type Description Optional[tfx.orchestration.portable.data_types.ExecutionInfo] Optional execution info returned by the launcher. Source code in zenml/orchestrators/utils.py def execute_step ( tfx_launcher : launcher . Launcher , ) -> Optional [ data_types . ExecutionInfo ]: \"\"\"Executes a tfx component. Args: tfx_launcher: A tfx launcher to execute the component. Returns: Optional execution info returned by the launcher. \"\"\" step_name = tfx_launcher . _pipeline_node . node_info . id # type: ignore[attr-defined] # noqa start_time = time . time () logger . info ( f \"Step ` { step_name } ` has started.\" ) try : execution_info = tfx_launcher . launch () except RuntimeError as e : if \"execution has already succeeded\" in str ( e ): # Hacky workaround to catch the error that a pipeline run with # this name already exists. Raise an error with a more descriptive # message instead. raise DuplicateRunNameError () else : raise run_duration = time . time () - start_time logger . info ( \"Step ` %s ` has finished in %s .\" , step_name , string_utils . get_human_readable_time ( run_duration ), ) return execution_info","title":"execute_step()"},{"location":"api_docs/pipelines/","text":"Pipelines zenml.pipelines special A ZenML pipeline is a sequence of tasks that execute in a specific order and yield artifacts. The artifacts are stored within the artifact store and indexed via the metadata store. Each individual task within a pipeline is known as a step. The standard pipelines within ZenML are designed to have easy interfaces to add pre-decided steps, with the order also pre-decided. Other sorts of pipelines can be created as well from scratch, building on the BasePipeline class. Pipelines can be written as simple functions. They are created by using decorators appropriate to the specific use case you have. The moment it is run , a pipeline is compiled and passed directly to the orchestrator. base_pipeline BasePipeline Abstract base class for all ZenML pipelines. Attributes: Name Type Description name The name of this pipeline. enable_cache A boolean indicating if caching is enabled for this pipeline. requirements_file Optional path to a pip requirements file that contains all requirements to run the pipeline. Source code in zenml/pipelines/base_pipeline.py class BasePipeline ( metaclass = BasePipelineMeta ): \"\"\"Abstract base class for all ZenML pipelines. Attributes: name: The name of this pipeline. enable_cache: A boolean indicating if caching is enabled for this pipeline. requirements_file: Optional path to a pip requirements file that contains all requirements to run the pipeline. \"\"\" STEP_SPEC : ClassVar [ Dict [ str , Any ]] = None # type: ignore[assignment] INSTANCE_CONFIGURATION : Dict [ Text , Any ] = {} def __init__ ( self , * args : BaseStep , ** kwargs : Any ) -> None : kwargs . update ( getattr ( self , INSTANCE_CONFIGURATION )) self . enable_cache = kwargs . pop ( PARAM_ENABLE_CACHE , True ) self . requirements_file = kwargs . pop ( PARAM_REQUIREMENTS_FILE , None ) self . dockerignore_file = kwargs . pop ( PARAM_DOCKERIGNORE_FILE , None ) self . name = self . __class__ . __name__ logger . info ( \"Creating run for pipeline: ` %s `\" , self . name ) logger . info ( f 'Cache { \"enabled\" if self . enable_cache else \"disabled\" } for ' f \"pipeline ` { self . name } `\" ) self . __steps : Dict [ str , BaseStep ] = {} self . _verify_arguments ( * args , ** kwargs ) def _verify_arguments ( self , * steps : BaseStep , ** kw_steps : BaseStep ) -> None : \"\"\"Verifies the initialization args and kwargs of this pipeline. This method makes sure that no missing/unexpected arguments or arguments of a wrong type are passed when creating a pipeline. If all arguments are correct, saves the steps to `self.__steps`. Args: *steps: The args passed to the init method of this pipeline. **kw_steps: The kwargs passed to the init method of this pipeline. Raises: PipelineInterfaceError: If there are too many/few arguments or arguments with a wrong name/type. \"\"\" input_step_keys = list ( self . STEP_SPEC . keys ()) if len ( steps ) > len ( input_step_keys ): raise PipelineInterfaceError ( f \"Too many input steps for pipeline ' { self . name } '. \" f \"This pipeline expects { len ( input_step_keys ) } step(s) \" f \"but got { len ( steps ) + len ( kw_steps ) } .\" ) combined_steps = {} step_cls_args : Set [ Type [ BaseStep ]] = set () for i , step in enumerate ( steps ): step_class = type ( step ) if not isinstance ( step , BaseStep ): raise PipelineInterfaceError ( f \"Wrong argument type (` { step_class } `) for positional \" f \"argument { i } of pipeline ' { self . name } '. Only \" f \"`@step` decorated functions or instances of `BaseStep` \" f \"subclasses can be used as arguments when creating \" f \"a pipeline.\" ) if step_class in step_cls_args : raise PipelineInterfaceError ( f \"Step object (` { step_class } `) has been used twice. Step \" f \"objects should be unique for each argument.\" ) key = input_step_keys [ i ] step . pipeline_parameter_name = key combined_steps [ key ] = step step_cls_args . add ( step_class ) step_cls_kwargs : Dict [ Type [ BaseStep ], str ] = {} for key , step in kw_steps . items (): step_class = type ( step ) if key in combined_steps : # a step for this key was already set by # the positional input steps raise PipelineInterfaceError ( f \"Unexpected keyword argument ' { key } ' for pipeline \" f \"' { self . name } '. A step for this key was \" f \"already passed as a positional argument.\" ) if not isinstance ( step , BaseStep ): raise PipelineInterfaceError ( f \"Wrong argument type (` { step_class } `) for argument \" f \"' { key } ' of pipeline ' { self . name } '. Only \" f \"`@step` decorated functions or instances of `BaseStep` \" f \"subclasses can be used as arguments when creating \" f \"a pipeline.\" ) if step_class in step_cls_kwargs : prev_key = step_cls_kwargs [ step_class ] raise PipelineInterfaceError ( f \"Same step object (` { step_class } `) passed for arguments \" f \"' { key } ' and ' { prev_key } '. Step objects should be \" f \"unique for each argument.\" ) if step_class in step_cls_args : raise PipelineInterfaceError ( f \"Step object (` { step_class } `) has been used twice. Step \" f \"objects should be unique for each argument.\" ) step . pipeline_parameter_name = key combined_steps [ key ] = step step_cls_kwargs [ step_class ] = key # check if there are any missing or unexpected steps expected_steps = set ( self . STEP_SPEC . keys ()) actual_steps = set ( combined_steps . keys ()) missing_steps = expected_steps - actual_steps unexpected_steps = actual_steps - expected_steps if missing_steps : raise PipelineInterfaceError ( f \"Missing input step(s) for pipeline \" f \"' { self . name } ': { missing_steps } .\" ) if unexpected_steps : raise PipelineInterfaceError ( f \"Unexpected input step(s) for pipeline \" f \"' { self . name } ': { unexpected_steps } . This pipeline \" f \"only requires the following steps: { expected_steps } .\" ) self . __steps = combined_steps @abstractmethod def connect ( self , * args : BaseStep , ** kwargs : BaseStep ) -> None : \"\"\"Function that connects inputs and outputs of the pipeline steps.\"\"\" raise NotImplementedError @property def steps ( self ) -> Dict [ str , BaseStep ]: \"\"\"Returns a dictionary of pipeline steps.\"\"\" return self . __steps @steps . setter def steps ( self , steps : Dict [ str , BaseStep ]) -> NoReturn : \"\"\"Setting the steps property is not allowed. This method always raises a PipelineInterfaceError. \"\"\" raise PipelineInterfaceError ( \"Cannot set steps manually!\" ) # TODO [ENG-376]: Enable specifying runtime configuration options either using # **kwargs here or by passing a `RuntimeConfiguration` object or a # path to a config file. def run ( self , run_name : Optional [ str ] = None ) -> Any : \"\"\"Runs the pipeline on the active stack of the current repository. Args: run_name: Name of the pipeline run. \"\"\" if SHOULD_PREVENT_PIPELINE_EXECUTION : # An environment variable was set to stop the execution of # pipelines. This is done to prevent execution of module-level # pipeline.run() calls inside docker containers which should only # run a single step. logger . info ( \"Preventing execution of pipeline ' %s '. If this is not \" \"intended behavior, make sure to unset the environment \" \"variable ' %s '.\" , self . name , ENV_ZENML_PREVENT_PIPELINE_EXECUTION , ) return # Activating the built-in integrations through lazy loading from zenml.integrations.registry import integration_registry integration_registry . activate_integrations () # Path of the file where pipeline.run() was called. This is needed by # the airflow orchestrator so it knows which file to copy into the DAG # directory dag_filepath = fileio . resolve_relative_path ( inspect . currentframe () . f_back . f_code . co_filename # type: ignore[union-attr] # noqa ) runtime_configuration = RuntimeConfiguration ( run_name = run_name , dag_filepath = dag_filepath ) stack = Repository () . active_stack analytics_utils . track_event ( event = analytics_utils . RUN_PIPELINE , metadata = { \"total_steps\" : len ( self . steps ), }, ) return stack . deploy_pipeline ( self , runtime_configuration = runtime_configuration ) def with_config ( self : T , config_file : str , overwrite_step_parameters : bool = False ) -> T : \"\"\"Configures this pipeline using a yaml file. Args: config_file: Path to a yaml file which contains configuration options for running this pipeline. See https://docs.zenml.io/features/pipeline-configuration#setting-step-parameters-using-a-config-file for details regarding the specification of this file. overwrite_step_parameters: If set to `True`, values from the configuration file will overwrite configuration parameters passed in code. Returns: The pipeline object that this method was called on. \"\"\" config_yaml = yaml_utils . read_yaml ( config_file ) if PipelineConfigurationKeys . STEPS in config_yaml : self . _read_config_steps ( config_yaml [ PipelineConfigurationKeys . STEPS ], overwrite = overwrite_step_parameters , ) return self def _read_config_steps ( self , steps : Dict [ str , Dict [ str , Any ]], overwrite : bool = False ) -> None : \"\"\"Reads and sets step parameters from a config file. Args: steps: Maps step names to dicts of parameter names and values. overwrite: If `True`, overwrite previously set step parameters. \"\"\" for step_name , step_dict in steps . items (): StepConfigurationKeys . key_check ( step_dict ) if step_name not in self . __steps : raise PipelineConfigurationError ( f \"Found ' { step_name } ' step in configuration yaml but it \" f \"doesn't exist in the pipeline steps \" f \" { list ( self . __steps . keys ()) } .\" ) step = self . __steps [ step_name ] step_parameters = ( step . CONFIG_CLASS . __fields__ . keys () if step . CONFIG_CLASS else {} ) parameters = step_dict . get ( StepConfigurationKeys . PARAMETERS_ , {}) for parameter , value in parameters . items (): if parameter not in step_parameters : raise PipelineConfigurationError ( f \"Found parameter ' { parameter } ' for ' { step_name } ' step \" f \"in configuration yaml but it doesn't exist in the \" f \"configuration class ` { step . CONFIG_CLASS } `. Available \" f \"parameters for this step: \" f \" { list ( step_parameters ) } .\" ) previous_value = step . PARAM_SPEC . get ( parameter , None ) if overwrite : step . PARAM_SPEC [ parameter ] = value else : step . PARAM_SPEC . setdefault ( parameter , value ) if overwrite or not previous_value : logger . debug ( \"Setting parameter %s = %s for step ' %s '.\" , parameter , value , step_name , ) if previous_value and not overwrite : logger . warning ( \"Parameter ' %s ' from configuration yaml will NOT be \" \"set as a configuration object was given when \" \"creating the step. Set `overwrite_step_parameters=\" \"True` when setting the configuration yaml to always \" \"use the options specified in the yaml file.\" , parameter , ) steps : Dict [ str , zenml . steps . base_step . BaseStep ] property writable Returns a dictionary of pipeline steps. connect ( self , * args , ** kwargs ) Function that connects inputs and outputs of the pipeline steps. Source code in zenml/pipelines/base_pipeline.py @abstractmethod def connect ( self , * args : BaseStep , ** kwargs : BaseStep ) -> None : \"\"\"Function that connects inputs and outputs of the pipeline steps.\"\"\" raise NotImplementedError run ( self , run_name = None ) Runs the pipeline on the active stack of the current repository. Parameters: Name Type Description Default run_name Optional[str] Name of the pipeline run. None Source code in zenml/pipelines/base_pipeline.py def run ( self , run_name : Optional [ str ] = None ) -> Any : \"\"\"Runs the pipeline on the active stack of the current repository. Args: run_name: Name of the pipeline run. \"\"\" if SHOULD_PREVENT_PIPELINE_EXECUTION : # An environment variable was set to stop the execution of # pipelines. This is done to prevent execution of module-level # pipeline.run() calls inside docker containers which should only # run a single step. logger . info ( \"Preventing execution of pipeline ' %s '. If this is not \" \"intended behavior, make sure to unset the environment \" \"variable ' %s '.\" , self . name , ENV_ZENML_PREVENT_PIPELINE_EXECUTION , ) return # Activating the built-in integrations through lazy loading from zenml.integrations.registry import integration_registry integration_registry . activate_integrations () # Path of the file where pipeline.run() was called. This is needed by # the airflow orchestrator so it knows which file to copy into the DAG # directory dag_filepath = fileio . resolve_relative_path ( inspect . currentframe () . f_back . f_code . co_filename # type: ignore[union-attr] # noqa ) runtime_configuration = RuntimeConfiguration ( run_name = run_name , dag_filepath = dag_filepath ) stack = Repository () . active_stack analytics_utils . track_event ( event = analytics_utils . RUN_PIPELINE , metadata = { \"total_steps\" : len ( self . steps ), }, ) return stack . deploy_pipeline ( self , runtime_configuration = runtime_configuration ) with_config ( self , config_file , overwrite_step_parameters = False ) Configures this pipeline using a yaml file. Parameters: Name Type Description Default config_file str Path to a yaml file which contains configuration options for running this pipeline. See https://docs.zenml.io/features/pipeline-configuration#setting-step-parameters-using-a-config-file for details regarding the specification of this file. required overwrite_step_parameters bool If set to True , values from the configuration file will overwrite configuration parameters passed in code. False Returns: Type Description ~T The pipeline object that this method was called on. Source code in zenml/pipelines/base_pipeline.py def with_config ( self : T , config_file : str , overwrite_step_parameters : bool = False ) -> T : \"\"\"Configures this pipeline using a yaml file. Args: config_file: Path to a yaml file which contains configuration options for running this pipeline. See https://docs.zenml.io/features/pipeline-configuration#setting-step-parameters-using-a-config-file for details regarding the specification of this file. overwrite_step_parameters: If set to `True`, values from the configuration file will overwrite configuration parameters passed in code. Returns: The pipeline object that this method was called on. \"\"\" config_yaml = yaml_utils . read_yaml ( config_file ) if PipelineConfigurationKeys . STEPS in config_yaml : self . _read_config_steps ( config_yaml [ PipelineConfigurationKeys . STEPS ], overwrite = overwrite_step_parameters , ) return self BasePipelineMeta ( type ) Pipeline Metaclass responsible for validating the pipeline definition. Source code in zenml/pipelines/base_pipeline.py class BasePipelineMeta ( type ): \"\"\"Pipeline Metaclass responsible for validating the pipeline definition.\"\"\" def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BasePipelineMeta\" : \"\"\"Saves argument names for later verification purposes\"\"\" cls = cast ( Type [ \"BasePipeline\" ], super () . __new__ ( mcs , name , bases , dct )) cls . STEP_SPEC = {} connect_spec = inspect . getfullargspec ( getattr ( cls , PIPELINE_INNER_FUNC_NAME ) ) connect_args = connect_spec . args if connect_args and connect_args [ 0 ] == \"self\" : connect_args . pop ( 0 ) for arg in connect_args : arg_type = connect_spec . annotations . get ( arg , None ) cls . STEP_SPEC . update ({ arg : arg_type }) return cls __new__ ( mcs , name , bases , dct ) special staticmethod Saves argument names for later verification purposes Source code in zenml/pipelines/base_pipeline.py def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BasePipelineMeta\" : \"\"\"Saves argument names for later verification purposes\"\"\" cls = cast ( Type [ \"BasePipeline\" ], super () . __new__ ( mcs , name , bases , dct )) cls . STEP_SPEC = {} connect_spec = inspect . getfullargspec ( getattr ( cls , PIPELINE_INNER_FUNC_NAME ) ) connect_args = connect_spec . args if connect_args and connect_args [ 0 ] == \"self\" : connect_args . pop ( 0 ) for arg in connect_args : arg_type = connect_spec . annotations . get ( arg , None ) cls . STEP_SPEC . update ({ arg : arg_type }) return cls builtin_pipelines special training_pipeline TrainingPipeline ( BasePipeline ) Class for the classic training pipeline implementation Source code in zenml/pipelines/builtin_pipelines/training_pipeline.py class TrainingPipeline ( BasePipeline ): \"\"\"Class for the classic training pipeline implementation\"\"\" def connect ( # type: ignore[override] self , datasource : step_interfaces . BaseDatasourceStep , splitter : step_interfaces . BaseSplitStep , analyzer : step_interfaces . BaseAnalyzerStep , preprocessor : step_interfaces . BasePreprocessorStep , trainer : step_interfaces . BaseTrainerStep , evaluator : step_interfaces . BaseEvaluatorStep , ) -> None : \"\"\"Main connect method for the standard training pipelines Args: datasource: the step responsible for the data ingestion splitter: the step responsible for splitting the dataset into train, test, val analyzer: the step responsible for extracting the statistics and the schema preprocessor: the step responsible for preprocessing the data trainer: the step responsible for training a model evaluator: the step responsible for computing the evaluation of the trained model \"\"\" # Ingesting the datasource dataset = datasource () # Splitting the data train , test , validation = splitter ( dataset = dataset ) # type:ignore # Analyzing the train dataset statistics , schema = analyzer ( dataset = train ) # type:ignore # Preprocessing the splits train_t , test_t , validation_t = preprocessor ( # type:ignore train_dataset = train , test_dataset = test , validation_dataset = validation , statistics = statistics , schema = schema , ) # Training the model model = trainer ( train_dataset = train_t , validation_dataset = validation_t ) # Evaluating the trained model evaluator ( model = model , dataset = test_t ) # type:ignore connect ( self , datasource , splitter , analyzer , preprocessor , trainer , evaluator ) Main connect method for the standard training pipelines Parameters: Name Type Description Default datasource BaseDatasourceStep the step responsible for the data ingestion required splitter BaseSplitStep the step responsible for splitting the dataset into train, test, val required analyzer BaseAnalyzerStep the step responsible for extracting the statistics and the schema required preprocessor BasePreprocessorStep the step responsible for preprocessing the data required trainer BaseTrainerStep the step responsible for training a model required evaluator BaseEvaluatorStep the step responsible for computing the evaluation of the trained model required Source code in zenml/pipelines/builtin_pipelines/training_pipeline.py def connect ( # type: ignore[override] self , datasource : step_interfaces . BaseDatasourceStep , splitter : step_interfaces . BaseSplitStep , analyzer : step_interfaces . BaseAnalyzerStep , preprocessor : step_interfaces . BasePreprocessorStep , trainer : step_interfaces . BaseTrainerStep , evaluator : step_interfaces . BaseEvaluatorStep , ) -> None : \"\"\"Main connect method for the standard training pipelines Args: datasource: the step responsible for the data ingestion splitter: the step responsible for splitting the dataset into train, test, val analyzer: the step responsible for extracting the statistics and the schema preprocessor: the step responsible for preprocessing the data trainer: the step responsible for training a model evaluator: the step responsible for computing the evaluation of the trained model \"\"\" # Ingesting the datasource dataset = datasource () # Splitting the data train , test , validation = splitter ( dataset = dataset ) # type:ignore # Analyzing the train dataset statistics , schema = analyzer ( dataset = train ) # type:ignore # Preprocessing the splits train_t , test_t , validation_t = preprocessor ( # type:ignore train_dataset = train , test_dataset = test , validation_dataset = validation , statistics = statistics , schema = schema , ) # Training the model model = trainer ( train_dataset = train_t , validation_dataset = validation_t ) # Evaluating the trained model evaluator ( model = model , dataset = test_t ) # type:ignore pipeline_decorator pipeline ( _func = None , * , name = None , enable_cache = True , requirements_file = None , dockerignore_file = None ) Outer decorator function for the creation of a ZenML pipeline In order to be able to work with parameters such as \"name\", it features a nested decorator structure. Parameters: Name Type Description Default _func Optional[~F] The decorated function. None name Optional[str] The name of the pipeline. If left empty, the name of the decorated function will be used as a fallback. None enable_cache bool Whether to use caching or not. True requirements_file Optional[str] Optional path to a pip requirements file that contains requirements to run the pipeline. None dockerignore_file Optional[str] Optional path to a dockerignore file to use when building docker images for running this pipeline. Note : If you pass a file, make sure it does not include the .zen directory as it is needed to run ZenML inside the container. None Returns: Type Description Union[Type[zenml.pipelines.base_pipeline.BasePipeline], Callable[[~F], Type[zenml.pipelines.base_pipeline.BasePipeline]]] the inner decorator which creates the pipeline class based on the ZenML BasePipeline Source code in zenml/pipelines/pipeline_decorator.py def pipeline ( _func : Optional [ F ] = None , * , name : Optional [ str ] = None , enable_cache : bool = True , requirements_file : Optional [ str ] = None , dockerignore_file : Optional [ str ] = None , ) -> Union [ Type [ BasePipeline ], Callable [[ F ], Type [ BasePipeline ]]]: \"\"\"Outer decorator function for the creation of a ZenML pipeline In order to be able to work with parameters such as \"name\", it features a nested decorator structure. Args: _func: The decorated function. name: The name of the pipeline. If left empty, the name of the decorated function will be used as a fallback. enable_cache: Whether to use caching or not. requirements_file: Optional path to a pip requirements file that contains requirements to run the pipeline. dockerignore_file: Optional path to a dockerignore file to use when building docker images for running this pipeline. **Note**: If you pass a file, make sure it does not include the `.zen` directory as it is needed to run ZenML inside the container. Returns: the inner decorator which creates the pipeline class based on the ZenML BasePipeline \"\"\" def inner_decorator ( func : F ) -> Type [ BasePipeline ]: \"\"\"Inner decorator function for the creation of a ZenML Pipeline Args: func: types.FunctionType, this function will be used as the \"connect\" method of the generated Pipeline Returns: the class of a newly generated ZenML Pipeline \"\"\" return type ( # noqa name if name else func . __name__ , ( BasePipeline ,), { PIPELINE_INNER_FUNC_NAME : staticmethod ( func ), INSTANCE_CONFIGURATION : { PARAM_ENABLE_CACHE : enable_cache , PARAM_REQUIREMENTS_FILE : requirements_file , PARAM_DOCKERIGNORE_FILE : dockerignore_file , }, }, ) if _func is None : return inner_decorator else : return inner_decorator ( _func )","title":"Pipelines"},{"location":"api_docs/pipelines/#pipelines","text":"","title":"Pipelines"},{"location":"api_docs/pipelines/#zenml.pipelines","text":"A ZenML pipeline is a sequence of tasks that execute in a specific order and yield artifacts. The artifacts are stored within the artifact store and indexed via the metadata store. Each individual task within a pipeline is known as a step. The standard pipelines within ZenML are designed to have easy interfaces to add pre-decided steps, with the order also pre-decided. Other sorts of pipelines can be created as well from scratch, building on the BasePipeline class. Pipelines can be written as simple functions. They are created by using decorators appropriate to the specific use case you have. The moment it is run , a pipeline is compiled and passed directly to the orchestrator.","title":"pipelines"},{"location":"api_docs/pipelines/#zenml.pipelines.base_pipeline","text":"","title":"base_pipeline"},{"location":"api_docs/pipelines/#zenml.pipelines.base_pipeline.BasePipeline","text":"Abstract base class for all ZenML pipelines. Attributes: Name Type Description name The name of this pipeline. enable_cache A boolean indicating if caching is enabled for this pipeline. requirements_file Optional path to a pip requirements file that contains all requirements to run the pipeline. Source code in zenml/pipelines/base_pipeline.py class BasePipeline ( metaclass = BasePipelineMeta ): \"\"\"Abstract base class for all ZenML pipelines. Attributes: name: The name of this pipeline. enable_cache: A boolean indicating if caching is enabled for this pipeline. requirements_file: Optional path to a pip requirements file that contains all requirements to run the pipeline. \"\"\" STEP_SPEC : ClassVar [ Dict [ str , Any ]] = None # type: ignore[assignment] INSTANCE_CONFIGURATION : Dict [ Text , Any ] = {} def __init__ ( self , * args : BaseStep , ** kwargs : Any ) -> None : kwargs . update ( getattr ( self , INSTANCE_CONFIGURATION )) self . enable_cache = kwargs . pop ( PARAM_ENABLE_CACHE , True ) self . requirements_file = kwargs . pop ( PARAM_REQUIREMENTS_FILE , None ) self . dockerignore_file = kwargs . pop ( PARAM_DOCKERIGNORE_FILE , None ) self . name = self . __class__ . __name__ logger . info ( \"Creating run for pipeline: ` %s `\" , self . name ) logger . info ( f 'Cache { \"enabled\" if self . enable_cache else \"disabled\" } for ' f \"pipeline ` { self . name } `\" ) self . __steps : Dict [ str , BaseStep ] = {} self . _verify_arguments ( * args , ** kwargs ) def _verify_arguments ( self , * steps : BaseStep , ** kw_steps : BaseStep ) -> None : \"\"\"Verifies the initialization args and kwargs of this pipeline. This method makes sure that no missing/unexpected arguments or arguments of a wrong type are passed when creating a pipeline. If all arguments are correct, saves the steps to `self.__steps`. Args: *steps: The args passed to the init method of this pipeline. **kw_steps: The kwargs passed to the init method of this pipeline. Raises: PipelineInterfaceError: If there are too many/few arguments or arguments with a wrong name/type. \"\"\" input_step_keys = list ( self . STEP_SPEC . keys ()) if len ( steps ) > len ( input_step_keys ): raise PipelineInterfaceError ( f \"Too many input steps for pipeline ' { self . name } '. \" f \"This pipeline expects { len ( input_step_keys ) } step(s) \" f \"but got { len ( steps ) + len ( kw_steps ) } .\" ) combined_steps = {} step_cls_args : Set [ Type [ BaseStep ]] = set () for i , step in enumerate ( steps ): step_class = type ( step ) if not isinstance ( step , BaseStep ): raise PipelineInterfaceError ( f \"Wrong argument type (` { step_class } `) for positional \" f \"argument { i } of pipeline ' { self . name } '. Only \" f \"`@step` decorated functions or instances of `BaseStep` \" f \"subclasses can be used as arguments when creating \" f \"a pipeline.\" ) if step_class in step_cls_args : raise PipelineInterfaceError ( f \"Step object (` { step_class } `) has been used twice. Step \" f \"objects should be unique for each argument.\" ) key = input_step_keys [ i ] step . pipeline_parameter_name = key combined_steps [ key ] = step step_cls_args . add ( step_class ) step_cls_kwargs : Dict [ Type [ BaseStep ], str ] = {} for key , step in kw_steps . items (): step_class = type ( step ) if key in combined_steps : # a step for this key was already set by # the positional input steps raise PipelineInterfaceError ( f \"Unexpected keyword argument ' { key } ' for pipeline \" f \"' { self . name } '. A step for this key was \" f \"already passed as a positional argument.\" ) if not isinstance ( step , BaseStep ): raise PipelineInterfaceError ( f \"Wrong argument type (` { step_class } `) for argument \" f \"' { key } ' of pipeline ' { self . name } '. Only \" f \"`@step` decorated functions or instances of `BaseStep` \" f \"subclasses can be used as arguments when creating \" f \"a pipeline.\" ) if step_class in step_cls_kwargs : prev_key = step_cls_kwargs [ step_class ] raise PipelineInterfaceError ( f \"Same step object (` { step_class } `) passed for arguments \" f \"' { key } ' and ' { prev_key } '. Step objects should be \" f \"unique for each argument.\" ) if step_class in step_cls_args : raise PipelineInterfaceError ( f \"Step object (` { step_class } `) has been used twice. Step \" f \"objects should be unique for each argument.\" ) step . pipeline_parameter_name = key combined_steps [ key ] = step step_cls_kwargs [ step_class ] = key # check if there are any missing or unexpected steps expected_steps = set ( self . STEP_SPEC . keys ()) actual_steps = set ( combined_steps . keys ()) missing_steps = expected_steps - actual_steps unexpected_steps = actual_steps - expected_steps if missing_steps : raise PipelineInterfaceError ( f \"Missing input step(s) for pipeline \" f \"' { self . name } ': { missing_steps } .\" ) if unexpected_steps : raise PipelineInterfaceError ( f \"Unexpected input step(s) for pipeline \" f \"' { self . name } ': { unexpected_steps } . This pipeline \" f \"only requires the following steps: { expected_steps } .\" ) self . __steps = combined_steps @abstractmethod def connect ( self , * args : BaseStep , ** kwargs : BaseStep ) -> None : \"\"\"Function that connects inputs and outputs of the pipeline steps.\"\"\" raise NotImplementedError @property def steps ( self ) -> Dict [ str , BaseStep ]: \"\"\"Returns a dictionary of pipeline steps.\"\"\" return self . __steps @steps . setter def steps ( self , steps : Dict [ str , BaseStep ]) -> NoReturn : \"\"\"Setting the steps property is not allowed. This method always raises a PipelineInterfaceError. \"\"\" raise PipelineInterfaceError ( \"Cannot set steps manually!\" ) # TODO [ENG-376]: Enable specifying runtime configuration options either using # **kwargs here or by passing a `RuntimeConfiguration` object or a # path to a config file. def run ( self , run_name : Optional [ str ] = None ) -> Any : \"\"\"Runs the pipeline on the active stack of the current repository. Args: run_name: Name of the pipeline run. \"\"\" if SHOULD_PREVENT_PIPELINE_EXECUTION : # An environment variable was set to stop the execution of # pipelines. This is done to prevent execution of module-level # pipeline.run() calls inside docker containers which should only # run a single step. logger . info ( \"Preventing execution of pipeline ' %s '. If this is not \" \"intended behavior, make sure to unset the environment \" \"variable ' %s '.\" , self . name , ENV_ZENML_PREVENT_PIPELINE_EXECUTION , ) return # Activating the built-in integrations through lazy loading from zenml.integrations.registry import integration_registry integration_registry . activate_integrations () # Path of the file where pipeline.run() was called. This is needed by # the airflow orchestrator so it knows which file to copy into the DAG # directory dag_filepath = fileio . resolve_relative_path ( inspect . currentframe () . f_back . f_code . co_filename # type: ignore[union-attr] # noqa ) runtime_configuration = RuntimeConfiguration ( run_name = run_name , dag_filepath = dag_filepath ) stack = Repository () . active_stack analytics_utils . track_event ( event = analytics_utils . RUN_PIPELINE , metadata = { \"total_steps\" : len ( self . steps ), }, ) return stack . deploy_pipeline ( self , runtime_configuration = runtime_configuration ) def with_config ( self : T , config_file : str , overwrite_step_parameters : bool = False ) -> T : \"\"\"Configures this pipeline using a yaml file. Args: config_file: Path to a yaml file which contains configuration options for running this pipeline. See https://docs.zenml.io/features/pipeline-configuration#setting-step-parameters-using-a-config-file for details regarding the specification of this file. overwrite_step_parameters: If set to `True`, values from the configuration file will overwrite configuration parameters passed in code. Returns: The pipeline object that this method was called on. \"\"\" config_yaml = yaml_utils . read_yaml ( config_file ) if PipelineConfigurationKeys . STEPS in config_yaml : self . _read_config_steps ( config_yaml [ PipelineConfigurationKeys . STEPS ], overwrite = overwrite_step_parameters , ) return self def _read_config_steps ( self , steps : Dict [ str , Dict [ str , Any ]], overwrite : bool = False ) -> None : \"\"\"Reads and sets step parameters from a config file. Args: steps: Maps step names to dicts of parameter names and values. overwrite: If `True`, overwrite previously set step parameters. \"\"\" for step_name , step_dict in steps . items (): StepConfigurationKeys . key_check ( step_dict ) if step_name not in self . __steps : raise PipelineConfigurationError ( f \"Found ' { step_name } ' step in configuration yaml but it \" f \"doesn't exist in the pipeline steps \" f \" { list ( self . __steps . keys ()) } .\" ) step = self . __steps [ step_name ] step_parameters = ( step . CONFIG_CLASS . __fields__ . keys () if step . CONFIG_CLASS else {} ) parameters = step_dict . get ( StepConfigurationKeys . PARAMETERS_ , {}) for parameter , value in parameters . items (): if parameter not in step_parameters : raise PipelineConfigurationError ( f \"Found parameter ' { parameter } ' for ' { step_name } ' step \" f \"in configuration yaml but it doesn't exist in the \" f \"configuration class ` { step . CONFIG_CLASS } `. Available \" f \"parameters for this step: \" f \" { list ( step_parameters ) } .\" ) previous_value = step . PARAM_SPEC . get ( parameter , None ) if overwrite : step . PARAM_SPEC [ parameter ] = value else : step . PARAM_SPEC . setdefault ( parameter , value ) if overwrite or not previous_value : logger . debug ( \"Setting parameter %s = %s for step ' %s '.\" , parameter , value , step_name , ) if previous_value and not overwrite : logger . warning ( \"Parameter ' %s ' from configuration yaml will NOT be \" \"set as a configuration object was given when \" \"creating the step. Set `overwrite_step_parameters=\" \"True` when setting the configuration yaml to always \" \"use the options specified in the yaml file.\" , parameter , )","title":"BasePipeline"},{"location":"api_docs/pipelines/#zenml.pipelines.base_pipeline.BasePipeline.steps","text":"Returns a dictionary of pipeline steps.","title":"steps"},{"location":"api_docs/pipelines/#zenml.pipelines.base_pipeline.BasePipeline.connect","text":"Function that connects inputs and outputs of the pipeline steps. Source code in zenml/pipelines/base_pipeline.py @abstractmethod def connect ( self , * args : BaseStep , ** kwargs : BaseStep ) -> None : \"\"\"Function that connects inputs and outputs of the pipeline steps.\"\"\" raise NotImplementedError","title":"connect()"},{"location":"api_docs/pipelines/#zenml.pipelines.base_pipeline.BasePipeline.run","text":"Runs the pipeline on the active stack of the current repository. Parameters: Name Type Description Default run_name Optional[str] Name of the pipeline run. None Source code in zenml/pipelines/base_pipeline.py def run ( self , run_name : Optional [ str ] = None ) -> Any : \"\"\"Runs the pipeline on the active stack of the current repository. Args: run_name: Name of the pipeline run. \"\"\" if SHOULD_PREVENT_PIPELINE_EXECUTION : # An environment variable was set to stop the execution of # pipelines. This is done to prevent execution of module-level # pipeline.run() calls inside docker containers which should only # run a single step. logger . info ( \"Preventing execution of pipeline ' %s '. If this is not \" \"intended behavior, make sure to unset the environment \" \"variable ' %s '.\" , self . name , ENV_ZENML_PREVENT_PIPELINE_EXECUTION , ) return # Activating the built-in integrations through lazy loading from zenml.integrations.registry import integration_registry integration_registry . activate_integrations () # Path of the file where pipeline.run() was called. This is needed by # the airflow orchestrator so it knows which file to copy into the DAG # directory dag_filepath = fileio . resolve_relative_path ( inspect . currentframe () . f_back . f_code . co_filename # type: ignore[union-attr] # noqa ) runtime_configuration = RuntimeConfiguration ( run_name = run_name , dag_filepath = dag_filepath ) stack = Repository () . active_stack analytics_utils . track_event ( event = analytics_utils . RUN_PIPELINE , metadata = { \"total_steps\" : len ( self . steps ), }, ) return stack . deploy_pipeline ( self , runtime_configuration = runtime_configuration )","title":"run()"},{"location":"api_docs/pipelines/#zenml.pipelines.base_pipeline.BasePipeline.with_config","text":"Configures this pipeline using a yaml file. Parameters: Name Type Description Default config_file str Path to a yaml file which contains configuration options for running this pipeline. See https://docs.zenml.io/features/pipeline-configuration#setting-step-parameters-using-a-config-file for details regarding the specification of this file. required overwrite_step_parameters bool If set to True , values from the configuration file will overwrite configuration parameters passed in code. False Returns: Type Description ~T The pipeline object that this method was called on. Source code in zenml/pipelines/base_pipeline.py def with_config ( self : T , config_file : str , overwrite_step_parameters : bool = False ) -> T : \"\"\"Configures this pipeline using a yaml file. Args: config_file: Path to a yaml file which contains configuration options for running this pipeline. See https://docs.zenml.io/features/pipeline-configuration#setting-step-parameters-using-a-config-file for details regarding the specification of this file. overwrite_step_parameters: If set to `True`, values from the configuration file will overwrite configuration parameters passed in code. Returns: The pipeline object that this method was called on. \"\"\" config_yaml = yaml_utils . read_yaml ( config_file ) if PipelineConfigurationKeys . STEPS in config_yaml : self . _read_config_steps ( config_yaml [ PipelineConfigurationKeys . STEPS ], overwrite = overwrite_step_parameters , ) return self","title":"with_config()"},{"location":"api_docs/pipelines/#zenml.pipelines.base_pipeline.BasePipelineMeta","text":"Pipeline Metaclass responsible for validating the pipeline definition. Source code in zenml/pipelines/base_pipeline.py class BasePipelineMeta ( type ): \"\"\"Pipeline Metaclass responsible for validating the pipeline definition.\"\"\" def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BasePipelineMeta\" : \"\"\"Saves argument names for later verification purposes\"\"\" cls = cast ( Type [ \"BasePipeline\" ], super () . __new__ ( mcs , name , bases , dct )) cls . STEP_SPEC = {} connect_spec = inspect . getfullargspec ( getattr ( cls , PIPELINE_INNER_FUNC_NAME ) ) connect_args = connect_spec . args if connect_args and connect_args [ 0 ] == \"self\" : connect_args . pop ( 0 ) for arg in connect_args : arg_type = connect_spec . annotations . get ( arg , None ) cls . STEP_SPEC . update ({ arg : arg_type }) return cls","title":"BasePipelineMeta"},{"location":"api_docs/pipelines/#zenml.pipelines.base_pipeline.BasePipelineMeta.__new__","text":"Saves argument names for later verification purposes Source code in zenml/pipelines/base_pipeline.py def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BasePipelineMeta\" : \"\"\"Saves argument names for later verification purposes\"\"\" cls = cast ( Type [ \"BasePipeline\" ], super () . __new__ ( mcs , name , bases , dct )) cls . STEP_SPEC = {} connect_spec = inspect . getfullargspec ( getattr ( cls , PIPELINE_INNER_FUNC_NAME ) ) connect_args = connect_spec . args if connect_args and connect_args [ 0 ] == \"self\" : connect_args . pop ( 0 ) for arg in connect_args : arg_type = connect_spec . annotations . get ( arg , None ) cls . STEP_SPEC . update ({ arg : arg_type }) return cls","title":"__new__()"},{"location":"api_docs/pipelines/#zenml.pipelines.builtin_pipelines","text":"","title":"builtin_pipelines"},{"location":"api_docs/pipelines/#zenml.pipelines.builtin_pipelines.training_pipeline","text":"","title":"training_pipeline"},{"location":"api_docs/pipelines/#zenml.pipelines.builtin_pipelines.training_pipeline.TrainingPipeline","text":"Class for the classic training pipeline implementation Source code in zenml/pipelines/builtin_pipelines/training_pipeline.py class TrainingPipeline ( BasePipeline ): \"\"\"Class for the classic training pipeline implementation\"\"\" def connect ( # type: ignore[override] self , datasource : step_interfaces . BaseDatasourceStep , splitter : step_interfaces . BaseSplitStep , analyzer : step_interfaces . BaseAnalyzerStep , preprocessor : step_interfaces . BasePreprocessorStep , trainer : step_interfaces . BaseTrainerStep , evaluator : step_interfaces . BaseEvaluatorStep , ) -> None : \"\"\"Main connect method for the standard training pipelines Args: datasource: the step responsible for the data ingestion splitter: the step responsible for splitting the dataset into train, test, val analyzer: the step responsible for extracting the statistics and the schema preprocessor: the step responsible for preprocessing the data trainer: the step responsible for training a model evaluator: the step responsible for computing the evaluation of the trained model \"\"\" # Ingesting the datasource dataset = datasource () # Splitting the data train , test , validation = splitter ( dataset = dataset ) # type:ignore # Analyzing the train dataset statistics , schema = analyzer ( dataset = train ) # type:ignore # Preprocessing the splits train_t , test_t , validation_t = preprocessor ( # type:ignore train_dataset = train , test_dataset = test , validation_dataset = validation , statistics = statistics , schema = schema , ) # Training the model model = trainer ( train_dataset = train_t , validation_dataset = validation_t ) # Evaluating the trained model evaluator ( model = model , dataset = test_t ) # type:ignore","title":"TrainingPipeline"},{"location":"api_docs/pipelines/#zenml.pipelines.builtin_pipelines.training_pipeline.TrainingPipeline.connect","text":"Main connect method for the standard training pipelines Parameters: Name Type Description Default datasource BaseDatasourceStep the step responsible for the data ingestion required splitter BaseSplitStep the step responsible for splitting the dataset into train, test, val required analyzer BaseAnalyzerStep the step responsible for extracting the statistics and the schema required preprocessor BasePreprocessorStep the step responsible for preprocessing the data required trainer BaseTrainerStep the step responsible for training a model required evaluator BaseEvaluatorStep the step responsible for computing the evaluation of the trained model required Source code in zenml/pipelines/builtin_pipelines/training_pipeline.py def connect ( # type: ignore[override] self , datasource : step_interfaces . BaseDatasourceStep , splitter : step_interfaces . BaseSplitStep , analyzer : step_interfaces . BaseAnalyzerStep , preprocessor : step_interfaces . BasePreprocessorStep , trainer : step_interfaces . BaseTrainerStep , evaluator : step_interfaces . BaseEvaluatorStep , ) -> None : \"\"\"Main connect method for the standard training pipelines Args: datasource: the step responsible for the data ingestion splitter: the step responsible for splitting the dataset into train, test, val analyzer: the step responsible for extracting the statistics and the schema preprocessor: the step responsible for preprocessing the data trainer: the step responsible for training a model evaluator: the step responsible for computing the evaluation of the trained model \"\"\" # Ingesting the datasource dataset = datasource () # Splitting the data train , test , validation = splitter ( dataset = dataset ) # type:ignore # Analyzing the train dataset statistics , schema = analyzer ( dataset = train ) # type:ignore # Preprocessing the splits train_t , test_t , validation_t = preprocessor ( # type:ignore train_dataset = train , test_dataset = test , validation_dataset = validation , statistics = statistics , schema = schema , ) # Training the model model = trainer ( train_dataset = train_t , validation_dataset = validation_t ) # Evaluating the trained model evaluator ( model = model , dataset = test_t ) # type:ignore","title":"connect()"},{"location":"api_docs/pipelines/#zenml.pipelines.pipeline_decorator","text":"","title":"pipeline_decorator"},{"location":"api_docs/pipelines/#zenml.pipelines.pipeline_decorator.pipeline","text":"Outer decorator function for the creation of a ZenML pipeline In order to be able to work with parameters such as \"name\", it features a nested decorator structure. Parameters: Name Type Description Default _func Optional[~F] The decorated function. None name Optional[str] The name of the pipeline. If left empty, the name of the decorated function will be used as a fallback. None enable_cache bool Whether to use caching or not. True requirements_file Optional[str] Optional path to a pip requirements file that contains requirements to run the pipeline. None dockerignore_file Optional[str] Optional path to a dockerignore file to use when building docker images for running this pipeline. Note : If you pass a file, make sure it does not include the .zen directory as it is needed to run ZenML inside the container. None Returns: Type Description Union[Type[zenml.pipelines.base_pipeline.BasePipeline], Callable[[~F], Type[zenml.pipelines.base_pipeline.BasePipeline]]] the inner decorator which creates the pipeline class based on the ZenML BasePipeline Source code in zenml/pipelines/pipeline_decorator.py def pipeline ( _func : Optional [ F ] = None , * , name : Optional [ str ] = None , enable_cache : bool = True , requirements_file : Optional [ str ] = None , dockerignore_file : Optional [ str ] = None , ) -> Union [ Type [ BasePipeline ], Callable [[ F ], Type [ BasePipeline ]]]: \"\"\"Outer decorator function for the creation of a ZenML pipeline In order to be able to work with parameters such as \"name\", it features a nested decorator structure. Args: _func: The decorated function. name: The name of the pipeline. If left empty, the name of the decorated function will be used as a fallback. enable_cache: Whether to use caching or not. requirements_file: Optional path to a pip requirements file that contains requirements to run the pipeline. dockerignore_file: Optional path to a dockerignore file to use when building docker images for running this pipeline. **Note**: If you pass a file, make sure it does not include the `.zen` directory as it is needed to run ZenML inside the container. Returns: the inner decorator which creates the pipeline class based on the ZenML BasePipeline \"\"\" def inner_decorator ( func : F ) -> Type [ BasePipeline ]: \"\"\"Inner decorator function for the creation of a ZenML Pipeline Args: func: types.FunctionType, this function will be used as the \"connect\" method of the generated Pipeline Returns: the class of a newly generated ZenML Pipeline \"\"\" return type ( # noqa name if name else func . __name__ , ( BasePipeline ,), { PIPELINE_INNER_FUNC_NAME : staticmethod ( func ), INSTANCE_CONFIGURATION : { PARAM_ENABLE_CACHE : enable_cache , PARAM_REQUIREMENTS_FILE : requirements_file , PARAM_DOCKERIGNORE_FILE : dockerignore_file , }, }, ) if _func is None : return inner_decorator else : return inner_decorator ( _func )","title":"pipeline()"},{"location":"api_docs/post_execution/","text":"Post Execution zenml.post_execution special After executing a pipeline, the user needs to be able to fetch it from history and perform certain tasks. The post_execution submodule provides a set of interfaces with which the user can interact with artifacts, the pipeline, steps, and the post-run pipeline object. artifact ArtifactView Post-execution artifact class which can be used to read artifact data that was created during a pipeline execution. Source code in zenml/post_execution/artifact.py class ArtifactView : \"\"\"Post-execution artifact class which can be used to read artifact data that was created during a pipeline execution. \"\"\" def __init__ ( self , id_ : int , type_ : str , uri : str , materializer : str , data_type : str , metadata_store : \"BaseMetadataStore\" , parent_step_id : int , ): \"\"\"Initializes a post-execution artifact object. In most cases `ArtifactView` objects should not be created manually but retrieved from a `StepView` via the `inputs` or `outputs` properties. Args: id_: The artifact id. type_: The type of this artifact. uri: Specifies where the artifact data is stored. materializer: Information needed to restore the materializer that was used to write this artifact. data_type: The type of data that was passed to the materializer when writing that artifact. Will be used as a default type to read the artifact. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline. parent_step_id: The ID of the parent step. \"\"\" self . _id = id_ self . _type = type_ self . _uri = uri self . _materializer = materializer self . _data_type = data_type self . _metadata_store = metadata_store self . _parent_step_id = parent_step_id @property def id ( self ) -> int : \"\"\"Returns the artifact id.\"\"\" return self . _id @property def type ( self ) -> str : \"\"\"Returns the artifact type.\"\"\" return self . _type @property def data_type ( self ) -> str : \"\"\"Returns the data type of the artifact.\"\"\" return self . _data_type @property def uri ( self ) -> str : \"\"\"Returns the URI where the artifact data is stored.\"\"\" return self . _uri @property def parent_step_id ( self ) -> int : \"\"\"Returns the ID of the parent step. This need not be equivalent to the ID of the producer step.\"\"\" return self . _parent_step_id @property def producer_step ( self ) -> \"StepView\" : \"\"\"Returns the original StepView that produced the artifact.\"\"\" # TODO [ENG-174]: Replace with artifact.id instead of passing self if # required. return self . _metadata_store . get_producer_step_from_artifact ( self ) @property def is_cached ( self ) -> bool : \"\"\"Returns True if artifact was cached in a previous run, else False.\"\"\" # self._metadata_store. return self . producer_step . id != self . parent_step_id def read ( self , output_data_type : Optional [ Type [ Any ]] = None , materializer_class : Optional [ Type [ \"BaseMaterializer\" ]] = None , ) -> Any : \"\"\"Materializes the data stored in this artifact. Args: output_data_type: The datatype to which the materializer should read, will be passed to the materializers `handle_input` method. materializer_class: The class of the materializer that should be used to read the artifact data. If no materializer class is given, we use the materializer that was used to write the artifact during execution of the pipeline. Returns: The materialized data. \"\"\" if not materializer_class : materializer_class = source_utils . load_source_path_class ( self . _materializer ) if not output_data_type : output_data_type = source_utils . load_source_path_class ( self . _data_type ) logger . debug ( \"Using ' %s ' to read ' %s ' (uri: %s ).\" , materializer_class . __qualname__ , self . _type , self . _uri , ) # TODO [ENG-162]: passing in `self` to initialize the materializer only # works because materializers only require a `.uri` property at the # moment. materializer = materializer_class ( self ) # type: ignore[arg-type] return materializer . handle_input ( output_data_type ) def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this artifact.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"type=' { self . _type } ', uri=' { self . _uri } ', \" f \"materializer=' { self . _materializer } ')\" ) def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same artifact.\"\"\" if isinstance ( other , ArtifactView ): return self . _id == other . _id and self . _uri == other . _uri return NotImplemented data_type : str property readonly Returns the data type of the artifact. id : int property readonly Returns the artifact id. is_cached : bool property readonly Returns True if artifact was cached in a previous run, else False. parent_step_id : int property readonly Returns the ID of the parent step. This need not be equivalent to the ID of the producer step. producer_step : StepView property readonly Returns the original StepView that produced the artifact. type : str property readonly Returns the artifact type. uri : str property readonly Returns the URI where the artifact data is stored. __eq__ ( self , other ) special Returns whether the other object is referring to the same artifact. Source code in zenml/post_execution/artifact.py def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same artifact.\"\"\" if isinstance ( other , ArtifactView ): return self . _id == other . _id and self . _uri == other . _uri return NotImplemented __init__ ( self , id_ , type_ , uri , materializer , data_type , metadata_store , parent_step_id ) special Initializes a post-execution artifact object. In most cases ArtifactView objects should not be created manually but retrieved from a StepView via the inputs or outputs properties. Parameters: Name Type Description Default id_ int The artifact id. required type_ str The type of this artifact. required uri str Specifies where the artifact data is stored. required materializer str Information needed to restore the materializer that was used to write this artifact. required data_type str The type of data that was passed to the materializer when writing that artifact. Will be used as a default type to read the artifact. required metadata_store BaseMetadataStore The metadata store which should be used to fetch additional information related to this pipeline. required parent_step_id int The ID of the parent step. required Source code in zenml/post_execution/artifact.py def __init__ ( self , id_ : int , type_ : str , uri : str , materializer : str , data_type : str , metadata_store : \"BaseMetadataStore\" , parent_step_id : int , ): \"\"\"Initializes a post-execution artifact object. In most cases `ArtifactView` objects should not be created manually but retrieved from a `StepView` via the `inputs` or `outputs` properties. Args: id_: The artifact id. type_: The type of this artifact. uri: Specifies where the artifact data is stored. materializer: Information needed to restore the materializer that was used to write this artifact. data_type: The type of data that was passed to the materializer when writing that artifact. Will be used as a default type to read the artifact. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline. parent_step_id: The ID of the parent step. \"\"\" self . _id = id_ self . _type = type_ self . _uri = uri self . _materializer = materializer self . _data_type = data_type self . _metadata_store = metadata_store self . _parent_step_id = parent_step_id __repr__ ( self ) special Returns a string representation of this artifact. Source code in zenml/post_execution/artifact.py def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this artifact.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"type=' { self . _type } ', uri=' { self . _uri } ', \" f \"materializer=' { self . _materializer } ')\" ) read ( self , output_data_type = None , materializer_class = None ) Materializes the data stored in this artifact. Parameters: Name Type Description Default output_data_type Optional[Type[Any]] The datatype to which the materializer should read, will be passed to the materializers handle_input method. None materializer_class Optional[Type[BaseMaterializer]] The class of the materializer that should be used to read the artifact data. If no materializer class is given, we use the materializer that was used to write the artifact during execution of the pipeline. None Returns: Type Description Any The materialized data. Source code in zenml/post_execution/artifact.py def read ( self , output_data_type : Optional [ Type [ Any ]] = None , materializer_class : Optional [ Type [ \"BaseMaterializer\" ]] = None , ) -> Any : \"\"\"Materializes the data stored in this artifact. Args: output_data_type: The datatype to which the materializer should read, will be passed to the materializers `handle_input` method. materializer_class: The class of the materializer that should be used to read the artifact data. If no materializer class is given, we use the materializer that was used to write the artifact during execution of the pipeline. Returns: The materialized data. \"\"\" if not materializer_class : materializer_class = source_utils . load_source_path_class ( self . _materializer ) if not output_data_type : output_data_type = source_utils . load_source_path_class ( self . _data_type ) logger . debug ( \"Using ' %s ' to read ' %s ' (uri: %s ).\" , materializer_class . __qualname__ , self . _type , self . _uri , ) # TODO [ENG-162]: passing in `self` to initialize the materializer only # works because materializers only require a `.uri` property at the # moment. materializer = materializer_class ( self ) # type: ignore[arg-type] return materializer . handle_input ( output_data_type ) pipeline PipelineView Post-execution pipeline class which can be used to query pipeline-related information from the metadata store. Source code in zenml/post_execution/pipeline.py class PipelineView : \"\"\"Post-execution pipeline class which can be used to query pipeline-related information from the metadata store. \"\"\" def __init__ ( self , id_ : int , name : str , metadata_store : \"BaseMetadataStore\" ): \"\"\"Initializes a post-execution pipeline object. In most cases `PipelineView` objects should not be created manually but retrieved using the `get_pipelines()` method of a `zenml.core.repo.Repository` instead. Args: id_: The context id of this pipeline. name: The name of this pipeline. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline. \"\"\" self . _id = id_ self . _name = name self . _metadata_store = metadata_store @property def name ( self ) -> str : \"\"\"Returns the name of the pipeline.\"\"\" return self . _name @property def runs ( self ) -> List [ \"PipelineRunView\" ]: \"\"\"Returns all stored runs of this pipeline. The runs are returned in chronological order, so the latest run will be the last element in this list. \"\"\" # Do not cache runs as new runs might appear during this objects # lifecycle runs = self . _metadata_store . get_pipeline_runs ( self ) return list ( runs . values ()) def get_run_names ( self ) -> List [ str ]: \"\"\"Returns a list of all run names.\"\"\" # Do not cache runs as new runs might appear during this objects # lifecycle runs = self . _metadata_store . get_pipeline_runs ( self ) return list ( runs . keys ()) def get_run ( self , name : str ) -> \"PipelineRunView\" : \"\"\"Returns a run for the given name. Args: name: The name of the run to return. Raises: KeyError: If there is no run with the given name. \"\"\" run = self . _metadata_store . get_pipeline_run ( self , name ) if not run : raise KeyError ( f \"No run found for name ` { name } `. This pipeline \" f \"only has runs with the following \" f \"names: ` { self . get_run_names () } `\" ) return run def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this pipeline.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . _name } ')\" ) def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same pipeline.\"\"\" if isinstance ( other , PipelineView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented name : str property readonly Returns the name of the pipeline. runs : List [ PipelineRunView ] property readonly Returns all stored runs of this pipeline. The runs are returned in chronological order, so the latest run will be the last element in this list. __eq__ ( self , other ) special Returns whether the other object is referring to the same pipeline. Source code in zenml/post_execution/pipeline.py def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same pipeline.\"\"\" if isinstance ( other , PipelineView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented __init__ ( self , id_ , name , metadata_store ) special Initializes a post-execution pipeline object. In most cases PipelineView objects should not be created manually but retrieved using the get_pipelines() method of a zenml.core.repo.Repository instead. Parameters: Name Type Description Default id_ int The context id of this pipeline. required name str The name of this pipeline. required metadata_store BaseMetadataStore The metadata store which should be used to fetch additional information related to this pipeline. required Source code in zenml/post_execution/pipeline.py def __init__ ( self , id_ : int , name : str , metadata_store : \"BaseMetadataStore\" ): \"\"\"Initializes a post-execution pipeline object. In most cases `PipelineView` objects should not be created manually but retrieved using the `get_pipelines()` method of a `zenml.core.repo.Repository` instead. Args: id_: The context id of this pipeline. name: The name of this pipeline. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline. \"\"\" self . _id = id_ self . _name = name self . _metadata_store = metadata_store __repr__ ( self ) special Returns a string representation of this pipeline. Source code in zenml/post_execution/pipeline.py def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this pipeline.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . _name } ')\" ) get_run ( self , name ) Returns a run for the given name. Parameters: Name Type Description Default name str The name of the run to return. required Exceptions: Type Description KeyError If there is no run with the given name. Source code in zenml/post_execution/pipeline.py def get_run ( self , name : str ) -> \"PipelineRunView\" : \"\"\"Returns a run for the given name. Args: name: The name of the run to return. Raises: KeyError: If there is no run with the given name. \"\"\" run = self . _metadata_store . get_pipeline_run ( self , name ) if not run : raise KeyError ( f \"No run found for name ` { name } `. This pipeline \" f \"only has runs with the following \" f \"names: ` { self . get_run_names () } `\" ) return run get_run_names ( self ) Returns a list of all run names. Source code in zenml/post_execution/pipeline.py def get_run_names ( self ) -> List [ str ]: \"\"\"Returns a list of all run names.\"\"\" # Do not cache runs as new runs might appear during this objects # lifecycle runs = self . _metadata_store . get_pipeline_runs ( self ) return list ( runs . keys ()) pipeline_run PipelineRunView Post-execution pipeline run class which can be used to query steps and artifact information associated with a pipeline execution. Source code in zenml/post_execution/pipeline_run.py class PipelineRunView : \"\"\"Post-execution pipeline run class which can be used to query steps and artifact information associated with a pipeline execution. \"\"\" def __init__ ( self , id_ : int , name : str , executions : List [ proto . Execution ], metadata_store : \"BaseMetadataStore\" , ): \"\"\"Initializes a post-execution pipeline run object. In most cases `PipelineRunView` objects should not be created manually but retrieved from a `PipelineView` object instead. Args: id_: The context id of this pipeline run. name: The name of this pipeline run. executions: All executions associated with this pipeline run. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline run. \"\"\" self . _id = id_ self . _name = name self . _metadata_store = metadata_store self . _executions = executions self . _steps : Dict [ str , StepView ] = OrderedDict () @property def name ( self ) -> str : \"\"\"Returns the name of the pipeline run.\"\"\" return self . _name @property def status ( self ) -> ExecutionStatus : \"\"\"Returns the current status of the pipeline run.\"\"\" step_statuses = ( step . status for step in self . steps ) if any ( status == ExecutionStatus . FAILED for status in step_statuses ): return ExecutionStatus . FAILED elif all ( status == ExecutionStatus . COMPLETED or status == ExecutionStatus . CACHED for status in step_statuses ): return ExecutionStatus . COMPLETED else : return ExecutionStatus . RUNNING @property def steps ( self ) -> List [ StepView ]: \"\"\"Returns all steps that were executed as part of this pipeline run.\"\"\" self . _ensure_steps_fetched () return list ( self . _steps . values ()) def get_step_names ( self ) -> List [ str ]: \"\"\"Returns a list of all step names.\"\"\" self . _ensure_steps_fetched () return list ( self . _steps . keys ()) def get_step ( self , name : str ) -> StepView : \"\"\"Returns a step for the given name. Args: name: The name of the step to return. Raises: KeyError: If there is no step with the given name. \"\"\" self . _ensure_steps_fetched () try : return self . _steps [ name ] except KeyError : raise KeyError ( f \"No step found for name ` { name } `. This pipeline \" f \"run only has steps with the following \" f \"names: ` { self . get_step_names () } `\" ) def _ensure_steps_fetched ( self ) -> None : \"\"\"Fetches all steps for this pipeline run from the metadata store.\"\"\" if self . _steps : # we already fetched the steps, no need to do anything return self . _steps = self . _metadata_store . get_pipeline_run_steps ( self ) def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this pipeline run.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . _name } ')\" ) def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same pipeline run.\"\"\" if isinstance ( other , PipelineRunView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented name : str property readonly Returns the name of the pipeline run. status : ExecutionStatus property readonly Returns the current status of the pipeline run. steps : List [ zenml . post_execution . step . StepView ] property readonly Returns all steps that were executed as part of this pipeline run. __eq__ ( self , other ) special Returns whether the other object is referring to the same pipeline run. Source code in zenml/post_execution/pipeline_run.py def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same pipeline run.\"\"\" if isinstance ( other , PipelineRunView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented __init__ ( self , id_ , name , executions , metadata_store ) special Initializes a post-execution pipeline run object. In most cases PipelineRunView objects should not be created manually but retrieved from a PipelineView object instead. Parameters: Name Type Description Default id_ int The context id of this pipeline run. required name str The name of this pipeline run. required executions List[ml_metadata.proto.metadata_store_pb2.Execution] All executions associated with this pipeline run. required metadata_store BaseMetadataStore The metadata store which should be used to fetch additional information related to this pipeline run. required Source code in zenml/post_execution/pipeline_run.py def __init__ ( self , id_ : int , name : str , executions : List [ proto . Execution ], metadata_store : \"BaseMetadataStore\" , ): \"\"\"Initializes a post-execution pipeline run object. In most cases `PipelineRunView` objects should not be created manually but retrieved from a `PipelineView` object instead. Args: id_: The context id of this pipeline run. name: The name of this pipeline run. executions: All executions associated with this pipeline run. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline run. \"\"\" self . _id = id_ self . _name = name self . _metadata_store = metadata_store self . _executions = executions self . _steps : Dict [ str , StepView ] = OrderedDict () __repr__ ( self ) special Returns a string representation of this pipeline run. Source code in zenml/post_execution/pipeline_run.py def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this pipeline run.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . _name } ')\" ) get_step ( self , name ) Returns a step for the given name. Parameters: Name Type Description Default name str The name of the step to return. required Exceptions: Type Description KeyError If there is no step with the given name. Source code in zenml/post_execution/pipeline_run.py def get_step ( self , name : str ) -> StepView : \"\"\"Returns a step for the given name. Args: name: The name of the step to return. Raises: KeyError: If there is no step with the given name. \"\"\" self . _ensure_steps_fetched () try : return self . _steps [ name ] except KeyError : raise KeyError ( f \"No step found for name ` { name } `. This pipeline \" f \"run only has steps with the following \" f \"names: ` { self . get_step_names () } `\" ) get_step_names ( self ) Returns a list of all step names. Source code in zenml/post_execution/pipeline_run.py def get_step_names ( self ) -> List [ str ]: \"\"\"Returns a list of all step names.\"\"\" self . _ensure_steps_fetched () return list ( self . _steps . keys ()) step StepView Post-execution step class which can be used to query artifact information associated with a pipeline step. Source code in zenml/post_execution/step.py class StepView : \"\"\"Post-execution step class which can be used to query artifact information associated with a pipeline step. \"\"\" def __init__ ( self , id_ : int , parents_step_ids : List [ int ], entrypoint_name : str , name : str , parameters : Dict [ str , Any ], metadata_store : \"BaseMetadataStore\" , ): \"\"\"Initializes a post-execution step object. In most cases `StepView` objects should not be created manually but retrieved from a `PipelineRunView` object instead. Args: id_: The execution id of this step. parents_step_ids: The execution ids of the parents of this step. entrypoint_name: The name of this step. name: The name of this step within the pipeline parameters: Parameters that were used to run this step. metadata_store: The metadata store which should be used to fetch additional information related to this step. \"\"\" self . _id = id_ self . _parents_step_ids = parents_step_ids self . _entrypoint_name = entrypoint_name self . _name = name self . _parameters = parameters self . _metadata_store = metadata_store self . _inputs : Dict [ str , ArtifactView ] = {} self . _outputs : Dict [ str , ArtifactView ] = {} @property def id ( self ) -> int : \"\"\"Returns the step id.\"\"\" return self . _id @property def parents_step_ids ( self ) -> List [ int ]: \"\"\"Returns a list of ID's of all parents of this step.\"\"\" return self . _parents_step_ids @property def parent_steps ( self ) -> List [ \"StepView\" ]: \"\"\"Returns a list of all parent steps of this step.\"\"\" steps = [ self . _metadata_store . get_step_by_id ( s ) for s in self . parents_step_ids ] return steps @property def entrypoint_name ( self ) -> str : \"\"\"Returns the step entrypoint_name. This name is equal to the name argument passed to the @step decorator or the actual function name if no explicit name was given. Examples: # the step entrypoint_name will be \"my_step\" @step(name=\"my_step\") def my_step_function(...) # the step entrypoint_name will be \"my_step_function\" @step def my_step_function(...) \"\"\" return self . _entrypoint_name @property def name ( self ) -> str : \"\"\"Returns the name as it is defined in the pipeline. This name is equal to the name given to the step within the pipeline context Examples: @step() def my_step_function(...) @pipeline def my_pipeline_function(step_a) p = my_pipeline_function( step_a = my_step_function() ) The name will be `step_a` \"\"\" return self . _name @property def parameters ( self ) -> Dict [ str , Any ]: \"\"\"The parameters used to run this step.\"\"\" return self . _parameters @property def status ( self ) -> ExecutionStatus : \"\"\"Returns the current status of the step.\"\"\" return self . _metadata_store . get_step_status ( self ) @property def is_cached ( self ) -> bool : \"\"\"Returns whether the step is cached or not.\"\"\" return self . status == ExecutionStatus . CACHED @property def inputs ( self ) -> Dict [ str , ArtifactView ]: \"\"\"Returns all input artifacts that were used to run this step.\"\"\" self . _ensure_inputs_outputs_fetched () return self . _inputs @property def input ( self ) -> ArtifactView : \"\"\"Returns the input artifact that was used to run this step. Raises: ValueError: If there were zero or multiple inputs to this step. \"\"\" if len ( self . inputs ) != 1 : raise ValueError ( \"Can't use the `StepView.input` property for steps with zero \" \"or multiple inputs, use `StepView.inputs` instead.\" ) return next ( iter ( self . inputs . values ())) @property def outputs ( self ) -> Dict [ str , ArtifactView ]: \"\"\"Returns all output artifacts that were written by this step.\"\"\" self . _ensure_inputs_outputs_fetched () return self . _outputs @property def output ( self ) -> ArtifactView : \"\"\"Returns the output artifact that was written by this step. Raises: ValueError: If there were zero or multiple step outputs. \"\"\" if len ( self . outputs ) != 1 : raise ValueError ( \"Can't use the `StepView.output` property for steps with zero \" \"or multiple outputs, use `StepView.outputs` instead.\" ) return next ( iter ( self . outputs . values ())) def _ensure_inputs_outputs_fetched ( self ) -> None : \"\"\"Fetches all step inputs and outputs from the metadata store.\"\"\" if self . _inputs or self . _outputs : # we already fetched inputs/outputs, no need to do anything return self . _inputs , self . _outputs = self . _metadata_store . get_step_artifacts ( self ) def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this step.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . name } ', entrypoint_name=' { self . entrypoint_name } '\" f \"parameters= { self . _parameters } )\" ) def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same step.\"\"\" if isinstance ( other , StepView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented entrypoint_name : str property readonly Returns the step entrypoint_name. This name is equal to the name argument passed to the @step decorator or the actual function name if no explicit name was given. Examples: the step entrypoint_name will be \"my_step\" @step(name=\"my_step\") def my_step_function(...) the step entrypoint_name will be \"my_step_function\" @step def my_step_function(...) id : int property readonly Returns the step id. input : ArtifactView property readonly Returns the input artifact that was used to run this step. Exceptions: Type Description ValueError If there were zero or multiple inputs to this step. inputs : Dict [ str , zenml . post_execution . artifact . ArtifactView ] property readonly Returns all input artifacts that were used to run this step. is_cached : bool property readonly Returns whether the step is cached or not. name : str property readonly Returns the name as it is defined in the pipeline. This name is equal to the name given to the step within the pipeline context Examples: @step() def my_step_function(...) @pipeline def my_pipeline_function(step_a) p = my_pipeline_function( step_a = my_step_function() ) The name will be step_a output : ArtifactView property readonly Returns the output artifact that was written by this step. Exceptions: Type Description ValueError If there were zero or multiple step outputs. outputs : Dict [ str , zenml . post_execution . artifact . ArtifactView ] property readonly Returns all output artifacts that were written by this step. parameters : Dict [ str , Any ] property readonly The parameters used to run this step. parent_steps : List [ StepView ] property readonly Returns a list of all parent steps of this step. parents_step_ids : List [ int ] property readonly Returns a list of ID's of all parents of this step. status : ExecutionStatus property readonly Returns the current status of the step. __eq__ ( self , other ) special Returns whether the other object is referring to the same step. Source code in zenml/post_execution/step.py def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same step.\"\"\" if isinstance ( other , StepView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented __init__ ( self , id_ , parents_step_ids , entrypoint_name , name , parameters , metadata_store ) special Initializes a post-execution step object. In most cases StepView objects should not be created manually but retrieved from a PipelineRunView object instead. Parameters: Name Type Description Default id_ int The execution id of this step. required parents_step_ids List[int] The execution ids of the parents of this step. required entrypoint_name str The name of this step. required name str The name of this step within the pipeline required parameters Dict[str, Any] Parameters that were used to run this step. required metadata_store BaseMetadataStore The metadata store which should be used to fetch additional information related to this step. required Source code in zenml/post_execution/step.py def __init__ ( self , id_ : int , parents_step_ids : List [ int ], entrypoint_name : str , name : str , parameters : Dict [ str , Any ], metadata_store : \"BaseMetadataStore\" , ): \"\"\"Initializes a post-execution step object. In most cases `StepView` objects should not be created manually but retrieved from a `PipelineRunView` object instead. Args: id_: The execution id of this step. parents_step_ids: The execution ids of the parents of this step. entrypoint_name: The name of this step. name: The name of this step within the pipeline parameters: Parameters that were used to run this step. metadata_store: The metadata store which should be used to fetch additional information related to this step. \"\"\" self . _id = id_ self . _parents_step_ids = parents_step_ids self . _entrypoint_name = entrypoint_name self . _name = name self . _parameters = parameters self . _metadata_store = metadata_store self . _inputs : Dict [ str , ArtifactView ] = {} self . _outputs : Dict [ str , ArtifactView ] = {} __repr__ ( self ) special Returns a string representation of this step. Source code in zenml/post_execution/step.py def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this step.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . name } ', entrypoint_name=' { self . entrypoint_name } '\" f \"parameters= { self . _parameters } )\" )","title":"Post Execution"},{"location":"api_docs/post_execution/#post-execution","text":"","title":"Post Execution"},{"location":"api_docs/post_execution/#zenml.post_execution","text":"After executing a pipeline, the user needs to be able to fetch it from history and perform certain tasks. The post_execution submodule provides a set of interfaces with which the user can interact with artifacts, the pipeline, steps, and the post-run pipeline object.","title":"post_execution"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact","text":"","title":"artifact"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView","text":"Post-execution artifact class which can be used to read artifact data that was created during a pipeline execution. Source code in zenml/post_execution/artifact.py class ArtifactView : \"\"\"Post-execution artifact class which can be used to read artifact data that was created during a pipeline execution. \"\"\" def __init__ ( self , id_ : int , type_ : str , uri : str , materializer : str , data_type : str , metadata_store : \"BaseMetadataStore\" , parent_step_id : int , ): \"\"\"Initializes a post-execution artifact object. In most cases `ArtifactView` objects should not be created manually but retrieved from a `StepView` via the `inputs` or `outputs` properties. Args: id_: The artifact id. type_: The type of this artifact. uri: Specifies where the artifact data is stored. materializer: Information needed to restore the materializer that was used to write this artifact. data_type: The type of data that was passed to the materializer when writing that artifact. Will be used as a default type to read the artifact. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline. parent_step_id: The ID of the parent step. \"\"\" self . _id = id_ self . _type = type_ self . _uri = uri self . _materializer = materializer self . _data_type = data_type self . _metadata_store = metadata_store self . _parent_step_id = parent_step_id @property def id ( self ) -> int : \"\"\"Returns the artifact id.\"\"\" return self . _id @property def type ( self ) -> str : \"\"\"Returns the artifact type.\"\"\" return self . _type @property def data_type ( self ) -> str : \"\"\"Returns the data type of the artifact.\"\"\" return self . _data_type @property def uri ( self ) -> str : \"\"\"Returns the URI where the artifact data is stored.\"\"\" return self . _uri @property def parent_step_id ( self ) -> int : \"\"\"Returns the ID of the parent step. This need not be equivalent to the ID of the producer step.\"\"\" return self . _parent_step_id @property def producer_step ( self ) -> \"StepView\" : \"\"\"Returns the original StepView that produced the artifact.\"\"\" # TODO [ENG-174]: Replace with artifact.id instead of passing self if # required. return self . _metadata_store . get_producer_step_from_artifact ( self ) @property def is_cached ( self ) -> bool : \"\"\"Returns True if artifact was cached in a previous run, else False.\"\"\" # self._metadata_store. return self . producer_step . id != self . parent_step_id def read ( self , output_data_type : Optional [ Type [ Any ]] = None , materializer_class : Optional [ Type [ \"BaseMaterializer\" ]] = None , ) -> Any : \"\"\"Materializes the data stored in this artifact. Args: output_data_type: The datatype to which the materializer should read, will be passed to the materializers `handle_input` method. materializer_class: The class of the materializer that should be used to read the artifact data. If no materializer class is given, we use the materializer that was used to write the artifact during execution of the pipeline. Returns: The materialized data. \"\"\" if not materializer_class : materializer_class = source_utils . load_source_path_class ( self . _materializer ) if not output_data_type : output_data_type = source_utils . load_source_path_class ( self . _data_type ) logger . debug ( \"Using ' %s ' to read ' %s ' (uri: %s ).\" , materializer_class . __qualname__ , self . _type , self . _uri , ) # TODO [ENG-162]: passing in `self` to initialize the materializer only # works because materializers only require a `.uri` property at the # moment. materializer = materializer_class ( self ) # type: ignore[arg-type] return materializer . handle_input ( output_data_type ) def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this artifact.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"type=' { self . _type } ', uri=' { self . _uri } ', \" f \"materializer=' { self . _materializer } ')\" ) def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same artifact.\"\"\" if isinstance ( other , ArtifactView ): return self . _id == other . _id and self . _uri == other . _uri return NotImplemented","title":"ArtifactView"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView.data_type","text":"Returns the data type of the artifact.","title":"data_type"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView.id","text":"Returns the artifact id.","title":"id"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView.is_cached","text":"Returns True if artifact was cached in a previous run, else False.","title":"is_cached"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView.parent_step_id","text":"Returns the ID of the parent step. This need not be equivalent to the ID of the producer step.","title":"parent_step_id"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView.producer_step","text":"Returns the original StepView that produced the artifact.","title":"producer_step"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView.type","text":"Returns the artifact type.","title":"type"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView.uri","text":"Returns the URI where the artifact data is stored.","title":"uri"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView.__eq__","text":"Returns whether the other object is referring to the same artifact. Source code in zenml/post_execution/artifact.py def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same artifact.\"\"\" if isinstance ( other , ArtifactView ): return self . _id == other . _id and self . _uri == other . _uri return NotImplemented","title":"__eq__()"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView.__init__","text":"Initializes a post-execution artifact object. In most cases ArtifactView objects should not be created manually but retrieved from a StepView via the inputs or outputs properties. Parameters: Name Type Description Default id_ int The artifact id. required type_ str The type of this artifact. required uri str Specifies where the artifact data is stored. required materializer str Information needed to restore the materializer that was used to write this artifact. required data_type str The type of data that was passed to the materializer when writing that artifact. Will be used as a default type to read the artifact. required metadata_store BaseMetadataStore The metadata store which should be used to fetch additional information related to this pipeline. required parent_step_id int The ID of the parent step. required Source code in zenml/post_execution/artifact.py def __init__ ( self , id_ : int , type_ : str , uri : str , materializer : str , data_type : str , metadata_store : \"BaseMetadataStore\" , parent_step_id : int , ): \"\"\"Initializes a post-execution artifact object. In most cases `ArtifactView` objects should not be created manually but retrieved from a `StepView` via the `inputs` or `outputs` properties. Args: id_: The artifact id. type_: The type of this artifact. uri: Specifies where the artifact data is stored. materializer: Information needed to restore the materializer that was used to write this artifact. data_type: The type of data that was passed to the materializer when writing that artifact. Will be used as a default type to read the artifact. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline. parent_step_id: The ID of the parent step. \"\"\" self . _id = id_ self . _type = type_ self . _uri = uri self . _materializer = materializer self . _data_type = data_type self . _metadata_store = metadata_store self . _parent_step_id = parent_step_id","title":"__init__()"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView.__repr__","text":"Returns a string representation of this artifact. Source code in zenml/post_execution/artifact.py def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this artifact.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"type=' { self . _type } ', uri=' { self . _uri } ', \" f \"materializer=' { self . _materializer } ')\" )","title":"__repr__()"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView.read","text":"Materializes the data stored in this artifact. Parameters: Name Type Description Default output_data_type Optional[Type[Any]] The datatype to which the materializer should read, will be passed to the materializers handle_input method. None materializer_class Optional[Type[BaseMaterializer]] The class of the materializer that should be used to read the artifact data. If no materializer class is given, we use the materializer that was used to write the artifact during execution of the pipeline. None Returns: Type Description Any The materialized data. Source code in zenml/post_execution/artifact.py def read ( self , output_data_type : Optional [ Type [ Any ]] = None , materializer_class : Optional [ Type [ \"BaseMaterializer\" ]] = None , ) -> Any : \"\"\"Materializes the data stored in this artifact. Args: output_data_type: The datatype to which the materializer should read, will be passed to the materializers `handle_input` method. materializer_class: The class of the materializer that should be used to read the artifact data. If no materializer class is given, we use the materializer that was used to write the artifact during execution of the pipeline. Returns: The materialized data. \"\"\" if not materializer_class : materializer_class = source_utils . load_source_path_class ( self . _materializer ) if not output_data_type : output_data_type = source_utils . load_source_path_class ( self . _data_type ) logger . debug ( \"Using ' %s ' to read ' %s ' (uri: %s ).\" , materializer_class . __qualname__ , self . _type , self . _uri , ) # TODO [ENG-162]: passing in `self` to initialize the materializer only # works because materializers only require a `.uri` property at the # moment. materializer = materializer_class ( self ) # type: ignore[arg-type] return materializer . handle_input ( output_data_type )","title":"read()"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline","text":"","title":"pipeline"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline.PipelineView","text":"Post-execution pipeline class which can be used to query pipeline-related information from the metadata store. Source code in zenml/post_execution/pipeline.py class PipelineView : \"\"\"Post-execution pipeline class which can be used to query pipeline-related information from the metadata store. \"\"\" def __init__ ( self , id_ : int , name : str , metadata_store : \"BaseMetadataStore\" ): \"\"\"Initializes a post-execution pipeline object. In most cases `PipelineView` objects should not be created manually but retrieved using the `get_pipelines()` method of a `zenml.core.repo.Repository` instead. Args: id_: The context id of this pipeline. name: The name of this pipeline. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline. \"\"\" self . _id = id_ self . _name = name self . _metadata_store = metadata_store @property def name ( self ) -> str : \"\"\"Returns the name of the pipeline.\"\"\" return self . _name @property def runs ( self ) -> List [ \"PipelineRunView\" ]: \"\"\"Returns all stored runs of this pipeline. The runs are returned in chronological order, so the latest run will be the last element in this list. \"\"\" # Do not cache runs as new runs might appear during this objects # lifecycle runs = self . _metadata_store . get_pipeline_runs ( self ) return list ( runs . values ()) def get_run_names ( self ) -> List [ str ]: \"\"\"Returns a list of all run names.\"\"\" # Do not cache runs as new runs might appear during this objects # lifecycle runs = self . _metadata_store . get_pipeline_runs ( self ) return list ( runs . keys ()) def get_run ( self , name : str ) -> \"PipelineRunView\" : \"\"\"Returns a run for the given name. Args: name: The name of the run to return. Raises: KeyError: If there is no run with the given name. \"\"\" run = self . _metadata_store . get_pipeline_run ( self , name ) if not run : raise KeyError ( f \"No run found for name ` { name } `. This pipeline \" f \"only has runs with the following \" f \"names: ` { self . get_run_names () } `\" ) return run def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this pipeline.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . _name } ')\" ) def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same pipeline.\"\"\" if isinstance ( other , PipelineView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented","title":"PipelineView"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline.PipelineView.name","text":"Returns the name of the pipeline.","title":"name"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline.PipelineView.runs","text":"Returns all stored runs of this pipeline. The runs are returned in chronological order, so the latest run will be the last element in this list.","title":"runs"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline.PipelineView.__eq__","text":"Returns whether the other object is referring to the same pipeline. Source code in zenml/post_execution/pipeline.py def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same pipeline.\"\"\" if isinstance ( other , PipelineView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented","title":"__eq__()"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline.PipelineView.__init__","text":"Initializes a post-execution pipeline object. In most cases PipelineView objects should not be created manually but retrieved using the get_pipelines() method of a zenml.core.repo.Repository instead. Parameters: Name Type Description Default id_ int The context id of this pipeline. required name str The name of this pipeline. required metadata_store BaseMetadataStore The metadata store which should be used to fetch additional information related to this pipeline. required Source code in zenml/post_execution/pipeline.py def __init__ ( self , id_ : int , name : str , metadata_store : \"BaseMetadataStore\" ): \"\"\"Initializes a post-execution pipeline object. In most cases `PipelineView` objects should not be created manually but retrieved using the `get_pipelines()` method of a `zenml.core.repo.Repository` instead. Args: id_: The context id of this pipeline. name: The name of this pipeline. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline. \"\"\" self . _id = id_ self . _name = name self . _metadata_store = metadata_store","title":"__init__()"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline.PipelineView.__repr__","text":"Returns a string representation of this pipeline. Source code in zenml/post_execution/pipeline.py def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this pipeline.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . _name } ')\" )","title":"__repr__()"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline.PipelineView.get_run","text":"Returns a run for the given name. Parameters: Name Type Description Default name str The name of the run to return. required Exceptions: Type Description KeyError If there is no run with the given name. Source code in zenml/post_execution/pipeline.py def get_run ( self , name : str ) -> \"PipelineRunView\" : \"\"\"Returns a run for the given name. Args: name: The name of the run to return. Raises: KeyError: If there is no run with the given name. \"\"\" run = self . _metadata_store . get_pipeline_run ( self , name ) if not run : raise KeyError ( f \"No run found for name ` { name } `. This pipeline \" f \"only has runs with the following \" f \"names: ` { self . get_run_names () } `\" ) return run","title":"get_run()"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline.PipelineView.get_run_names","text":"Returns a list of all run names. Source code in zenml/post_execution/pipeline.py def get_run_names ( self ) -> List [ str ]: \"\"\"Returns a list of all run names.\"\"\" # Do not cache runs as new runs might appear during this objects # lifecycle runs = self . _metadata_store . get_pipeline_runs ( self ) return list ( runs . keys ())","title":"get_run_names()"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline_run","text":"","title":"pipeline_run"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline_run.PipelineRunView","text":"Post-execution pipeline run class which can be used to query steps and artifact information associated with a pipeline execution. Source code in zenml/post_execution/pipeline_run.py class PipelineRunView : \"\"\"Post-execution pipeline run class which can be used to query steps and artifact information associated with a pipeline execution. \"\"\" def __init__ ( self , id_ : int , name : str , executions : List [ proto . Execution ], metadata_store : \"BaseMetadataStore\" , ): \"\"\"Initializes a post-execution pipeline run object. In most cases `PipelineRunView` objects should not be created manually but retrieved from a `PipelineView` object instead. Args: id_: The context id of this pipeline run. name: The name of this pipeline run. executions: All executions associated with this pipeline run. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline run. \"\"\" self . _id = id_ self . _name = name self . _metadata_store = metadata_store self . _executions = executions self . _steps : Dict [ str , StepView ] = OrderedDict () @property def name ( self ) -> str : \"\"\"Returns the name of the pipeline run.\"\"\" return self . _name @property def status ( self ) -> ExecutionStatus : \"\"\"Returns the current status of the pipeline run.\"\"\" step_statuses = ( step . status for step in self . steps ) if any ( status == ExecutionStatus . FAILED for status in step_statuses ): return ExecutionStatus . FAILED elif all ( status == ExecutionStatus . COMPLETED or status == ExecutionStatus . CACHED for status in step_statuses ): return ExecutionStatus . COMPLETED else : return ExecutionStatus . RUNNING @property def steps ( self ) -> List [ StepView ]: \"\"\"Returns all steps that were executed as part of this pipeline run.\"\"\" self . _ensure_steps_fetched () return list ( self . _steps . values ()) def get_step_names ( self ) -> List [ str ]: \"\"\"Returns a list of all step names.\"\"\" self . _ensure_steps_fetched () return list ( self . _steps . keys ()) def get_step ( self , name : str ) -> StepView : \"\"\"Returns a step for the given name. Args: name: The name of the step to return. Raises: KeyError: If there is no step with the given name. \"\"\" self . _ensure_steps_fetched () try : return self . _steps [ name ] except KeyError : raise KeyError ( f \"No step found for name ` { name } `. This pipeline \" f \"run only has steps with the following \" f \"names: ` { self . get_step_names () } `\" ) def _ensure_steps_fetched ( self ) -> None : \"\"\"Fetches all steps for this pipeline run from the metadata store.\"\"\" if self . _steps : # we already fetched the steps, no need to do anything return self . _steps = self . _metadata_store . get_pipeline_run_steps ( self ) def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this pipeline run.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . _name } ')\" ) def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same pipeline run.\"\"\" if isinstance ( other , PipelineRunView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented","title":"PipelineRunView"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline_run.PipelineRunView.name","text":"Returns the name of the pipeline run.","title":"name"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline_run.PipelineRunView.status","text":"Returns the current status of the pipeline run.","title":"status"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline_run.PipelineRunView.steps","text":"Returns all steps that were executed as part of this pipeline run.","title":"steps"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline_run.PipelineRunView.__eq__","text":"Returns whether the other object is referring to the same pipeline run. Source code in zenml/post_execution/pipeline_run.py def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same pipeline run.\"\"\" if isinstance ( other , PipelineRunView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented","title":"__eq__()"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline_run.PipelineRunView.__init__","text":"Initializes a post-execution pipeline run object. In most cases PipelineRunView objects should not be created manually but retrieved from a PipelineView object instead. Parameters: Name Type Description Default id_ int The context id of this pipeline run. required name str The name of this pipeline run. required executions List[ml_metadata.proto.metadata_store_pb2.Execution] All executions associated with this pipeline run. required metadata_store BaseMetadataStore The metadata store which should be used to fetch additional information related to this pipeline run. required Source code in zenml/post_execution/pipeline_run.py def __init__ ( self , id_ : int , name : str , executions : List [ proto . Execution ], metadata_store : \"BaseMetadataStore\" , ): \"\"\"Initializes a post-execution pipeline run object. In most cases `PipelineRunView` objects should not be created manually but retrieved from a `PipelineView` object instead. Args: id_: The context id of this pipeline run. name: The name of this pipeline run. executions: All executions associated with this pipeline run. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline run. \"\"\" self . _id = id_ self . _name = name self . _metadata_store = metadata_store self . _executions = executions self . _steps : Dict [ str , StepView ] = OrderedDict ()","title":"__init__()"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline_run.PipelineRunView.__repr__","text":"Returns a string representation of this pipeline run. Source code in zenml/post_execution/pipeline_run.py def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this pipeline run.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . _name } ')\" )","title":"__repr__()"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline_run.PipelineRunView.get_step","text":"Returns a step for the given name. Parameters: Name Type Description Default name str The name of the step to return. required Exceptions: Type Description KeyError If there is no step with the given name. Source code in zenml/post_execution/pipeline_run.py def get_step ( self , name : str ) -> StepView : \"\"\"Returns a step for the given name. Args: name: The name of the step to return. Raises: KeyError: If there is no step with the given name. \"\"\" self . _ensure_steps_fetched () try : return self . _steps [ name ] except KeyError : raise KeyError ( f \"No step found for name ` { name } `. This pipeline \" f \"run only has steps with the following \" f \"names: ` { self . get_step_names () } `\" )","title":"get_step()"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline_run.PipelineRunView.get_step_names","text":"Returns a list of all step names. Source code in zenml/post_execution/pipeline_run.py def get_step_names ( self ) -> List [ str ]: \"\"\"Returns a list of all step names.\"\"\" self . _ensure_steps_fetched () return list ( self . _steps . keys ())","title":"get_step_names()"},{"location":"api_docs/post_execution/#zenml.post_execution.step","text":"","title":"step"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView","text":"Post-execution step class which can be used to query artifact information associated with a pipeline step. Source code in zenml/post_execution/step.py class StepView : \"\"\"Post-execution step class which can be used to query artifact information associated with a pipeline step. \"\"\" def __init__ ( self , id_ : int , parents_step_ids : List [ int ], entrypoint_name : str , name : str , parameters : Dict [ str , Any ], metadata_store : \"BaseMetadataStore\" , ): \"\"\"Initializes a post-execution step object. In most cases `StepView` objects should not be created manually but retrieved from a `PipelineRunView` object instead. Args: id_: The execution id of this step. parents_step_ids: The execution ids of the parents of this step. entrypoint_name: The name of this step. name: The name of this step within the pipeline parameters: Parameters that were used to run this step. metadata_store: The metadata store which should be used to fetch additional information related to this step. \"\"\" self . _id = id_ self . _parents_step_ids = parents_step_ids self . _entrypoint_name = entrypoint_name self . _name = name self . _parameters = parameters self . _metadata_store = metadata_store self . _inputs : Dict [ str , ArtifactView ] = {} self . _outputs : Dict [ str , ArtifactView ] = {} @property def id ( self ) -> int : \"\"\"Returns the step id.\"\"\" return self . _id @property def parents_step_ids ( self ) -> List [ int ]: \"\"\"Returns a list of ID's of all parents of this step.\"\"\" return self . _parents_step_ids @property def parent_steps ( self ) -> List [ \"StepView\" ]: \"\"\"Returns a list of all parent steps of this step.\"\"\" steps = [ self . _metadata_store . get_step_by_id ( s ) for s in self . parents_step_ids ] return steps @property def entrypoint_name ( self ) -> str : \"\"\"Returns the step entrypoint_name. This name is equal to the name argument passed to the @step decorator or the actual function name if no explicit name was given. Examples: # the step entrypoint_name will be \"my_step\" @step(name=\"my_step\") def my_step_function(...) # the step entrypoint_name will be \"my_step_function\" @step def my_step_function(...) \"\"\" return self . _entrypoint_name @property def name ( self ) -> str : \"\"\"Returns the name as it is defined in the pipeline. This name is equal to the name given to the step within the pipeline context Examples: @step() def my_step_function(...) @pipeline def my_pipeline_function(step_a) p = my_pipeline_function( step_a = my_step_function() ) The name will be `step_a` \"\"\" return self . _name @property def parameters ( self ) -> Dict [ str , Any ]: \"\"\"The parameters used to run this step.\"\"\" return self . _parameters @property def status ( self ) -> ExecutionStatus : \"\"\"Returns the current status of the step.\"\"\" return self . _metadata_store . get_step_status ( self ) @property def is_cached ( self ) -> bool : \"\"\"Returns whether the step is cached or not.\"\"\" return self . status == ExecutionStatus . CACHED @property def inputs ( self ) -> Dict [ str , ArtifactView ]: \"\"\"Returns all input artifacts that were used to run this step.\"\"\" self . _ensure_inputs_outputs_fetched () return self . _inputs @property def input ( self ) -> ArtifactView : \"\"\"Returns the input artifact that was used to run this step. Raises: ValueError: If there were zero or multiple inputs to this step. \"\"\" if len ( self . inputs ) != 1 : raise ValueError ( \"Can't use the `StepView.input` property for steps with zero \" \"or multiple inputs, use `StepView.inputs` instead.\" ) return next ( iter ( self . inputs . values ())) @property def outputs ( self ) -> Dict [ str , ArtifactView ]: \"\"\"Returns all output artifacts that were written by this step.\"\"\" self . _ensure_inputs_outputs_fetched () return self . _outputs @property def output ( self ) -> ArtifactView : \"\"\"Returns the output artifact that was written by this step. Raises: ValueError: If there were zero or multiple step outputs. \"\"\" if len ( self . outputs ) != 1 : raise ValueError ( \"Can't use the `StepView.output` property for steps with zero \" \"or multiple outputs, use `StepView.outputs` instead.\" ) return next ( iter ( self . outputs . values ())) def _ensure_inputs_outputs_fetched ( self ) -> None : \"\"\"Fetches all step inputs and outputs from the metadata store.\"\"\" if self . _inputs or self . _outputs : # we already fetched inputs/outputs, no need to do anything return self . _inputs , self . _outputs = self . _metadata_store . get_step_artifacts ( self ) def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this step.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . name } ', entrypoint_name=' { self . entrypoint_name } '\" f \"parameters= { self . _parameters } )\" ) def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same step.\"\"\" if isinstance ( other , StepView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented","title":"StepView"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.entrypoint_name","text":"Returns the step entrypoint_name. This name is equal to the name argument passed to the @step decorator or the actual function name if no explicit name was given. Examples:","title":"entrypoint_name"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.entrypoint_name--the-step-entrypoint_name-will-be-my_step","text":"@step(name=\"my_step\") def my_step_function(...)","title":"the step entrypoint_name will be \"my_step\""},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.entrypoint_name--the-step-entrypoint_name-will-be-my_step_function","text":"@step def my_step_function(...)","title":"the step entrypoint_name will be \"my_step_function\""},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.id","text":"Returns the step id.","title":"id"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.input","text":"Returns the input artifact that was used to run this step. Exceptions: Type Description ValueError If there were zero or multiple inputs to this step.","title":"input"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.inputs","text":"Returns all input artifacts that were used to run this step.","title":"inputs"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.is_cached","text":"Returns whether the step is cached or not.","title":"is_cached"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.name","text":"Returns the name as it is defined in the pipeline. This name is equal to the name given to the step within the pipeline context Examples: @step() def my_step_function(...) @pipeline def my_pipeline_function(step_a) p = my_pipeline_function( step_a = my_step_function() ) The name will be step_a","title":"name"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.output","text":"Returns the output artifact that was written by this step. Exceptions: Type Description ValueError If there were zero or multiple step outputs.","title":"output"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.outputs","text":"Returns all output artifacts that were written by this step.","title":"outputs"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.parameters","text":"The parameters used to run this step.","title":"parameters"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.parent_steps","text":"Returns a list of all parent steps of this step.","title":"parent_steps"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.parents_step_ids","text":"Returns a list of ID's of all parents of this step.","title":"parents_step_ids"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.status","text":"Returns the current status of the step.","title":"status"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.__eq__","text":"Returns whether the other object is referring to the same step. Source code in zenml/post_execution/step.py def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same step.\"\"\" if isinstance ( other , StepView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented","title":"__eq__()"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.__init__","text":"Initializes a post-execution step object. In most cases StepView objects should not be created manually but retrieved from a PipelineRunView object instead. Parameters: Name Type Description Default id_ int The execution id of this step. required parents_step_ids List[int] The execution ids of the parents of this step. required entrypoint_name str The name of this step. required name str The name of this step within the pipeline required parameters Dict[str, Any] Parameters that were used to run this step. required metadata_store BaseMetadataStore The metadata store which should be used to fetch additional information related to this step. required Source code in zenml/post_execution/step.py def __init__ ( self , id_ : int , parents_step_ids : List [ int ], entrypoint_name : str , name : str , parameters : Dict [ str , Any ], metadata_store : \"BaseMetadataStore\" , ): \"\"\"Initializes a post-execution step object. In most cases `StepView` objects should not be created manually but retrieved from a `PipelineRunView` object instead. Args: id_: The execution id of this step. parents_step_ids: The execution ids of the parents of this step. entrypoint_name: The name of this step. name: The name of this step within the pipeline parameters: Parameters that were used to run this step. metadata_store: The metadata store which should be used to fetch additional information related to this step. \"\"\" self . _id = id_ self . _parents_step_ids = parents_step_ids self . _entrypoint_name = entrypoint_name self . _name = name self . _parameters = parameters self . _metadata_store = metadata_store self . _inputs : Dict [ str , ArtifactView ] = {} self . _outputs : Dict [ str , ArtifactView ] = {}","title":"__init__()"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.__repr__","text":"Returns a string representation of this step. Source code in zenml/post_execution/step.py def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this step.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . name } ', entrypoint_name=' { self . entrypoint_name } '\" f \"parameters= { self . _parameters } )\" )","title":"__repr__()"},{"location":"api_docs/repository/","text":"Repository zenml.repository Repository ZenML repository class. ZenML repositories store configuration options for ZenML stacks as well as their components. Source code in zenml/repository.py class Repository : \"\"\"ZenML repository class. ZenML repositories store configuration options for ZenML stacks as well as their components. \"\"\" def __init__ ( self , root : Optional [ Path ] = None ): \"\"\"Initializes a repository instance. Args: root: Optional root directory of the repository. If no path is given, this function tries to find the repository using the environment variable `ZENML_REPOSITORY_PATH` (if set) and recursively searching in the parent directories of the current working directory. Raises: RepositoryNotFoundError: If no ZenML repository directory is found. \"\"\" self . _root = Repository . find_repository ( root ) # load the repository configuration file if it exists, otherwise use # an empty configuration as default config_path = self . _config_path () if fileio . file_exists ( config_path ): config_dict = yaml_utils . read_yaml ( config_path ) self . __config = RepositoryConfiguration . parse_obj ( config_dict ) else : self . __config = RepositoryConfiguration . empty_configuration () if self . version != zenml . __version__ : # TODO [ENG-366]: Create compatibility table so we don't have to # warn about mismatching repository and ZenML version each time logger . warning ( \"This ZenML repository was created with a different version \" \"of ZenML (Repository version: %s , current ZenML version: %s ). \" \"In case you encounter any errors, please delete and \" \"reinitialize this repository.\" , self . version , zenml . __version__ , ) def _config_path ( self ) -> str : \"\"\"Path to the repository configuration file.\"\"\" return str ( self . config_directory / \"config.yaml\" ) def _get_stack_component_config_path ( self , component_type : StackComponentType , name : str ) -> str : \"\"\"Path to the configuration file of a stack component.\"\"\" path = self . config_directory / component_type . plural / f \" { name } .yaml\" return str ( path ) def _write_config ( self ) -> None : \"\"\"Writes the repository configuration file.\"\"\" config_dict = json . loads ( self . __config . json ()) yaml_utils . write_yaml ( self . _config_path (), config_dict ) @staticmethod @track ( event = INITIALIZE_REPO ) def initialize ( root : Path = Path . cwd ()) -> None : \"\"\"Initializes a new ZenML repository at the given path. The newly created repository will contain a single stack with a local orchestrator, a local artifact store and a local SQLite metadata store. Args: root: The root directory where the repository should be created. Raises: InitializationException: If the root directory already contains a ZenML repository. \"\"\" logger . debug ( \"Initializing new repository at path %s .\" , root ) if Repository . is_repository_directory ( root ): raise InitializationException ( f \"Found existing ZenML repository at path ' { root } '.\" ) config_directory = str ( root / LOCAL_CONFIG_DIRECTORY_NAME ) fileio . create_dir_recursive_if_not_exists ( config_directory ) # register and activate a local stack repo = Repository ( root = root ) stack = Stack . default_local_stack () repo . register_stack ( stack ) repo . activate_stack ( stack . name ) @property def version ( self ) -> str : \"\"\"The version of the repository.\"\"\" return self . __config . version @property def root ( self ) -> Path : \"\"\"The root directory of this repository.\"\"\" return self . _root @property def config_directory ( self ) -> Path : \"\"\"The configuration directory of this repository.\"\"\" return self . root / LOCAL_CONFIG_DIRECTORY_NAME @property def stacks ( self ) -> List [ Stack ]: \"\"\"All stacks registered in this repository.\"\"\" return [ self . get_stack ( name = name ) for name in self . __config . stacks ] @property def stack_configurations ( self ) -> Dict [ str , StackConfiguration ]: \"\"\"Configuration objects for all stacks registered in this repository. This property is intended as a quick way to get information about the components of the registered stacks without loading all installed integrations. The contained stack configurations might be invalid if they were modified by hand, to ensure you get valid stacks use `repo.stacks()` instead. Modifying the contents of the returned dictionary does not actually register/deregister stacks, use `repo.register_stack(...)` or `repo.deregister_stack(...)` instead. \"\"\" return self . __config . stacks . copy () @property def active_stack ( self ) -> Stack : \"\"\"The active stack for this repository. Raises: RuntimeError: If no active stack name is configured. KeyError: If no stack was found for the configured name or one of the stack components is not registered. \"\"\" if not self . __config . active_stack_name : raise RuntimeError ( \"No active stack name configured. Run \" \"`zenml stack set STACK_NAME` to update the active stack.\" ) return self . get_stack ( name = self . __config . active_stack_name ) @property def active_stack_name ( self ) -> str : \"\"\"The name of the active stack for this repository. Raises: RuntimeError: If no active stack name is configured. \"\"\" if not self . __config . active_stack_name : raise RuntimeError ( \"No active stack name configured. Run \" \"`zenml stack set STACK_NAME` to update the active stack.\" ) return self . __config . active_stack_name # TODO [ENG-367]: Should we replace the stack name by the actual stack # object? It would be more consistent with the rest of the API but # requires some additional care (checking if the stack + components are # actually registered in this repository). Downside: We would need to # load all the integrations to create the stack object which makes the CLI # command to set the active stack much slower. @track ( event = SET_STACK ) def activate_stack ( self , name : str ) -> None : \"\"\"Activates the stack for the given name. Args: name: Name of the stack to activate. Raises: KeyError: If no stack exists for the given name. \"\"\" if name not in self . __config . stacks : raise KeyError ( f \"Unable to find stack for name ' { name } '.\" ) self . __config . active_stack_name = name self . _write_config () def get_stack ( self , name : str ) -> Stack : \"\"\"Fetches a stack. Args: name: The name of the stack to fetch. Raises: KeyError: If no stack exists for the given name or one of the stacks components is not registered. \"\"\" logger . debug ( \"Fetching stack with name ' %s '.\" , name ) if name not in self . __config . stacks : raise KeyError ( f \"Unable to find stack with name ' { name } '. Available names: \" f \" { set ( self . __config . stacks ) } .\" ) stack_configuration = self . __config . stacks [ name ] stack_components = {} for ( component_type_name , component_name , ) in stack_configuration . dict () . items (): component_type = StackComponentType ( component_type_name ) if not component_name : # optional component which is not set, continue continue component = self . get_stack_component ( component_type = component_type , name = component_name , ) stack_components [ component_type ] = component return Stack . from_components ( name = name , components = stack_components ) @track ( event = REGISTERED_STACK ) def register_stack ( self , stack : Stack ) -> None : \"\"\"Registers a stack and it's components. If any of the stacks' components aren't registered in the repository yet, this method will try to register them as well. Args: stack: The stack to register. Raises: StackExistsError: If a stack with the same name already exists. StackComponentExistsError: If a component of the stack wasn't registered and a different component with the same name already exists. \"\"\" if stack . name in self . __config . stacks : raise StackExistsError ( f \"Unable to register stack with name ' { stack . name } ': Found \" f \"existing stack with this name.\" ) components = {} for component_type , component in stack . components . items (): try : existing_component = self . get_stack_component ( component_type = component_type , name = component . name ) if existing_component . uuid != component . uuid : raise StackComponentExistsError ( f \"Unable to register one of the stacks components: \" f \"A component of type ' { component_type } ' and name \" f \"' { component . name } ' already exists.\" ) except KeyError : # a component of the stack isn't registered yet -> register it self . register_stack_component ( component ) components [ component_type . value ] = component . name stack_configuration = StackConfiguration ( ** components ) self . __config . stacks [ stack . name ] = stack_configuration self . _write_config () logger . info ( \"Registered stack with name ' %s '.\" , stack . name ) def deregister_stack ( self , name : str ) -> None : \"\"\"Deregisters a stack. Args: name: The name of the stack to deregister. Raises: ValueError: If the stack is the currently active stack for this repository. \"\"\" if name == self . active_stack_name : raise ValueError ( f \"Unable to deregister active stack ' { name } '.\" ) try : del self . __config . stacks [ name ] self . _write_config () logger . info ( \"Deregistered stack with name ' %s '.\" , name ) except KeyError : logger . warning ( \"Unable to deregister stack with name ' %s ': No stack exists \" \"with this name.\" , name , ) def get_stack_components ( self , component_type : StackComponentType ) -> List [ StackComponent ]: \"\"\"Fetches all registered stack components of the given type.\"\"\" component_names = self . __config . stack_components [ component_type ] . keys () return [ self . get_stack_component ( component_type = component_type , name = name ) for name in component_names ] def get_stack_component ( self , component_type : StackComponentType , name : str ) -> StackComponent : \"\"\"Fetches a registered stack component. Args: component_type: The type of the component to fetch. name: The name of the component to fetch. Raises: KeyError: If no stack component exists for the given type and name. \"\"\" logger . debug ( \"Fetching stack component of type ' %s ' with name ' %s '.\" , component_type . value , name , ) components = self . __config . stack_components [ component_type ] if name not in components : raise KeyError ( f \"Unable to find stack component (type: { component_type } ) \" f \"with name ' { name } '. Available names: { set ( components ) } .\" ) from zenml.stack.stack_component_class_registry import ( StackComponentClassRegistry , ) component_flavor = components [ name ] component_class = StackComponentClassRegistry . get_class ( component_type = component_type , component_flavor = component_flavor ) component_config_path = self . _get_stack_component_config_path ( component_type = component_type , name = name ) component_config = yaml_utils . read_yaml ( component_config_path ) return component_class . parse_obj ( component_config ) def register_stack_component ( self , component : StackComponent , ) -> None : \"\"\"Registers a stack component. Args: component: The component to register. Raises: StackComponentExistsError: If a stack component with the same type and name already exists. \"\"\" components = self . __config . stack_components [ component . type ] if component . name in components : raise StackComponentExistsError ( f \"Unable to register stack component (type: { component . type } ) \" f \"with name ' { component . name } ': Found existing stack component \" f \"with this name.\" ) # write the component configuration file component_config_path = self . _get_stack_component_config_path ( component_type = component . type , name = component . name ) fileio . create_dir_recursive_if_not_exists ( os . path . dirname ( component_config_path ) ) yaml_utils . write_yaml ( component_config_path , json . loads ( component . json ()) ) # add the component to the repository configuration and write it to disk components [ component . name ] = component . flavor . value self . _write_config () logger . info ( \"Registered stack component with name ' %s '.\" , component . name ) analytics_metadata = { \"type\" : component . type . value , \"flavor\" : component . flavor . value , } track_event ( REGISTERED_STACK_COMPONENT , metadata = analytics_metadata ) def deregister_stack_component ( self , component_type : StackComponentType , name : str ) -> None : \"\"\"Deregisters a stack component. Args: component_type: The type of the component to deregister. name: The name of the component to deregister. \"\"\" for stack_name , stack_config in self . stack_configurations . items (): if stack_config . contains_component ( component_type = component_type , name = name ): raise ValueError ( f \"Unable to deregister stack component (type: \" f \" { component_type } , name: { name } ) that is part of a \" f \"registered stack (stack name: ' { stack_name } ').\" ) components = self . __config . stack_components [ component_type ] try : del components [ name ] self . _write_config () logger . info ( \"Deregistered stack component (type: %s ) with name ' %s '.\" , component_type . value , name , ) except KeyError : logger . warning ( \"Unable to deregister stack component (type: %s ) with name \" \"' %s ': No stack component exists with this name.\" , component_type . value , name , ) component_config_path = self . _get_stack_component_config_path ( component_type = component_type , name = name ) if fileio . file_exists ( component_config_path ): fileio . remove ( component_config_path ) # TODO [ENG-368]: Discuss whether we want to unify these two methods. @track ( event = GET_PIPELINES ) def get_pipelines ( self , stack_name : Optional [ str ] = None ) -> List [ PipelineView ]: \"\"\"Fetches post-execution pipeline views. Args: stack_name: If specified, pipelines in the metadata store of the given stack are returned. Otherwise pipelines in the metadata store of the currently active stack are returned. Returns: A list of post-execution pipeline views. Raises: KeyError: If no stack with the given name exists. \"\"\" stack_name = stack_name or self . active_stack_name metadata_store = self . get_stack ( stack_name ) . metadata_store return metadata_store . get_pipelines () @track ( event = GET_PIPELINE ) def get_pipeline ( self , pipeline_name : str , stack_name : Optional [ str ] = None ) -> Optional [ PipelineView ]: \"\"\"Fetches a post-execution pipeline view. Args: pipeline_name: Name of the pipeline. stack_name: If specified, pipelines in the metadata store of the given stack are returned. Otherwise pipelines in the metadata store of the currently active stack are returned. Returns: A post-execution pipeline view for the given name or `None` if it doesn't exist. Raises: KeyError: If no stack with the given name exists. \"\"\" stack_name = stack_name or self . active_stack_name metadata_store = self . get_stack ( stack_name ) . metadata_store return metadata_store . get_pipeline ( pipeline_name ) @staticmethod def is_repository_directory ( path : Path ) -> bool : \"\"\"Checks whether a ZenML repository exists at the given path.\"\"\" config_dir = path / LOCAL_CONFIG_DIRECTORY_NAME return fileio . is_dir ( str ( config_dir )) @staticmethod def find_repository ( path : Optional [ Path ] = None ) -> Path : \"\"\"Finds path of a ZenML repository directory. Args: path: Optional path to look for the repository. If no path is given, this function tries to find the repository using the environment variable `ZENML_REPOSITORY_PATH` (if set) and recursively searching in the parent directories of the current working directory. Returns: Absolute path to a ZenML repository directory. Raises: RepositoryNotFoundError: If no ZenML repository is found. \"\"\" if not path : # try to get path from the environment variable env_var_path = os . getenv ( ENV_ZENML_REPOSITORY_PATH ) if env_var_path : path = Path ( env_var_path ) if path : # explicit path via parameter or environment variable, don't search # parent directories search_parent_directories = False error_message = ( f \"Unable to find ZenML repository at path ' { path } '. Make sure \" f \"to create a ZenML repository by calling `zenml init` when \" f \"specifying an explicit repository path in code or via the \" f \"environment variable ' { ENV_ZENML_REPOSITORY_PATH } '.\" ) else : # try to find the repo in the parent directories of the current # working directory path = Path . cwd () search_parent_directories = True error_message = ( f \"Unable to find ZenML repository in your current working \" f \"directory ( { path } ) or any parent directories. If you \" f \"want to use an existing repository which is in a different \" f \"location, set the environment variable \" f \"' { ENV_ZENML_REPOSITORY_PATH } '. If you want to create a new \" f \"repository, run `zenml init`.\" ) def _find_repo_helper ( path_ : Path ) -> Path : \"\"\"Helper function to recursively search parent directories for a ZenML repository.\"\"\" if Repository . is_repository_directory ( path_ ): return path_ if not search_parent_directories or fileio . is_root ( str ( path_ )): raise RepositoryNotFoundError ( error_message ) return _find_repo_helper ( path_ . parent ) return _find_repo_helper ( path ) . resolve () active_stack : Stack property readonly The active stack for this repository. Exceptions: Type Description RuntimeError If no active stack name is configured. KeyError If no stack was found for the configured name or one of the stack components is not registered. active_stack_name : str property readonly The name of the active stack for this repository. Exceptions: Type Description RuntimeError If no active stack name is configured. config_directory : Path property readonly The configuration directory of this repository. root : Path property readonly The root directory of this repository. stack_configurations : Dict [ str , zenml . repository . StackConfiguration ] property readonly Configuration objects for all stacks registered in this repository. This property is intended as a quick way to get information about the components of the registered stacks without loading all installed integrations. The contained stack configurations might be invalid if they were modified by hand, to ensure you get valid stacks use repo.stacks() instead. Modifying the contents of the returned dictionary does not actually register/deregister stacks, use repo.register_stack(...) or repo.deregister_stack(...) instead. stacks : List [ zenml . stack . stack . Stack ] property readonly All stacks registered in this repository. version : str property readonly The version of the repository. __init__ ( self , root = None ) special Initializes a repository instance. Parameters: Name Type Description Default root Optional[pathlib.Path] Optional root directory of the repository. If no path is given, this function tries to find the repository using the environment variable ZENML_REPOSITORY_PATH (if set) and recursively searching in the parent directories of the current working directory. None Exceptions: Type Description RepositoryNotFoundError If no ZenML repository directory is found. Source code in zenml/repository.py def __init__ ( self , root : Optional [ Path ] = None ): \"\"\"Initializes a repository instance. Args: root: Optional root directory of the repository. If no path is given, this function tries to find the repository using the environment variable `ZENML_REPOSITORY_PATH` (if set) and recursively searching in the parent directories of the current working directory. Raises: RepositoryNotFoundError: If no ZenML repository directory is found. \"\"\" self . _root = Repository . find_repository ( root ) # load the repository configuration file if it exists, otherwise use # an empty configuration as default config_path = self . _config_path () if fileio . file_exists ( config_path ): config_dict = yaml_utils . read_yaml ( config_path ) self . __config = RepositoryConfiguration . parse_obj ( config_dict ) else : self . __config = RepositoryConfiguration . empty_configuration () if self . version != zenml . __version__ : # TODO [ENG-366]: Create compatibility table so we don't have to # warn about mismatching repository and ZenML version each time logger . warning ( \"This ZenML repository was created with a different version \" \"of ZenML (Repository version: %s , current ZenML version: %s ). \" \"In case you encounter any errors, please delete and \" \"reinitialize this repository.\" , self . version , zenml . __version__ , ) activate_stack ( * args , ** kwargs ) Inner decorator function. Source code in zenml/repository.py def inner_func ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function.\"\"\" track_event ( event_name , metadata = metadata ) result = func ( * args , ** kwargs ) return result deregister_stack ( self , name ) Deregisters a stack. Parameters: Name Type Description Default name str The name of the stack to deregister. required Exceptions: Type Description ValueError If the stack is the currently active stack for this repository. Source code in zenml/repository.py def deregister_stack ( self , name : str ) -> None : \"\"\"Deregisters a stack. Args: name: The name of the stack to deregister. Raises: ValueError: If the stack is the currently active stack for this repository. \"\"\" if name == self . active_stack_name : raise ValueError ( f \"Unable to deregister active stack ' { name } '.\" ) try : del self . __config . stacks [ name ] self . _write_config () logger . info ( \"Deregistered stack with name ' %s '.\" , name ) except KeyError : logger . warning ( \"Unable to deregister stack with name ' %s ': No stack exists \" \"with this name.\" , name , ) deregister_stack_component ( self , component_type , name ) Deregisters a stack component. Parameters: Name Type Description Default component_type StackComponentType The type of the component to deregister. required name str The name of the component to deregister. required Source code in zenml/repository.py def deregister_stack_component ( self , component_type : StackComponentType , name : str ) -> None : \"\"\"Deregisters a stack component. Args: component_type: The type of the component to deregister. name: The name of the component to deregister. \"\"\" for stack_name , stack_config in self . stack_configurations . items (): if stack_config . contains_component ( component_type = component_type , name = name ): raise ValueError ( f \"Unable to deregister stack component (type: \" f \" { component_type } , name: { name } ) that is part of a \" f \"registered stack (stack name: ' { stack_name } ').\" ) components = self . __config . stack_components [ component_type ] try : del components [ name ] self . _write_config () logger . info ( \"Deregistered stack component (type: %s ) with name ' %s '.\" , component_type . value , name , ) except KeyError : logger . warning ( \"Unable to deregister stack component (type: %s ) with name \" \"' %s ': No stack component exists with this name.\" , component_type . value , name , ) component_config_path = self . _get_stack_component_config_path ( component_type = component_type , name = name ) if fileio . file_exists ( component_config_path ): fileio . remove ( component_config_path ) find_repository ( path = None ) staticmethod Finds path of a ZenML repository directory. Parameters: Name Type Description Default path Optional[pathlib.Path] Optional path to look for the repository. If no path is given, this function tries to find the repository using the environment variable ZENML_REPOSITORY_PATH (if set) and recursively searching in the parent directories of the current working directory. None Returns: Type Description Path Absolute path to a ZenML repository directory. Exceptions: Type Description RepositoryNotFoundError If no ZenML repository is found. Source code in zenml/repository.py @staticmethod def find_repository ( path : Optional [ Path ] = None ) -> Path : \"\"\"Finds path of a ZenML repository directory. Args: path: Optional path to look for the repository. If no path is given, this function tries to find the repository using the environment variable `ZENML_REPOSITORY_PATH` (if set) and recursively searching in the parent directories of the current working directory. Returns: Absolute path to a ZenML repository directory. Raises: RepositoryNotFoundError: If no ZenML repository is found. \"\"\" if not path : # try to get path from the environment variable env_var_path = os . getenv ( ENV_ZENML_REPOSITORY_PATH ) if env_var_path : path = Path ( env_var_path ) if path : # explicit path via parameter or environment variable, don't search # parent directories search_parent_directories = False error_message = ( f \"Unable to find ZenML repository at path ' { path } '. Make sure \" f \"to create a ZenML repository by calling `zenml init` when \" f \"specifying an explicit repository path in code or via the \" f \"environment variable ' { ENV_ZENML_REPOSITORY_PATH } '.\" ) else : # try to find the repo in the parent directories of the current # working directory path = Path . cwd () search_parent_directories = True error_message = ( f \"Unable to find ZenML repository in your current working \" f \"directory ( { path } ) or any parent directories. If you \" f \"want to use an existing repository which is in a different \" f \"location, set the environment variable \" f \"' { ENV_ZENML_REPOSITORY_PATH } '. If you want to create a new \" f \"repository, run `zenml init`.\" ) def _find_repo_helper ( path_ : Path ) -> Path : \"\"\"Helper function to recursively search parent directories for a ZenML repository.\"\"\" if Repository . is_repository_directory ( path_ ): return path_ if not search_parent_directories or fileio . is_root ( str ( path_ )): raise RepositoryNotFoundError ( error_message ) return _find_repo_helper ( path_ . parent ) return _find_repo_helper ( path ) . resolve () get_pipeline ( * args , ** kwargs ) Inner decorator function. Source code in zenml/repository.py def inner_func ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function.\"\"\" track_event ( event_name , metadata = metadata ) result = func ( * args , ** kwargs ) return result get_pipelines ( * args , ** kwargs ) Inner decorator function. Source code in zenml/repository.py def inner_func ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function.\"\"\" track_event ( event_name , metadata = metadata ) result = func ( * args , ** kwargs ) return result get_stack ( self , name ) Fetches a stack. Parameters: Name Type Description Default name str The name of the stack to fetch. required Exceptions: Type Description KeyError If no stack exists for the given name or one of the stacks components is not registered. Source code in zenml/repository.py def get_stack ( self , name : str ) -> Stack : \"\"\"Fetches a stack. Args: name: The name of the stack to fetch. Raises: KeyError: If no stack exists for the given name or one of the stacks components is not registered. \"\"\" logger . debug ( \"Fetching stack with name ' %s '.\" , name ) if name not in self . __config . stacks : raise KeyError ( f \"Unable to find stack with name ' { name } '. Available names: \" f \" { set ( self . __config . stacks ) } .\" ) stack_configuration = self . __config . stacks [ name ] stack_components = {} for ( component_type_name , component_name , ) in stack_configuration . dict () . items (): component_type = StackComponentType ( component_type_name ) if not component_name : # optional component which is not set, continue continue component = self . get_stack_component ( component_type = component_type , name = component_name , ) stack_components [ component_type ] = component return Stack . from_components ( name = name , components = stack_components ) get_stack_component ( self , component_type , name ) Fetches a registered stack component. Parameters: Name Type Description Default component_type StackComponentType The type of the component to fetch. required name str The name of the component to fetch. required Exceptions: Type Description KeyError If no stack component exists for the given type and name. Source code in zenml/repository.py def get_stack_component ( self , component_type : StackComponentType , name : str ) -> StackComponent : \"\"\"Fetches a registered stack component. Args: component_type: The type of the component to fetch. name: The name of the component to fetch. Raises: KeyError: If no stack component exists for the given type and name. \"\"\" logger . debug ( \"Fetching stack component of type ' %s ' with name ' %s '.\" , component_type . value , name , ) components = self . __config . stack_components [ component_type ] if name not in components : raise KeyError ( f \"Unable to find stack component (type: { component_type } ) \" f \"with name ' { name } '. Available names: { set ( components ) } .\" ) from zenml.stack.stack_component_class_registry import ( StackComponentClassRegistry , ) component_flavor = components [ name ] component_class = StackComponentClassRegistry . get_class ( component_type = component_type , component_flavor = component_flavor ) component_config_path = self . _get_stack_component_config_path ( component_type = component_type , name = name ) component_config = yaml_utils . read_yaml ( component_config_path ) return component_class . parse_obj ( component_config ) get_stack_components ( self , component_type ) Fetches all registered stack components of the given type. Source code in zenml/repository.py def get_stack_components ( self , component_type : StackComponentType ) -> List [ StackComponent ]: \"\"\"Fetches all registered stack components of the given type.\"\"\" component_names = self . __config . stack_components [ component_type ] . keys () return [ self . get_stack_component ( component_type = component_type , name = name ) for name in component_names ] initialize ( * args , ** kwargs ) staticmethod Inner decorator function. Source code in zenml/repository.py def inner_func ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function.\"\"\" track_event ( event_name , metadata = metadata ) result = func ( * args , ** kwargs ) return result is_repository_directory ( path ) staticmethod Checks whether a ZenML repository exists at the given path. Source code in zenml/repository.py @staticmethod def is_repository_directory ( path : Path ) -> bool : \"\"\"Checks whether a ZenML repository exists at the given path.\"\"\" config_dir = path / LOCAL_CONFIG_DIRECTORY_NAME return fileio . is_dir ( str ( config_dir )) register_stack ( * args , ** kwargs ) Inner decorator function. Source code in zenml/repository.py def inner_func ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function.\"\"\" track_event ( event_name , metadata = metadata ) result = func ( * args , ** kwargs ) return result register_stack_component ( self , component ) Registers a stack component. Parameters: Name Type Description Default component StackComponent The component to register. required Exceptions: Type Description StackComponentExistsError If a stack component with the same type and name already exists. Source code in zenml/repository.py def register_stack_component ( self , component : StackComponent , ) -> None : \"\"\"Registers a stack component. Args: component: The component to register. Raises: StackComponentExistsError: If a stack component with the same type and name already exists. \"\"\" components = self . __config . stack_components [ component . type ] if component . name in components : raise StackComponentExistsError ( f \"Unable to register stack component (type: { component . type } ) \" f \"with name ' { component . name } ': Found existing stack component \" f \"with this name.\" ) # write the component configuration file component_config_path = self . _get_stack_component_config_path ( component_type = component . type , name = component . name ) fileio . create_dir_recursive_if_not_exists ( os . path . dirname ( component_config_path ) ) yaml_utils . write_yaml ( component_config_path , json . loads ( component . json ()) ) # add the component to the repository configuration and write it to disk components [ component . name ] = component . flavor . value self . _write_config () logger . info ( \"Registered stack component with name ' %s '.\" , component . name ) analytics_metadata = { \"type\" : component . type . value , \"flavor\" : component . flavor . value , } track_event ( REGISTERED_STACK_COMPONENT , metadata = analytics_metadata ) RepositoryConfiguration ( BaseModel ) pydantic-model Pydantic object used for serializing repository configuration options. Attributes: Name Type Description version str Version of ZenML that was used to create the repository. active_stack_name Optional[str] Optional name of the active stack. stacks Dict[str, zenml.repository.StackConfiguration] Maps stack names to a configuration object containing the names and flavors of all stack components. stack_components DefaultDict[zenml.enums.StackComponentType, Dict[str, str]] Contains names and flavors of all registered stack components. Source code in zenml/repository.py class RepositoryConfiguration ( BaseModel ): \"\"\"Pydantic object used for serializing repository configuration options. Attributes: version: Version of ZenML that was used to create the repository. active_stack_name: Optional name of the active stack. stacks: Maps stack names to a configuration object containing the names and flavors of all stack components. stack_components: Contains names and flavors of all registered stack components. \"\"\" version : str active_stack_name : Optional [ str ] stacks : Dict [ str , StackConfiguration ] stack_components : DefaultDict [ StackComponentType , Dict [ str , str ]] @classmethod def empty_configuration ( cls ) -> \"RepositoryConfiguration\" : \"\"\"Helper method to create an empty configuration object.\"\"\" return cls ( version = zenml . __version__ , stacks = {}, stack_components = {}, ) @validator ( \"stack_components\" ) def _construct_defaultdict ( cls , stack_components : Dict [ StackComponentType , Dict [ str , str ]] ) -> DefaultDict [ StackComponentType , Dict [ str , str ]]: \"\"\"Ensures that `stack_components` is a defaultdict so stack components of a new component type can be added without issues.\"\"\" return defaultdict ( dict , stack_components ) empty_configuration () classmethod Helper method to create an empty configuration object. Source code in zenml/repository.py @classmethod def empty_configuration ( cls ) -> \"RepositoryConfiguration\" : \"\"\"Helper method to create an empty configuration object.\"\"\" return cls ( version = zenml . __version__ , stacks = {}, stack_components = {}, ) StackConfiguration ( BaseModel ) pydantic-model Pydantic object used for serializing stack configuration options. Source code in zenml/repository.py class StackConfiguration ( BaseModel ): \"\"\"Pydantic object used for serializing stack configuration options.\"\"\" orchestrator : str metadata_store : str artifact_store : str container_registry : Optional [ str ] def contains_component ( self , component_type : StackComponentType , name : str ) -> bool : \"\"\"Checks if the stack contains a specific component.\"\"\" return self . dict () . get ( component_type . value ) == name class Config : \"\"\"Pydantic configuration class.\"\"\" allow_mutation = False Config Pydantic configuration class. Source code in zenml/repository.py class Config : \"\"\"Pydantic configuration class.\"\"\" allow_mutation = False contains_component ( self , component_type , name ) Checks if the stack contains a specific component. Source code in zenml/repository.py def contains_component ( self , component_type : StackComponentType , name : str ) -> bool : \"\"\"Checks if the stack contains a specific component.\"\"\" return self . dict () . get ( component_type . value ) == name","title":"Repository"},{"location":"api_docs/repository/#repository","text":"","title":"Repository"},{"location":"api_docs/repository/#zenml.repository","text":"","title":"repository"},{"location":"api_docs/repository/#zenml.repository.Repository","text":"ZenML repository class. ZenML repositories store configuration options for ZenML stacks as well as their components. Source code in zenml/repository.py class Repository : \"\"\"ZenML repository class. ZenML repositories store configuration options for ZenML stacks as well as their components. \"\"\" def __init__ ( self , root : Optional [ Path ] = None ): \"\"\"Initializes a repository instance. Args: root: Optional root directory of the repository. If no path is given, this function tries to find the repository using the environment variable `ZENML_REPOSITORY_PATH` (if set) and recursively searching in the parent directories of the current working directory. Raises: RepositoryNotFoundError: If no ZenML repository directory is found. \"\"\" self . _root = Repository . find_repository ( root ) # load the repository configuration file if it exists, otherwise use # an empty configuration as default config_path = self . _config_path () if fileio . file_exists ( config_path ): config_dict = yaml_utils . read_yaml ( config_path ) self . __config = RepositoryConfiguration . parse_obj ( config_dict ) else : self . __config = RepositoryConfiguration . empty_configuration () if self . version != zenml . __version__ : # TODO [ENG-366]: Create compatibility table so we don't have to # warn about mismatching repository and ZenML version each time logger . warning ( \"This ZenML repository was created with a different version \" \"of ZenML (Repository version: %s , current ZenML version: %s ). \" \"In case you encounter any errors, please delete and \" \"reinitialize this repository.\" , self . version , zenml . __version__ , ) def _config_path ( self ) -> str : \"\"\"Path to the repository configuration file.\"\"\" return str ( self . config_directory / \"config.yaml\" ) def _get_stack_component_config_path ( self , component_type : StackComponentType , name : str ) -> str : \"\"\"Path to the configuration file of a stack component.\"\"\" path = self . config_directory / component_type . plural / f \" { name } .yaml\" return str ( path ) def _write_config ( self ) -> None : \"\"\"Writes the repository configuration file.\"\"\" config_dict = json . loads ( self . __config . json ()) yaml_utils . write_yaml ( self . _config_path (), config_dict ) @staticmethod @track ( event = INITIALIZE_REPO ) def initialize ( root : Path = Path . cwd ()) -> None : \"\"\"Initializes a new ZenML repository at the given path. The newly created repository will contain a single stack with a local orchestrator, a local artifact store and a local SQLite metadata store. Args: root: The root directory where the repository should be created. Raises: InitializationException: If the root directory already contains a ZenML repository. \"\"\" logger . debug ( \"Initializing new repository at path %s .\" , root ) if Repository . is_repository_directory ( root ): raise InitializationException ( f \"Found existing ZenML repository at path ' { root } '.\" ) config_directory = str ( root / LOCAL_CONFIG_DIRECTORY_NAME ) fileio . create_dir_recursive_if_not_exists ( config_directory ) # register and activate a local stack repo = Repository ( root = root ) stack = Stack . default_local_stack () repo . register_stack ( stack ) repo . activate_stack ( stack . name ) @property def version ( self ) -> str : \"\"\"The version of the repository.\"\"\" return self . __config . version @property def root ( self ) -> Path : \"\"\"The root directory of this repository.\"\"\" return self . _root @property def config_directory ( self ) -> Path : \"\"\"The configuration directory of this repository.\"\"\" return self . root / LOCAL_CONFIG_DIRECTORY_NAME @property def stacks ( self ) -> List [ Stack ]: \"\"\"All stacks registered in this repository.\"\"\" return [ self . get_stack ( name = name ) for name in self . __config . stacks ] @property def stack_configurations ( self ) -> Dict [ str , StackConfiguration ]: \"\"\"Configuration objects for all stacks registered in this repository. This property is intended as a quick way to get information about the components of the registered stacks without loading all installed integrations. The contained stack configurations might be invalid if they were modified by hand, to ensure you get valid stacks use `repo.stacks()` instead. Modifying the contents of the returned dictionary does not actually register/deregister stacks, use `repo.register_stack(...)` or `repo.deregister_stack(...)` instead. \"\"\" return self . __config . stacks . copy () @property def active_stack ( self ) -> Stack : \"\"\"The active stack for this repository. Raises: RuntimeError: If no active stack name is configured. KeyError: If no stack was found for the configured name or one of the stack components is not registered. \"\"\" if not self . __config . active_stack_name : raise RuntimeError ( \"No active stack name configured. Run \" \"`zenml stack set STACK_NAME` to update the active stack.\" ) return self . get_stack ( name = self . __config . active_stack_name ) @property def active_stack_name ( self ) -> str : \"\"\"The name of the active stack for this repository. Raises: RuntimeError: If no active stack name is configured. \"\"\" if not self . __config . active_stack_name : raise RuntimeError ( \"No active stack name configured. Run \" \"`zenml stack set STACK_NAME` to update the active stack.\" ) return self . __config . active_stack_name # TODO [ENG-367]: Should we replace the stack name by the actual stack # object? It would be more consistent with the rest of the API but # requires some additional care (checking if the stack + components are # actually registered in this repository). Downside: We would need to # load all the integrations to create the stack object which makes the CLI # command to set the active stack much slower. @track ( event = SET_STACK ) def activate_stack ( self , name : str ) -> None : \"\"\"Activates the stack for the given name. Args: name: Name of the stack to activate. Raises: KeyError: If no stack exists for the given name. \"\"\" if name not in self . __config . stacks : raise KeyError ( f \"Unable to find stack for name ' { name } '.\" ) self . __config . active_stack_name = name self . _write_config () def get_stack ( self , name : str ) -> Stack : \"\"\"Fetches a stack. Args: name: The name of the stack to fetch. Raises: KeyError: If no stack exists for the given name or one of the stacks components is not registered. \"\"\" logger . debug ( \"Fetching stack with name ' %s '.\" , name ) if name not in self . __config . stacks : raise KeyError ( f \"Unable to find stack with name ' { name } '. Available names: \" f \" { set ( self . __config . stacks ) } .\" ) stack_configuration = self . __config . stacks [ name ] stack_components = {} for ( component_type_name , component_name , ) in stack_configuration . dict () . items (): component_type = StackComponentType ( component_type_name ) if not component_name : # optional component which is not set, continue continue component = self . get_stack_component ( component_type = component_type , name = component_name , ) stack_components [ component_type ] = component return Stack . from_components ( name = name , components = stack_components ) @track ( event = REGISTERED_STACK ) def register_stack ( self , stack : Stack ) -> None : \"\"\"Registers a stack and it's components. If any of the stacks' components aren't registered in the repository yet, this method will try to register them as well. Args: stack: The stack to register. Raises: StackExistsError: If a stack with the same name already exists. StackComponentExistsError: If a component of the stack wasn't registered and a different component with the same name already exists. \"\"\" if stack . name in self . __config . stacks : raise StackExistsError ( f \"Unable to register stack with name ' { stack . name } ': Found \" f \"existing stack with this name.\" ) components = {} for component_type , component in stack . components . items (): try : existing_component = self . get_stack_component ( component_type = component_type , name = component . name ) if existing_component . uuid != component . uuid : raise StackComponentExistsError ( f \"Unable to register one of the stacks components: \" f \"A component of type ' { component_type } ' and name \" f \"' { component . name } ' already exists.\" ) except KeyError : # a component of the stack isn't registered yet -> register it self . register_stack_component ( component ) components [ component_type . value ] = component . name stack_configuration = StackConfiguration ( ** components ) self . __config . stacks [ stack . name ] = stack_configuration self . _write_config () logger . info ( \"Registered stack with name ' %s '.\" , stack . name ) def deregister_stack ( self , name : str ) -> None : \"\"\"Deregisters a stack. Args: name: The name of the stack to deregister. Raises: ValueError: If the stack is the currently active stack for this repository. \"\"\" if name == self . active_stack_name : raise ValueError ( f \"Unable to deregister active stack ' { name } '.\" ) try : del self . __config . stacks [ name ] self . _write_config () logger . info ( \"Deregistered stack with name ' %s '.\" , name ) except KeyError : logger . warning ( \"Unable to deregister stack with name ' %s ': No stack exists \" \"with this name.\" , name , ) def get_stack_components ( self , component_type : StackComponentType ) -> List [ StackComponent ]: \"\"\"Fetches all registered stack components of the given type.\"\"\" component_names = self . __config . stack_components [ component_type ] . keys () return [ self . get_stack_component ( component_type = component_type , name = name ) for name in component_names ] def get_stack_component ( self , component_type : StackComponentType , name : str ) -> StackComponent : \"\"\"Fetches a registered stack component. Args: component_type: The type of the component to fetch. name: The name of the component to fetch. Raises: KeyError: If no stack component exists for the given type and name. \"\"\" logger . debug ( \"Fetching stack component of type ' %s ' with name ' %s '.\" , component_type . value , name , ) components = self . __config . stack_components [ component_type ] if name not in components : raise KeyError ( f \"Unable to find stack component (type: { component_type } ) \" f \"with name ' { name } '. Available names: { set ( components ) } .\" ) from zenml.stack.stack_component_class_registry import ( StackComponentClassRegistry , ) component_flavor = components [ name ] component_class = StackComponentClassRegistry . get_class ( component_type = component_type , component_flavor = component_flavor ) component_config_path = self . _get_stack_component_config_path ( component_type = component_type , name = name ) component_config = yaml_utils . read_yaml ( component_config_path ) return component_class . parse_obj ( component_config ) def register_stack_component ( self , component : StackComponent , ) -> None : \"\"\"Registers a stack component. Args: component: The component to register. Raises: StackComponentExistsError: If a stack component with the same type and name already exists. \"\"\" components = self . __config . stack_components [ component . type ] if component . name in components : raise StackComponentExistsError ( f \"Unable to register stack component (type: { component . type } ) \" f \"with name ' { component . name } ': Found existing stack component \" f \"with this name.\" ) # write the component configuration file component_config_path = self . _get_stack_component_config_path ( component_type = component . type , name = component . name ) fileio . create_dir_recursive_if_not_exists ( os . path . dirname ( component_config_path ) ) yaml_utils . write_yaml ( component_config_path , json . loads ( component . json ()) ) # add the component to the repository configuration and write it to disk components [ component . name ] = component . flavor . value self . _write_config () logger . info ( \"Registered stack component with name ' %s '.\" , component . name ) analytics_metadata = { \"type\" : component . type . value , \"flavor\" : component . flavor . value , } track_event ( REGISTERED_STACK_COMPONENT , metadata = analytics_metadata ) def deregister_stack_component ( self , component_type : StackComponentType , name : str ) -> None : \"\"\"Deregisters a stack component. Args: component_type: The type of the component to deregister. name: The name of the component to deregister. \"\"\" for stack_name , stack_config in self . stack_configurations . items (): if stack_config . contains_component ( component_type = component_type , name = name ): raise ValueError ( f \"Unable to deregister stack component (type: \" f \" { component_type } , name: { name } ) that is part of a \" f \"registered stack (stack name: ' { stack_name } ').\" ) components = self . __config . stack_components [ component_type ] try : del components [ name ] self . _write_config () logger . info ( \"Deregistered stack component (type: %s ) with name ' %s '.\" , component_type . value , name , ) except KeyError : logger . warning ( \"Unable to deregister stack component (type: %s ) with name \" \"' %s ': No stack component exists with this name.\" , component_type . value , name , ) component_config_path = self . _get_stack_component_config_path ( component_type = component_type , name = name ) if fileio . file_exists ( component_config_path ): fileio . remove ( component_config_path ) # TODO [ENG-368]: Discuss whether we want to unify these two methods. @track ( event = GET_PIPELINES ) def get_pipelines ( self , stack_name : Optional [ str ] = None ) -> List [ PipelineView ]: \"\"\"Fetches post-execution pipeline views. Args: stack_name: If specified, pipelines in the metadata store of the given stack are returned. Otherwise pipelines in the metadata store of the currently active stack are returned. Returns: A list of post-execution pipeline views. Raises: KeyError: If no stack with the given name exists. \"\"\" stack_name = stack_name or self . active_stack_name metadata_store = self . get_stack ( stack_name ) . metadata_store return metadata_store . get_pipelines () @track ( event = GET_PIPELINE ) def get_pipeline ( self , pipeline_name : str , stack_name : Optional [ str ] = None ) -> Optional [ PipelineView ]: \"\"\"Fetches a post-execution pipeline view. Args: pipeline_name: Name of the pipeline. stack_name: If specified, pipelines in the metadata store of the given stack are returned. Otherwise pipelines in the metadata store of the currently active stack are returned. Returns: A post-execution pipeline view for the given name or `None` if it doesn't exist. Raises: KeyError: If no stack with the given name exists. \"\"\" stack_name = stack_name or self . active_stack_name metadata_store = self . get_stack ( stack_name ) . metadata_store return metadata_store . get_pipeline ( pipeline_name ) @staticmethod def is_repository_directory ( path : Path ) -> bool : \"\"\"Checks whether a ZenML repository exists at the given path.\"\"\" config_dir = path / LOCAL_CONFIG_DIRECTORY_NAME return fileio . is_dir ( str ( config_dir )) @staticmethod def find_repository ( path : Optional [ Path ] = None ) -> Path : \"\"\"Finds path of a ZenML repository directory. Args: path: Optional path to look for the repository. If no path is given, this function tries to find the repository using the environment variable `ZENML_REPOSITORY_PATH` (if set) and recursively searching in the parent directories of the current working directory. Returns: Absolute path to a ZenML repository directory. Raises: RepositoryNotFoundError: If no ZenML repository is found. \"\"\" if not path : # try to get path from the environment variable env_var_path = os . getenv ( ENV_ZENML_REPOSITORY_PATH ) if env_var_path : path = Path ( env_var_path ) if path : # explicit path via parameter or environment variable, don't search # parent directories search_parent_directories = False error_message = ( f \"Unable to find ZenML repository at path ' { path } '. Make sure \" f \"to create a ZenML repository by calling `zenml init` when \" f \"specifying an explicit repository path in code or via the \" f \"environment variable ' { ENV_ZENML_REPOSITORY_PATH } '.\" ) else : # try to find the repo in the parent directories of the current # working directory path = Path . cwd () search_parent_directories = True error_message = ( f \"Unable to find ZenML repository in your current working \" f \"directory ( { path } ) or any parent directories. If you \" f \"want to use an existing repository which is in a different \" f \"location, set the environment variable \" f \"' { ENV_ZENML_REPOSITORY_PATH } '. If you want to create a new \" f \"repository, run `zenml init`.\" ) def _find_repo_helper ( path_ : Path ) -> Path : \"\"\"Helper function to recursively search parent directories for a ZenML repository.\"\"\" if Repository . is_repository_directory ( path_ ): return path_ if not search_parent_directories or fileio . is_root ( str ( path_ )): raise RepositoryNotFoundError ( error_message ) return _find_repo_helper ( path_ . parent ) return _find_repo_helper ( path ) . resolve ()","title":"Repository"},{"location":"api_docs/repository/#zenml.repository.Repository.active_stack","text":"The active stack for this repository. Exceptions: Type Description RuntimeError If no active stack name is configured. KeyError If no stack was found for the configured name or one of the stack components is not registered.","title":"active_stack"},{"location":"api_docs/repository/#zenml.repository.Repository.active_stack_name","text":"The name of the active stack for this repository. Exceptions: Type Description RuntimeError If no active stack name is configured.","title":"active_stack_name"},{"location":"api_docs/repository/#zenml.repository.Repository.config_directory","text":"The configuration directory of this repository.","title":"config_directory"},{"location":"api_docs/repository/#zenml.repository.Repository.root","text":"The root directory of this repository.","title":"root"},{"location":"api_docs/repository/#zenml.repository.Repository.stack_configurations","text":"Configuration objects for all stacks registered in this repository. This property is intended as a quick way to get information about the components of the registered stacks without loading all installed integrations. The contained stack configurations might be invalid if they were modified by hand, to ensure you get valid stacks use repo.stacks() instead. Modifying the contents of the returned dictionary does not actually register/deregister stacks, use repo.register_stack(...) or repo.deregister_stack(...) instead.","title":"stack_configurations"},{"location":"api_docs/repository/#zenml.repository.Repository.stacks","text":"All stacks registered in this repository.","title":"stacks"},{"location":"api_docs/repository/#zenml.repository.Repository.version","text":"The version of the repository.","title":"version"},{"location":"api_docs/repository/#zenml.repository.Repository.__init__","text":"Initializes a repository instance. Parameters: Name Type Description Default root Optional[pathlib.Path] Optional root directory of the repository. If no path is given, this function tries to find the repository using the environment variable ZENML_REPOSITORY_PATH (if set) and recursively searching in the parent directories of the current working directory. None Exceptions: Type Description RepositoryNotFoundError If no ZenML repository directory is found. Source code in zenml/repository.py def __init__ ( self , root : Optional [ Path ] = None ): \"\"\"Initializes a repository instance. Args: root: Optional root directory of the repository. If no path is given, this function tries to find the repository using the environment variable `ZENML_REPOSITORY_PATH` (if set) and recursively searching in the parent directories of the current working directory. Raises: RepositoryNotFoundError: If no ZenML repository directory is found. \"\"\" self . _root = Repository . find_repository ( root ) # load the repository configuration file if it exists, otherwise use # an empty configuration as default config_path = self . _config_path () if fileio . file_exists ( config_path ): config_dict = yaml_utils . read_yaml ( config_path ) self . __config = RepositoryConfiguration . parse_obj ( config_dict ) else : self . __config = RepositoryConfiguration . empty_configuration () if self . version != zenml . __version__ : # TODO [ENG-366]: Create compatibility table so we don't have to # warn about mismatching repository and ZenML version each time logger . warning ( \"This ZenML repository was created with a different version \" \"of ZenML (Repository version: %s , current ZenML version: %s ). \" \"In case you encounter any errors, please delete and \" \"reinitialize this repository.\" , self . version , zenml . __version__ , )","title":"__init__()"},{"location":"api_docs/repository/#zenml.repository.Repository.activate_stack","text":"Inner decorator function. Source code in zenml/repository.py def inner_func ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function.\"\"\" track_event ( event_name , metadata = metadata ) result = func ( * args , ** kwargs ) return result","title":"activate_stack()"},{"location":"api_docs/repository/#zenml.repository.Repository.deregister_stack","text":"Deregisters a stack. Parameters: Name Type Description Default name str The name of the stack to deregister. required Exceptions: Type Description ValueError If the stack is the currently active stack for this repository. Source code in zenml/repository.py def deregister_stack ( self , name : str ) -> None : \"\"\"Deregisters a stack. Args: name: The name of the stack to deregister. Raises: ValueError: If the stack is the currently active stack for this repository. \"\"\" if name == self . active_stack_name : raise ValueError ( f \"Unable to deregister active stack ' { name } '.\" ) try : del self . __config . stacks [ name ] self . _write_config () logger . info ( \"Deregistered stack with name ' %s '.\" , name ) except KeyError : logger . warning ( \"Unable to deregister stack with name ' %s ': No stack exists \" \"with this name.\" , name , )","title":"deregister_stack()"},{"location":"api_docs/repository/#zenml.repository.Repository.deregister_stack_component","text":"Deregisters a stack component. Parameters: Name Type Description Default component_type StackComponentType The type of the component to deregister. required name str The name of the component to deregister. required Source code in zenml/repository.py def deregister_stack_component ( self , component_type : StackComponentType , name : str ) -> None : \"\"\"Deregisters a stack component. Args: component_type: The type of the component to deregister. name: The name of the component to deregister. \"\"\" for stack_name , stack_config in self . stack_configurations . items (): if stack_config . contains_component ( component_type = component_type , name = name ): raise ValueError ( f \"Unable to deregister stack component (type: \" f \" { component_type } , name: { name } ) that is part of a \" f \"registered stack (stack name: ' { stack_name } ').\" ) components = self . __config . stack_components [ component_type ] try : del components [ name ] self . _write_config () logger . info ( \"Deregistered stack component (type: %s ) with name ' %s '.\" , component_type . value , name , ) except KeyError : logger . warning ( \"Unable to deregister stack component (type: %s ) with name \" \"' %s ': No stack component exists with this name.\" , component_type . value , name , ) component_config_path = self . _get_stack_component_config_path ( component_type = component_type , name = name ) if fileio . file_exists ( component_config_path ): fileio . remove ( component_config_path )","title":"deregister_stack_component()"},{"location":"api_docs/repository/#zenml.repository.Repository.find_repository","text":"Finds path of a ZenML repository directory. Parameters: Name Type Description Default path Optional[pathlib.Path] Optional path to look for the repository. If no path is given, this function tries to find the repository using the environment variable ZENML_REPOSITORY_PATH (if set) and recursively searching in the parent directories of the current working directory. None Returns: Type Description Path Absolute path to a ZenML repository directory. Exceptions: Type Description RepositoryNotFoundError If no ZenML repository is found. Source code in zenml/repository.py @staticmethod def find_repository ( path : Optional [ Path ] = None ) -> Path : \"\"\"Finds path of a ZenML repository directory. Args: path: Optional path to look for the repository. If no path is given, this function tries to find the repository using the environment variable `ZENML_REPOSITORY_PATH` (if set) and recursively searching in the parent directories of the current working directory. Returns: Absolute path to a ZenML repository directory. Raises: RepositoryNotFoundError: If no ZenML repository is found. \"\"\" if not path : # try to get path from the environment variable env_var_path = os . getenv ( ENV_ZENML_REPOSITORY_PATH ) if env_var_path : path = Path ( env_var_path ) if path : # explicit path via parameter or environment variable, don't search # parent directories search_parent_directories = False error_message = ( f \"Unable to find ZenML repository at path ' { path } '. Make sure \" f \"to create a ZenML repository by calling `zenml init` when \" f \"specifying an explicit repository path in code or via the \" f \"environment variable ' { ENV_ZENML_REPOSITORY_PATH } '.\" ) else : # try to find the repo in the parent directories of the current # working directory path = Path . cwd () search_parent_directories = True error_message = ( f \"Unable to find ZenML repository in your current working \" f \"directory ( { path } ) or any parent directories. If you \" f \"want to use an existing repository which is in a different \" f \"location, set the environment variable \" f \"' { ENV_ZENML_REPOSITORY_PATH } '. If you want to create a new \" f \"repository, run `zenml init`.\" ) def _find_repo_helper ( path_ : Path ) -> Path : \"\"\"Helper function to recursively search parent directories for a ZenML repository.\"\"\" if Repository . is_repository_directory ( path_ ): return path_ if not search_parent_directories or fileio . is_root ( str ( path_ )): raise RepositoryNotFoundError ( error_message ) return _find_repo_helper ( path_ . parent ) return _find_repo_helper ( path ) . resolve ()","title":"find_repository()"},{"location":"api_docs/repository/#zenml.repository.Repository.get_pipeline","text":"Inner decorator function. Source code in zenml/repository.py def inner_func ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function.\"\"\" track_event ( event_name , metadata = metadata ) result = func ( * args , ** kwargs ) return result","title":"get_pipeline()"},{"location":"api_docs/repository/#zenml.repository.Repository.get_pipelines","text":"Inner decorator function. Source code in zenml/repository.py def inner_func ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function.\"\"\" track_event ( event_name , metadata = metadata ) result = func ( * args , ** kwargs ) return result","title":"get_pipelines()"},{"location":"api_docs/repository/#zenml.repository.Repository.get_stack","text":"Fetches a stack. Parameters: Name Type Description Default name str The name of the stack to fetch. required Exceptions: Type Description KeyError If no stack exists for the given name or one of the stacks components is not registered. Source code in zenml/repository.py def get_stack ( self , name : str ) -> Stack : \"\"\"Fetches a stack. Args: name: The name of the stack to fetch. Raises: KeyError: If no stack exists for the given name or one of the stacks components is not registered. \"\"\" logger . debug ( \"Fetching stack with name ' %s '.\" , name ) if name not in self . __config . stacks : raise KeyError ( f \"Unable to find stack with name ' { name } '. Available names: \" f \" { set ( self . __config . stacks ) } .\" ) stack_configuration = self . __config . stacks [ name ] stack_components = {} for ( component_type_name , component_name , ) in stack_configuration . dict () . items (): component_type = StackComponentType ( component_type_name ) if not component_name : # optional component which is not set, continue continue component = self . get_stack_component ( component_type = component_type , name = component_name , ) stack_components [ component_type ] = component return Stack . from_components ( name = name , components = stack_components )","title":"get_stack()"},{"location":"api_docs/repository/#zenml.repository.Repository.get_stack_component","text":"Fetches a registered stack component. Parameters: Name Type Description Default component_type StackComponentType The type of the component to fetch. required name str The name of the component to fetch. required Exceptions: Type Description KeyError If no stack component exists for the given type and name. Source code in zenml/repository.py def get_stack_component ( self , component_type : StackComponentType , name : str ) -> StackComponent : \"\"\"Fetches a registered stack component. Args: component_type: The type of the component to fetch. name: The name of the component to fetch. Raises: KeyError: If no stack component exists for the given type and name. \"\"\" logger . debug ( \"Fetching stack component of type ' %s ' with name ' %s '.\" , component_type . value , name , ) components = self . __config . stack_components [ component_type ] if name not in components : raise KeyError ( f \"Unable to find stack component (type: { component_type } ) \" f \"with name ' { name } '. Available names: { set ( components ) } .\" ) from zenml.stack.stack_component_class_registry import ( StackComponentClassRegistry , ) component_flavor = components [ name ] component_class = StackComponentClassRegistry . get_class ( component_type = component_type , component_flavor = component_flavor ) component_config_path = self . _get_stack_component_config_path ( component_type = component_type , name = name ) component_config = yaml_utils . read_yaml ( component_config_path ) return component_class . parse_obj ( component_config )","title":"get_stack_component()"},{"location":"api_docs/repository/#zenml.repository.Repository.get_stack_components","text":"Fetches all registered stack components of the given type. Source code in zenml/repository.py def get_stack_components ( self , component_type : StackComponentType ) -> List [ StackComponent ]: \"\"\"Fetches all registered stack components of the given type.\"\"\" component_names = self . __config . stack_components [ component_type ] . keys () return [ self . get_stack_component ( component_type = component_type , name = name ) for name in component_names ]","title":"get_stack_components()"},{"location":"api_docs/repository/#zenml.repository.Repository.initialize","text":"Inner decorator function. Source code in zenml/repository.py def inner_func ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function.\"\"\" track_event ( event_name , metadata = metadata ) result = func ( * args , ** kwargs ) return result","title":"initialize()"},{"location":"api_docs/repository/#zenml.repository.Repository.is_repository_directory","text":"Checks whether a ZenML repository exists at the given path. Source code in zenml/repository.py @staticmethod def is_repository_directory ( path : Path ) -> bool : \"\"\"Checks whether a ZenML repository exists at the given path.\"\"\" config_dir = path / LOCAL_CONFIG_DIRECTORY_NAME return fileio . is_dir ( str ( config_dir ))","title":"is_repository_directory()"},{"location":"api_docs/repository/#zenml.repository.Repository.register_stack","text":"Inner decorator function. Source code in zenml/repository.py def inner_func ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function.\"\"\" track_event ( event_name , metadata = metadata ) result = func ( * args , ** kwargs ) return result","title":"register_stack()"},{"location":"api_docs/repository/#zenml.repository.Repository.register_stack_component","text":"Registers a stack component. Parameters: Name Type Description Default component StackComponent The component to register. required Exceptions: Type Description StackComponentExistsError If a stack component with the same type and name already exists. Source code in zenml/repository.py def register_stack_component ( self , component : StackComponent , ) -> None : \"\"\"Registers a stack component. Args: component: The component to register. Raises: StackComponentExistsError: If a stack component with the same type and name already exists. \"\"\" components = self . __config . stack_components [ component . type ] if component . name in components : raise StackComponentExistsError ( f \"Unable to register stack component (type: { component . type } ) \" f \"with name ' { component . name } ': Found existing stack component \" f \"with this name.\" ) # write the component configuration file component_config_path = self . _get_stack_component_config_path ( component_type = component . type , name = component . name ) fileio . create_dir_recursive_if_not_exists ( os . path . dirname ( component_config_path ) ) yaml_utils . write_yaml ( component_config_path , json . loads ( component . json ()) ) # add the component to the repository configuration and write it to disk components [ component . name ] = component . flavor . value self . _write_config () logger . info ( \"Registered stack component with name ' %s '.\" , component . name ) analytics_metadata = { \"type\" : component . type . value , \"flavor\" : component . flavor . value , } track_event ( REGISTERED_STACK_COMPONENT , metadata = analytics_metadata )","title":"register_stack_component()"},{"location":"api_docs/repository/#zenml.repository.RepositoryConfiguration","text":"Pydantic object used for serializing repository configuration options. Attributes: Name Type Description version str Version of ZenML that was used to create the repository. active_stack_name Optional[str] Optional name of the active stack. stacks Dict[str, zenml.repository.StackConfiguration] Maps stack names to a configuration object containing the names and flavors of all stack components. stack_components DefaultDict[zenml.enums.StackComponentType, Dict[str, str]] Contains names and flavors of all registered stack components. Source code in zenml/repository.py class RepositoryConfiguration ( BaseModel ): \"\"\"Pydantic object used for serializing repository configuration options. Attributes: version: Version of ZenML that was used to create the repository. active_stack_name: Optional name of the active stack. stacks: Maps stack names to a configuration object containing the names and flavors of all stack components. stack_components: Contains names and flavors of all registered stack components. \"\"\" version : str active_stack_name : Optional [ str ] stacks : Dict [ str , StackConfiguration ] stack_components : DefaultDict [ StackComponentType , Dict [ str , str ]] @classmethod def empty_configuration ( cls ) -> \"RepositoryConfiguration\" : \"\"\"Helper method to create an empty configuration object.\"\"\" return cls ( version = zenml . __version__ , stacks = {}, stack_components = {}, ) @validator ( \"stack_components\" ) def _construct_defaultdict ( cls , stack_components : Dict [ StackComponentType , Dict [ str , str ]] ) -> DefaultDict [ StackComponentType , Dict [ str , str ]]: \"\"\"Ensures that `stack_components` is a defaultdict so stack components of a new component type can be added without issues.\"\"\" return defaultdict ( dict , stack_components )","title":"RepositoryConfiguration"},{"location":"api_docs/repository/#zenml.repository.RepositoryConfiguration.empty_configuration","text":"Helper method to create an empty configuration object. Source code in zenml/repository.py @classmethod def empty_configuration ( cls ) -> \"RepositoryConfiguration\" : \"\"\"Helper method to create an empty configuration object.\"\"\" return cls ( version = zenml . __version__ , stacks = {}, stack_components = {}, )","title":"empty_configuration()"},{"location":"api_docs/repository/#zenml.repository.StackConfiguration","text":"Pydantic object used for serializing stack configuration options. Source code in zenml/repository.py class StackConfiguration ( BaseModel ): \"\"\"Pydantic object used for serializing stack configuration options.\"\"\" orchestrator : str metadata_store : str artifact_store : str container_registry : Optional [ str ] def contains_component ( self , component_type : StackComponentType , name : str ) -> bool : \"\"\"Checks if the stack contains a specific component.\"\"\" return self . dict () . get ( component_type . value ) == name class Config : \"\"\"Pydantic configuration class.\"\"\" allow_mutation = False","title":"StackConfiguration"},{"location":"api_docs/repository/#zenml.repository.StackConfiguration.Config","text":"Pydantic configuration class. Source code in zenml/repository.py class Config : \"\"\"Pydantic configuration class.\"\"\" allow_mutation = False","title":"Config"},{"location":"api_docs/repository/#zenml.repository.StackConfiguration.contains_component","text":"Checks if the stack contains a specific component. Source code in zenml/repository.py def contains_component ( self , component_type : StackComponentType , name : str ) -> bool : \"\"\"Checks if the stack contains a specific component.\"\"\" return self . dict () . get ( component_type . value ) == name","title":"contains_component()"},{"location":"api_docs/runtime_configuration/","text":"Runtime Configuration zenml.runtime_configuration RuntimeConfiguration ( dict , Generic ) RuntimeConfiguration store dynamic options for a pipeline run. Use stack.runtime_options() to get all available runtime options for the components of a specific ZenML stack. This class is a dict subclass, so getting/setting runtime options is done using some_value = runtime_configuration[\"some_key\"] and runtime_configuration[\"some_key\"] = 1 . Source code in zenml/runtime_configuration.py class RuntimeConfiguration ( Dict [ str , Any ]): \"\"\"RuntimeConfiguration store dynamic options for a pipeline run. Use `stack.runtime_options()` to get all available runtime options for the components of a specific ZenML stack. This class is a `dict` subclass, so getting/setting runtime options is done using `some_value = runtime_configuration[\"some_key\"]` and `runtime_configuration[\"some_key\"] = 1`. \"\"\" def __init__ ( self , * , run_name : Optional [ str ] = None , ** runtime_options : Any ): \"\"\"Initializes a RuntimeConfiguration object. Args: run_name: Optional name of the pipeline run. **runtime_options: Additional runtime options. \"\"\" runtime_options [ RUN_NAME_OPTION_KEY ] = run_name super () . __init__ ( runtime_options ) @property def run_name ( self ) -> Optional [ str ]: \"\"\"Name of the pipeline run.\"\"\" return cast ( Optional [ str ], self [ RUN_NAME_OPTION_KEY ]) run_name : Optional [ str ] property readonly Name of the pipeline run. __init__ ( self , * , run_name = None , ** runtime_options ) special Initializes a RuntimeConfiguration object. Parameters: Name Type Description Default run_name Optional[str] Optional name of the pipeline run. None **runtime_options Any Additional runtime options. {} Source code in zenml/runtime_configuration.py def __init__ ( self , * , run_name : Optional [ str ] = None , ** runtime_options : Any ): \"\"\"Initializes a RuntimeConfiguration object. Args: run_name: Optional name of the pipeline run. **runtime_options: Additional runtime options. \"\"\" runtime_options [ RUN_NAME_OPTION_KEY ] = run_name super () . __init__ ( runtime_options )","title":"Runtime Configuration"},{"location":"api_docs/runtime_configuration/#runtime-configuration","text":"","title":"Runtime Configuration"},{"location":"api_docs/runtime_configuration/#zenml.runtime_configuration","text":"","title":"runtime_configuration"},{"location":"api_docs/runtime_configuration/#zenml.runtime_configuration.RuntimeConfiguration","text":"RuntimeConfiguration store dynamic options for a pipeline run. Use stack.runtime_options() to get all available runtime options for the components of a specific ZenML stack. This class is a dict subclass, so getting/setting runtime options is done using some_value = runtime_configuration[\"some_key\"] and runtime_configuration[\"some_key\"] = 1 . Source code in zenml/runtime_configuration.py class RuntimeConfiguration ( Dict [ str , Any ]): \"\"\"RuntimeConfiguration store dynamic options for a pipeline run. Use `stack.runtime_options()` to get all available runtime options for the components of a specific ZenML stack. This class is a `dict` subclass, so getting/setting runtime options is done using `some_value = runtime_configuration[\"some_key\"]` and `runtime_configuration[\"some_key\"] = 1`. \"\"\" def __init__ ( self , * , run_name : Optional [ str ] = None , ** runtime_options : Any ): \"\"\"Initializes a RuntimeConfiguration object. Args: run_name: Optional name of the pipeline run. **runtime_options: Additional runtime options. \"\"\" runtime_options [ RUN_NAME_OPTION_KEY ] = run_name super () . __init__ ( runtime_options ) @property def run_name ( self ) -> Optional [ str ]: \"\"\"Name of the pipeline run.\"\"\" return cast ( Optional [ str ], self [ RUN_NAME_OPTION_KEY ])","title":"RuntimeConfiguration"},{"location":"api_docs/runtime_configuration/#zenml.runtime_configuration.RuntimeConfiguration.run_name","text":"Name of the pipeline run.","title":"run_name"},{"location":"api_docs/runtime_configuration/#zenml.runtime_configuration.RuntimeConfiguration.__init__","text":"Initializes a RuntimeConfiguration object. Parameters: Name Type Description Default run_name Optional[str] Optional name of the pipeline run. None **runtime_options Any Additional runtime options. {} Source code in zenml/runtime_configuration.py def __init__ ( self , * , run_name : Optional [ str ] = None , ** runtime_options : Any ): \"\"\"Initializes a RuntimeConfiguration object. Args: run_name: Optional name of the pipeline run. **runtime_options: Additional runtime options. \"\"\" runtime_options [ RUN_NAME_OPTION_KEY ] = run_name super () . __init__ ( runtime_options )","title":"__init__()"},{"location":"api_docs/stack/","text":"Stack zenml.stack special stack Stack ZenML stack class. A ZenML stack is a collection of multiple stack components that are required to run ZenML pipelines. Some of these components (orchestrator, metadata store and artifact store) are required to run any kind of pipeline, other components like the container registry are only required if other stack components depend on them. Source code in zenml/stack/stack.py class Stack : \"\"\"ZenML stack class. A ZenML stack is a collection of multiple stack components that are required to run ZenML pipelines. Some of these components (orchestrator, metadata store and artifact store) are required to run any kind of pipeline, other components like the container registry are only required if other stack components depend on them. \"\"\" def __init__ ( self , name : str , * , orchestrator : \"BaseOrchestrator\" , metadata_store : \"BaseMetadataStore\" , artifact_store : \"BaseArtifactStore\" , container_registry : Optional [ \"BaseContainerRegistry\" ] = None , ): \"\"\"Initializes and validates a stack instance. Raises: StackValidationError: If the stack configuration is not valid. \"\"\" self . _name = name self . _orchestrator = orchestrator self . _metadata_store = metadata_store self . _artifact_store = artifact_store self . _container_registry = container_registry self . validate () @classmethod def from_components ( cls , name : str , components : Dict [ StackComponentType , \"StackComponent\" ] ) -> \"Stack\" : \"\"\"Creates a stack instance from a dict of stack components. Args: name: The name of the stack. components: The components of the stack. Returns: A stack instance consisting of the given components. Raises: TypeError: If a required component is missing or a component doesn't inherit from the expected base class. \"\"\" from zenml.artifact_stores import BaseArtifactStore from zenml.container_registries import BaseContainerRegistry from zenml.metadata_stores import BaseMetadataStore from zenml.orchestrators import BaseOrchestrator def _raise_type_error ( component : Optional [ \"StackComponent\" ], expected_class : Type [ Any ] ) -> NoReturn : \"\"\"Raises a TypeError that the component has an unexpected type.\"\"\" raise TypeError ( f \"Unable to create stack: Wrong stack component type \" f \"` { component . __class__ . __name__ } ` (expected: subclass \" f \"of ` { expected_class . __name__ } `)\" ) orchestrator = components . get ( StackComponentType . ORCHESTRATOR ) if not isinstance ( orchestrator , BaseOrchestrator ): _raise_type_error ( orchestrator , BaseOrchestrator ) metadata_store = components . get ( StackComponentType . METADATA_STORE ) if not isinstance ( metadata_store , BaseMetadataStore ): _raise_type_error ( metadata_store , BaseMetadataStore ) artifact_store = components . get ( StackComponentType . ARTIFACT_STORE ) if not isinstance ( artifact_store , BaseArtifactStore ): _raise_type_error ( artifact_store , BaseArtifactStore ) container_registry = components . get ( StackComponentType . CONTAINER_REGISTRY ) if container_registry is not None and not isinstance ( container_registry , BaseContainerRegistry ): _raise_type_error ( container_registry , BaseContainerRegistry ) return Stack ( name = name , orchestrator = orchestrator , metadata_store = metadata_store , artifact_store = artifact_store , container_registry = container_registry , ) @classmethod def default_local_stack ( cls ) -> \"Stack\" : \"\"\"Creates a stack instance which is configured to run locally.\"\"\" from zenml.artifact_stores import LocalArtifactStore from zenml.metadata_stores import SQLiteMetadataStore from zenml.orchestrators import LocalOrchestrator orchestrator = LocalOrchestrator ( name = \"local_orchestrator\" ) artifact_store_uuid = uuid . uuid4 () artifact_store_path = os . path . join ( GlobalConfig . config_directory (), \"local_stores\" , str ( artifact_store_uuid ), ) artifact_store = LocalArtifactStore ( name = \"local_artifact_store\" , uuid = artifact_store_uuid , path = artifact_store_path , ) metadata_store_path = os . path . join ( artifact_store_path , \"metadata.db\" ) metadata_store = SQLiteMetadataStore ( name = \"local_metadata_store\" , uri = metadata_store_path ) return cls ( name = \"local_stack\" , orchestrator = orchestrator , metadata_store = metadata_store , artifact_store = artifact_store , ) @property def components ( self ) -> Dict [ StackComponentType , \"StackComponent\" ]: \"\"\"All components of the stack.\"\"\" return { component . type : component for component in [ self . orchestrator , self . metadata_store , self . artifact_store , self . container_registry , ] if component is not None } @property def name ( self ) -> str : \"\"\"The name of the stack.\"\"\" return self . _name @property def orchestrator ( self ) -> \"BaseOrchestrator\" : \"\"\"The orchestrator of the stack.\"\"\" return self . _orchestrator @property def metadata_store ( self ) -> \"BaseMetadataStore\" : \"\"\"The metadata store of the stack.\"\"\" return self . _metadata_store @property def artifact_store ( self ) -> \"BaseArtifactStore\" : \"\"\"The artifact store of the stack.\"\"\" return self . _artifact_store @property def container_registry ( self ) -> Optional [ \"BaseContainerRegistry\" ]: \"\"\"The container registry of the stack.\"\"\" return self . _container_registry # TODO [ENG-371]: Implement CLI method to generate configuration file from a # stack's available runtime options @property def runtime_options ( self ) -> Dict [ str , Any ]: \"\"\"Runtime options that are available to configure this stack. This method combines the available runtime options for all components of this stack. See `StackComponent.runtime_options()` for more information. \"\"\" runtime_options : Dict [ str , Any ] = {} for component in self . components . values (): duplicate_runtime_options = ( runtime_options . keys () & component . runtime_options . keys () ) if duplicate_runtime_options : logger . warning ( \"Found duplicate runtime options %s .\" , duplicate_runtime_options , ) runtime_options . update ( component . runtime_options ) return runtime_options def requirements ( self , exclude_components : Optional [ AbstractSet [ StackComponentType ]] = None , ) -> Set [ str ]: \"\"\"Set of PyPI requirements for the stack. This method combines the requirements of all stack components (except the ones specified in `exclude_components`). Args: exclude_components: Set of component types for which the requirements should not be included in the output. \"\"\" exclude_components = exclude_components or set () requirements = [ component . requirements for component in self . components . values () if component . type not in exclude_components ] return set . union ( * requirements ) if requirements else set () def validate ( self ) -> None : \"\"\"Checks whether the stack configuration is valid. To check if a stack configuration is valid, the following criteria must be met: - all components must support the execution mode (either local or remote execution) specified by the orchestrator of the stack - the `StackValidator` of each stack component has to validate the stack to make sure all the components are compatible with each other Raises: StackValidationError: If the stack configuration is not valid. \"\"\" # TODO [ENG-372]: Differentiate between orchestrators running a pipeline # locally and remotely (potentially using subclasses or an # `orchestrator.mode` property?) and make sure all components support # either local/remote execution for component in self . components . values (): if component . validator : component . validator . validate ( stack = self ) def deploy_pipeline ( self , pipeline : \"BasePipeline\" , runtime_configuration : RuntimeConfiguration , ) -> Any : \"\"\"Deploys a pipeline on this stack. Args: pipeline: The pipeline to deploy. runtime_configuration: Contains all the runtime configuration options specified for the pipeline run. Returns: The return value of the call to `orchestrator.run_pipeline(...)`. \"\"\" for component in self . components . values (): component . prepare_pipeline_deployment ( pipeline = pipeline , stack = self , runtime_configuration = runtime_configuration , ) for component in self . components . values (): component . prepare_pipeline_run () run_name = runtime_configuration . run_name or ( f \" { pipeline . name } -\" f ' { datetime . now () . strftime ( \" %d _%h_%y-%H_%M_%S_ %f \" ) } ' ) logger . info ( \"Using stack ` %s ` to run pipeline ` %s `...\" , self . name , pipeline . name , ) start_time = time . time () return_value = self . orchestrator . run_pipeline ( pipeline , stack = self , run_name = run_name ) run_duration = time . time () - start_time logger . info ( \"Pipeline run ` %s ` has finished in %s .\" , run_name , string_utils . get_human_readable_time ( run_duration ), ) for component in self . components . values (): component . cleanup_pipeline_run () return return_value # TODO [ENG-373]: Include provisioning status in CLI `zenml stack describe` # and `zenml stack-component describe` commands @property def is_provisioned ( self ) -> bool : \"\"\"If the stack provisioned resources to run locally.\"\"\" return all ( component . is_provisioned for component in self . components . values () ) @property def is_running ( self ) -> bool : \"\"\"If the stack is running locally.\"\"\" return all ( component . is_running for component in self . components . values () ) def provision ( self ) -> None : \"\"\"Provisions resources to run the stack locally. Raises: NotImplementedError: If any unprovisioned component does not implement provisioning. \"\"\" logger . info ( \"Provisioning resources for stack ' %s '.\" , self . name ) for component in self . components . values (): if not component . is_provisioned : component . provision () logger . info ( \"Provisioned resources for %s .\" , component ) def deprovision ( self ) -> None : \"\"\"Deprovisions all local resources of the stack. Raises: NotImplementedError: If any provisioned component does not implement deprovisioning. \"\"\" logger . info ( \"Deprovisioning resources for stack ' %s '.\" , self . name ) for component in self . components . values (): if component . is_provisioned : try : component . deprovision () logger . info ( \"Deprovisioned resources for %s .\" , component ) except NotImplementedError as e : logger . warning ( e ) def resume ( self ) -> None : \"\"\"Resumes the provisioned local resources of the stack. Raises: ProvisioningError: If any stack component is missing provisioned resources. \"\"\" logger . info ( \"Resuming provisioned resources for stack ' %s '.\" , self . name ) for component in self . components . values (): if component . is_running : # the component is already running, no need to resume anything pass elif component . is_provisioned : component . resume () logger . info ( \"Resumed resources for %s .\" , component ) else : raise ProvisioningError ( f \"Unable to resume resources for { component } : No \" f \"resources have been provisioned for this component.\" ) def suspend ( self ) -> None : \"\"\"Suspends the provisioned local resources of the stack.\"\"\" logger . info ( \"Suspending provisioned resources for stack ' %s '.\" , self . name ) for component in self . components . values (): if component . is_running : try : component . suspend () logger . info ( \"Suspended resources for %s .\" , component ) except NotImplementedError : logger . warning ( \"Suspending provisioned resources not implemented \" \"for %s . Continuing without suspending resources...\" , component , ) artifact_store : BaseArtifactStore property readonly The artifact store of the stack. components : Dict [ zenml . enums . StackComponentType , StackComponent ] property readonly All components of the stack. container_registry : Optional [ BaseContainerRegistry ] property readonly The container registry of the stack. is_provisioned : bool property readonly If the stack provisioned resources to run locally. is_running : bool property readonly If the stack is running locally. metadata_store : BaseMetadataStore property readonly The metadata store of the stack. name : str property readonly The name of the stack. orchestrator : BaseOrchestrator property readonly The orchestrator of the stack. runtime_options : Dict [ str , Any ] property readonly Runtime options that are available to configure this stack. This method combines the available runtime options for all components of this stack. See StackComponent.runtime_options() for more information. __init__ ( self , name , * , orchestrator , metadata_store , artifact_store , container_registry = None ) special Initializes and validates a stack instance. Exceptions: Type Description StackValidationError If the stack configuration is not valid. Source code in zenml/stack/stack.py def __init__ ( self , name : str , * , orchestrator : \"BaseOrchestrator\" , metadata_store : \"BaseMetadataStore\" , artifact_store : \"BaseArtifactStore\" , container_registry : Optional [ \"BaseContainerRegistry\" ] = None , ): \"\"\"Initializes and validates a stack instance. Raises: StackValidationError: If the stack configuration is not valid. \"\"\" self . _name = name self . _orchestrator = orchestrator self . _metadata_store = metadata_store self . _artifact_store = artifact_store self . _container_registry = container_registry self . validate () default_local_stack () classmethod Creates a stack instance which is configured to run locally. Source code in zenml/stack/stack.py @classmethod def default_local_stack ( cls ) -> \"Stack\" : \"\"\"Creates a stack instance which is configured to run locally.\"\"\" from zenml.artifact_stores import LocalArtifactStore from zenml.metadata_stores import SQLiteMetadataStore from zenml.orchestrators import LocalOrchestrator orchestrator = LocalOrchestrator ( name = \"local_orchestrator\" ) artifact_store_uuid = uuid . uuid4 () artifact_store_path = os . path . join ( GlobalConfig . config_directory (), \"local_stores\" , str ( artifact_store_uuid ), ) artifact_store = LocalArtifactStore ( name = \"local_artifact_store\" , uuid = artifact_store_uuid , path = artifact_store_path , ) metadata_store_path = os . path . join ( artifact_store_path , \"metadata.db\" ) metadata_store = SQLiteMetadataStore ( name = \"local_metadata_store\" , uri = metadata_store_path ) return cls ( name = \"local_stack\" , orchestrator = orchestrator , metadata_store = metadata_store , artifact_store = artifact_store , ) deploy_pipeline ( self , pipeline , runtime_configuration ) Deploys a pipeline on this stack. Parameters: Name Type Description Default pipeline BasePipeline The pipeline to deploy. required runtime_configuration RuntimeConfiguration Contains all the runtime configuration options specified for the pipeline run. required Returns: Type Description Any The return value of the call to orchestrator.run_pipeline(...) . Source code in zenml/stack/stack.py def deploy_pipeline ( self , pipeline : \"BasePipeline\" , runtime_configuration : RuntimeConfiguration , ) -> Any : \"\"\"Deploys a pipeline on this stack. Args: pipeline: The pipeline to deploy. runtime_configuration: Contains all the runtime configuration options specified for the pipeline run. Returns: The return value of the call to `orchestrator.run_pipeline(...)`. \"\"\" for component in self . components . values (): component . prepare_pipeline_deployment ( pipeline = pipeline , stack = self , runtime_configuration = runtime_configuration , ) for component in self . components . values (): component . prepare_pipeline_run () run_name = runtime_configuration . run_name or ( f \" { pipeline . name } -\" f ' { datetime . now () . strftime ( \" %d _%h_%y-%H_%M_%S_ %f \" ) } ' ) logger . info ( \"Using stack ` %s ` to run pipeline ` %s `...\" , self . name , pipeline . name , ) start_time = time . time () return_value = self . orchestrator . run_pipeline ( pipeline , stack = self , run_name = run_name ) run_duration = time . time () - start_time logger . info ( \"Pipeline run ` %s ` has finished in %s .\" , run_name , string_utils . get_human_readable_time ( run_duration ), ) for component in self . components . values (): component . cleanup_pipeline_run () return return_value deprovision ( self ) Deprovisions all local resources of the stack. Exceptions: Type Description NotImplementedError If any provisioned component does not implement deprovisioning. Source code in zenml/stack/stack.py def deprovision ( self ) -> None : \"\"\"Deprovisions all local resources of the stack. Raises: NotImplementedError: If any provisioned component does not implement deprovisioning. \"\"\" logger . info ( \"Deprovisioning resources for stack ' %s '.\" , self . name ) for component in self . components . values (): if component . is_provisioned : try : component . deprovision () logger . info ( \"Deprovisioned resources for %s .\" , component ) except NotImplementedError as e : logger . warning ( e ) from_components ( name , components ) classmethod Creates a stack instance from a dict of stack components. Parameters: Name Type Description Default name str The name of the stack. required components Dict[zenml.enums.StackComponentType, StackComponent] The components of the stack. required Returns: Type Description Stack A stack instance consisting of the given components. Exceptions: Type Description TypeError If a required component is missing or a component doesn't inherit from the expected base class. Source code in zenml/stack/stack.py @classmethod def from_components ( cls , name : str , components : Dict [ StackComponentType , \"StackComponent\" ] ) -> \"Stack\" : \"\"\"Creates a stack instance from a dict of stack components. Args: name: The name of the stack. components: The components of the stack. Returns: A stack instance consisting of the given components. Raises: TypeError: If a required component is missing or a component doesn't inherit from the expected base class. \"\"\" from zenml.artifact_stores import BaseArtifactStore from zenml.container_registries import BaseContainerRegistry from zenml.metadata_stores import BaseMetadataStore from zenml.orchestrators import BaseOrchestrator def _raise_type_error ( component : Optional [ \"StackComponent\" ], expected_class : Type [ Any ] ) -> NoReturn : \"\"\"Raises a TypeError that the component has an unexpected type.\"\"\" raise TypeError ( f \"Unable to create stack: Wrong stack component type \" f \"` { component . __class__ . __name__ } ` (expected: subclass \" f \"of ` { expected_class . __name__ } `)\" ) orchestrator = components . get ( StackComponentType . ORCHESTRATOR ) if not isinstance ( orchestrator , BaseOrchestrator ): _raise_type_error ( orchestrator , BaseOrchestrator ) metadata_store = components . get ( StackComponentType . METADATA_STORE ) if not isinstance ( metadata_store , BaseMetadataStore ): _raise_type_error ( metadata_store , BaseMetadataStore ) artifact_store = components . get ( StackComponentType . ARTIFACT_STORE ) if not isinstance ( artifact_store , BaseArtifactStore ): _raise_type_error ( artifact_store , BaseArtifactStore ) container_registry = components . get ( StackComponentType . CONTAINER_REGISTRY ) if container_registry is not None and not isinstance ( container_registry , BaseContainerRegistry ): _raise_type_error ( container_registry , BaseContainerRegistry ) return Stack ( name = name , orchestrator = orchestrator , metadata_store = metadata_store , artifact_store = artifact_store , container_registry = container_registry , ) provision ( self ) Provisions resources to run the stack locally. Exceptions: Type Description NotImplementedError If any unprovisioned component does not implement provisioning. Source code in zenml/stack/stack.py def provision ( self ) -> None : \"\"\"Provisions resources to run the stack locally. Raises: NotImplementedError: If any unprovisioned component does not implement provisioning. \"\"\" logger . info ( \"Provisioning resources for stack ' %s '.\" , self . name ) for component in self . components . values (): if not component . is_provisioned : component . provision () logger . info ( \"Provisioned resources for %s .\" , component ) requirements ( self , exclude_components = None ) Set of PyPI requirements for the stack. This method combines the requirements of all stack components (except the ones specified in exclude_components ). Parameters: Name Type Description Default exclude_components Optional[AbstractSet[zenml.enums.StackComponentType]] Set of component types for which the requirements should not be included in the output. None Source code in zenml/stack/stack.py def requirements ( self , exclude_components : Optional [ AbstractSet [ StackComponentType ]] = None , ) -> Set [ str ]: \"\"\"Set of PyPI requirements for the stack. This method combines the requirements of all stack components (except the ones specified in `exclude_components`). Args: exclude_components: Set of component types for which the requirements should not be included in the output. \"\"\" exclude_components = exclude_components or set () requirements = [ component . requirements for component in self . components . values () if component . type not in exclude_components ] return set . union ( * requirements ) if requirements else set () resume ( self ) Resumes the provisioned local resources of the stack. Exceptions: Type Description ProvisioningError If any stack component is missing provisioned resources. Source code in zenml/stack/stack.py def resume ( self ) -> None : \"\"\"Resumes the provisioned local resources of the stack. Raises: ProvisioningError: If any stack component is missing provisioned resources. \"\"\" logger . info ( \"Resuming provisioned resources for stack ' %s '.\" , self . name ) for component in self . components . values (): if component . is_running : # the component is already running, no need to resume anything pass elif component . is_provisioned : component . resume () logger . info ( \"Resumed resources for %s .\" , component ) else : raise ProvisioningError ( f \"Unable to resume resources for { component } : No \" f \"resources have been provisioned for this component.\" ) suspend ( self ) Suspends the provisioned local resources of the stack. Source code in zenml/stack/stack.py def suspend ( self ) -> None : \"\"\"Suspends the provisioned local resources of the stack.\"\"\" logger . info ( \"Suspending provisioned resources for stack ' %s '.\" , self . name ) for component in self . components . values (): if component . is_running : try : component . suspend () logger . info ( \"Suspended resources for %s .\" , component ) except NotImplementedError : logger . warning ( \"Suspending provisioned resources not implemented \" \"for %s . Continuing without suspending resources...\" , component , ) validate ( self ) Checks whether the stack configuration is valid. To check if a stack configuration is valid, the following criteria must be met: - all components must support the execution mode (either local or remote execution) specified by the orchestrator of the stack - the StackValidator of each stack component has to validate the stack to make sure all the components are compatible with each other Exceptions: Type Description StackValidationError If the stack configuration is not valid. Source code in zenml/stack/stack.py def validate ( self ) -> None : \"\"\"Checks whether the stack configuration is valid. To check if a stack configuration is valid, the following criteria must be met: - all components must support the execution mode (either local or remote execution) specified by the orchestrator of the stack - the `StackValidator` of each stack component has to validate the stack to make sure all the components are compatible with each other Raises: StackValidationError: If the stack configuration is not valid. \"\"\" # TODO [ENG-372]: Differentiate between orchestrators running a pipeline # locally and remotely (potentially using subclasses or an # `orchestrator.mode` property?) and make sure all components support # either local/remote execution for component in self . components . values (): if component . validator : component . validator . validate ( stack = self ) stack_component StackComponent ( BaseModel , ABC ) pydantic-model Abstract StackComponent class for all components of a ZenML stack. Attributes: Name Type Description name str The name of the component. uuid UUID Unique identifier of the component. supports_local_execution bool If the component supports running locally. supports_remote_execution bool If the component supports running remotely. Source code in zenml/stack/stack_component.py class StackComponent ( BaseModel , ABC ): \"\"\"Abstract StackComponent class for all components of a ZenML stack. Attributes: name: The name of the component. uuid: Unique identifier of the component. supports_local_execution: If the component supports running locally. supports_remote_execution: If the component supports running remotely. \"\"\" name : str uuid : UUID = Field ( default_factory = uuid4 ) supports_local_execution : bool supports_remote_execution : bool @property @abstractmethod def type ( self ) -> StackComponentType : \"\"\"The component type.\"\"\" @property @abstractmethod def flavor ( self ) -> StackComponentFlavor : \"\"\"The component flavor.\"\"\" @property def log_file ( self ) -> Optional [ str ]: \"\"\"Optional path to a log file for the stack component.\"\"\" # TODO [ENG-136]: Add support for multiple log files for a stack # component. E.g. let each component return a generator that yields # logs instead of specifying a single file path. return None @property def runtime_options ( self ) -> Dict [ str , Any ]: \"\"\"Runtime options that are available to configure this component. The items of the dictionary should map option names (which can be used to configure the option in the `RuntimeConfiguration`) to default values for the option (or `None` if there is no default value). \"\"\" return {} @property def requirements ( self ) -> Set [ str ]: \"\"\"Set of PyPI requirements for the component.\"\"\" return set ( get_requirements_for_module ( self . __module__ )) def prepare_pipeline_deployment ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , runtime_configuration : \"RuntimeConfiguration\" , ) -> None : \"\"\"Prepares deploying the pipeline. This method gets called immediately before a pipeline is deployed. Subclasses should override it if they require runtime configuration options or if they need to run code before the pipeline deployment. Args: pipeline: The pipeline that will be deployed. stack: The stack on which the pipeline will be deployed. runtime_configuration: Contains all the runtime configuration options specified for the pipeline run. \"\"\" def prepare_pipeline_run ( self ) -> None : \"\"\"Prepares running the pipeline.\"\"\" def cleanup_pipeline_run ( self ) -> None : \"\"\"Cleans up resources after the pipeline run is finished.\"\"\" @property def validator ( self ) -> Optional [ \"StackValidator\" ]: \"\"\"The optional validator of the stack component. This validator will be called each time a stack with the stack component is initialized. Subclasses should override this property and return a `StackValidator` that makes sure they're not included in any stack that they're not compatible with. \"\"\" return None @property def is_provisioned ( self ) -> bool : \"\"\"If the component provisioned resources to run locally.\"\"\" return True @property def is_running ( self ) -> bool : \"\"\"If the component is running locally.\"\"\" return True def provision ( self ) -> None : \"\"\"Provisions resources to run the component locally.\"\"\" raise NotImplementedError ( f \"Provisioning local resources not implemented for { self } .\" ) def deprovision ( self ) -> None : \"\"\"Deprovisions all local resources of the component.\"\"\" raise NotImplementedError ( f \"Deprovisioning local resource not implemented for { self } .\" ) def resume ( self ) -> None : \"\"\"Resumes the provisioned local resources of the component.\"\"\" raise NotImplementedError ( f \"Resuming provisioned resources not implemented for { self } .\" ) def suspend ( self ) -> None : \"\"\"Suspends the provisioned local resources of the component.\"\"\" raise NotImplementedError ( f \"Suspending provisioned resources not implemented for { self } .\" ) def __repr__ ( self ) -> str : \"\"\"String representation of the stack component.\"\"\" attribute_representation = \", \" . join ( f \" { key } = { value } \" for key , value in self . dict () . items () ) return ( f \" { self . __class__ . __qualname__ } (type= { self . type } , \" f \"flavor= { self . flavor } , { attribute_representation } )\" ) def __str__ ( self ) -> str : \"\"\"String representation of the stack component.\"\"\" return self . __repr__ () class Config : \"\"\"Pydantic configuration class.\"\"\" # public attributes are immutable allow_mutation = False # all attributes with leading underscore are private and therefore # are mutable and not included in serialization underscore_attrs_are_private = True # exclude these two fields from being serialized fields = { \"supports_local_execution\" : { \"exclude\" : True }, \"supports_remote_execution\" : { \"exclude\" : True }, } flavor : StackComponentFlavor property readonly The component flavor. is_provisioned : bool property readonly If the component provisioned resources to run locally. is_running : bool property readonly If the component is running locally. log_file : Optional [ str ] property readonly Optional path to a log file for the stack component. requirements : Set [ str ] property readonly Set of PyPI requirements for the component. runtime_options : Dict [ str , Any ] property readonly Runtime options that are available to configure this component. The items of the dictionary should map option names (which can be used to configure the option in the RuntimeConfiguration ) to default values for the option (or None if there is no default value). type : StackComponentType property readonly The component type. validator : Optional [ StackValidator ] property readonly The optional validator of the stack component. This validator will be called each time a stack with the stack component is initialized. Subclasses should override this property and return a StackValidator that makes sure they're not included in any stack that they're not compatible with. Config Pydantic configuration class. Source code in zenml/stack/stack_component.py class Config : \"\"\"Pydantic configuration class.\"\"\" # public attributes are immutable allow_mutation = False # all attributes with leading underscore are private and therefore # are mutable and not included in serialization underscore_attrs_are_private = True # exclude these two fields from being serialized fields = { \"supports_local_execution\" : { \"exclude\" : True }, \"supports_remote_execution\" : { \"exclude\" : True }, } __repr__ ( self ) special String representation of the stack component. Source code in zenml/stack/stack_component.py def __repr__ ( self ) -> str : \"\"\"String representation of the stack component.\"\"\" attribute_representation = \", \" . join ( f \" { key } = { value } \" for key , value in self . dict () . items () ) return ( f \" { self . __class__ . __qualname__ } (type= { self . type } , \" f \"flavor= { self . flavor } , { attribute_representation } )\" ) __str__ ( self ) special String representation of the stack component. Source code in zenml/stack/stack_component.py def __str__ ( self ) -> str : \"\"\"String representation of the stack component.\"\"\" return self . __repr__ () cleanup_pipeline_run ( self ) Cleans up resources after the pipeline run is finished. Source code in zenml/stack/stack_component.py def cleanup_pipeline_run ( self ) -> None : \"\"\"Cleans up resources after the pipeline run is finished.\"\"\" deprovision ( self ) Deprovisions all local resources of the component. Source code in zenml/stack/stack_component.py def deprovision ( self ) -> None : \"\"\"Deprovisions all local resources of the component.\"\"\" raise NotImplementedError ( f \"Deprovisioning local resource not implemented for { self } .\" ) prepare_pipeline_deployment ( self , pipeline , stack , runtime_configuration ) Prepares deploying the pipeline. This method gets called immediately before a pipeline is deployed. Subclasses should override it if they require runtime configuration options or if they need to run code before the pipeline deployment. Parameters: Name Type Description Default pipeline BasePipeline The pipeline that will be deployed. required stack Stack The stack on which the pipeline will be deployed. required runtime_configuration RuntimeConfiguration Contains all the runtime configuration options specified for the pipeline run. required Source code in zenml/stack/stack_component.py def prepare_pipeline_deployment ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , runtime_configuration : \"RuntimeConfiguration\" , ) -> None : \"\"\"Prepares deploying the pipeline. This method gets called immediately before a pipeline is deployed. Subclasses should override it if they require runtime configuration options or if they need to run code before the pipeline deployment. Args: pipeline: The pipeline that will be deployed. stack: The stack on which the pipeline will be deployed. runtime_configuration: Contains all the runtime configuration options specified for the pipeline run. \"\"\" prepare_pipeline_run ( self ) Prepares running the pipeline. Source code in zenml/stack/stack_component.py def prepare_pipeline_run ( self ) -> None : \"\"\"Prepares running the pipeline.\"\"\" provision ( self ) Provisions resources to run the component locally. Source code in zenml/stack/stack_component.py def provision ( self ) -> None : \"\"\"Provisions resources to run the component locally.\"\"\" raise NotImplementedError ( f \"Provisioning local resources not implemented for { self } .\" ) resume ( self ) Resumes the provisioned local resources of the component. Source code in zenml/stack/stack_component.py def resume ( self ) -> None : \"\"\"Resumes the provisioned local resources of the component.\"\"\" raise NotImplementedError ( f \"Resuming provisioned resources not implemented for { self } .\" ) suspend ( self ) Suspends the provisioned local resources of the component. Source code in zenml/stack/stack_component.py def suspend ( self ) -> None : \"\"\"Suspends the provisioned local resources of the component.\"\"\" raise NotImplementedError ( f \"Suspending provisioned resources not implemented for { self } .\" ) stack_component_class_registry StackComponentClassRegistry Registry for stack component classes. All stack component classes must be registered here so they can be instantiated from the component type and flavor specified inside the ZenML repository configuration. Source code in zenml/stack/stack_component_class_registry.py class StackComponentClassRegistry : \"\"\"Registry for stack component classes. All stack component classes must be registered here so they can be instantiated from the component type and flavor specified inside the ZenML repository configuration. \"\"\" component_classes : ClassVar [ DefaultDict [ StackComponentType , Dict [ str , Type [ StackComponent ]]] ] = defaultdict ( dict ) @classmethod def register_class ( cls , component_type : StackComponentType , component_flavor : StackComponentFlavor , component_class : Type [ StackComponent ], ) -> None : \"\"\"Registers a stack component class. Args: component_type: The type of the component class to register. component_flavor: The flavor of the component class to register. component_class: The component class to register. \"\"\" component_flavor = component_flavor . value flavors = cls . component_classes [ component_type ] if component_flavor in flavors : logger . warning ( \"Overwriting previously registered stack component class ` %s ` \" \"for type ' %s ' and flavor ' %s '.\" , flavors [ component_flavor ] . __class__ . __name__ , component_type . value , component_flavor , ) flavors [ component_flavor ] = component_class logger . debug ( \"Registered stack component class for type ' %s ' and flavor ' %s '.\" , component_type . value , component_flavor , ) @classmethod def get_class ( cls , component_type : StackComponentType , component_flavor : Union [ StackComponentFlavor , str ], ) -> Type [ StackComponent ]: \"\"\"Returns the stack component class for the given type and flavor. Args: component_type: The type of the component class to return. component_flavor: The flavor of the component class to return. Raises: KeyError: If no component class is registered for the given type and flavor. \"\"\" # TODO [ENG-374]: Think about activating the integrations here to make # sure all potential StackComponent classes are registered if isinstance ( component_flavor , StackComponentFlavor ): component_flavor = component_flavor . value available_flavors = cls . component_classes [ component_type ] try : return available_flavors [ component_flavor ] except KeyError : raise KeyError ( f \"No stack component class found for type { component_type } \" f \"and flavor { component_flavor } . Registered flavors for this \" f \"type: { set ( available_flavors ) } . If your stack component \" f \"class is part of a ZenML integration, make sure to active \" f \"them by calling \" f \"`IntegrationRegistry.activate_integrations()`.\" ) from None get_class ( component_type , component_flavor ) classmethod Returns the stack component class for the given type and flavor. Parameters: Name Type Description Default component_type StackComponentType The type of the component class to return. required component_flavor Union[zenml.enums.StackComponentFlavor, str] The flavor of the component class to return. required Exceptions: Type Description KeyError If no component class is registered for the given type and flavor. Source code in zenml/stack/stack_component_class_registry.py @classmethod def get_class ( cls , component_type : StackComponentType , component_flavor : Union [ StackComponentFlavor , str ], ) -> Type [ StackComponent ]: \"\"\"Returns the stack component class for the given type and flavor. Args: component_type: The type of the component class to return. component_flavor: The flavor of the component class to return. Raises: KeyError: If no component class is registered for the given type and flavor. \"\"\" # TODO [ENG-374]: Think about activating the integrations here to make # sure all potential StackComponent classes are registered if isinstance ( component_flavor , StackComponentFlavor ): component_flavor = component_flavor . value available_flavors = cls . component_classes [ component_type ] try : return available_flavors [ component_flavor ] except KeyError : raise KeyError ( f \"No stack component class found for type { component_type } \" f \"and flavor { component_flavor } . Registered flavors for this \" f \"type: { set ( available_flavors ) } . If your stack component \" f \"class is part of a ZenML integration, make sure to active \" f \"them by calling \" f \"`IntegrationRegistry.activate_integrations()`.\" ) from None register_class ( component_type , component_flavor , component_class ) classmethod Registers a stack component class. Parameters: Name Type Description Default component_type StackComponentType The type of the component class to register. required component_flavor StackComponentFlavor The flavor of the component class to register. required component_class Type[zenml.stack.stack_component.StackComponent] The component class to register. required Source code in zenml/stack/stack_component_class_registry.py @classmethod def register_class ( cls , component_type : StackComponentType , component_flavor : StackComponentFlavor , component_class : Type [ StackComponent ], ) -> None : \"\"\"Registers a stack component class. Args: component_type: The type of the component class to register. component_flavor: The flavor of the component class to register. component_class: The component class to register. \"\"\" component_flavor = component_flavor . value flavors = cls . component_classes [ component_type ] if component_flavor in flavors : logger . warning ( \"Overwriting previously registered stack component class ` %s ` \" \"for type ' %s ' and flavor ' %s '.\" , flavors [ component_flavor ] . __class__ . __name__ , component_type . value , component_flavor , ) flavors [ component_flavor ] = component_class logger . debug ( \"Registered stack component class for type ' %s ' and flavor ' %s '.\" , component_type . value , component_flavor , ) register_stack_component_class ( component_type , component_flavor ) Parametrized decorator function to register stack component classes. Parameters: Name Type Description Default component_type StackComponentType The type of the component class to register. required component_flavor StackComponentFlavor The flavor of the component class to register. required Returns: Type Description Callable[[Type[~C]], Type[~C]] A decorator function that registers and returns the decorated stack component class. Source code in zenml/stack/stack_component_class_registry.py def register_stack_component_class ( component_type : StackComponentType , component_flavor : StackComponentFlavor ) -> Callable [[ Type [ C ]], Type [ C ]]: \"\"\"Parametrized decorator function to register stack component classes. Args: component_type: The type of the component class to register. component_flavor: The flavor of the component class to register. Returns: A decorator function that registers and returns the decorated stack component class. \"\"\" def decorator_function ( cls : Type [ C ]) -> Type [ C ]: \"\"\"Registers the stack component class and returns it unmodified.\"\"\" StackComponentClassRegistry . register_class ( component_type = component_type , component_flavor = component_flavor , component_class = cls , ) return cls return decorator_function stack_validator StackValidator A StackValidator is used to validate a stack configuration. Each StackComponent can provide a StackValidator to make sure it is compatible with all components of the stack. The KubeflowOrchestrator for example will always require the stack to have a container registry in order to push the docker images that are required to run a pipeline in Kubeflow Pipelines. Source code in zenml/stack/stack_validator.py class StackValidator : \"\"\"A `StackValidator` is used to validate a stack configuration. Each `StackComponent` can provide a `StackValidator` to make sure it is compatible with all components of the stack. The `KubeflowOrchestrator` for example will always require the stack to have a container registry in order to push the docker images that are required to run a pipeline in Kubeflow Pipelines. \"\"\" def __init__ ( self , required_components : Optional [ AbstractSet [ StackComponentType ]] = None , custom_validation_function : Optional [ Callable [[ \"Stack\" ], bool ]] = None , ): \"\"\"Initializes a `StackValidator` instance. Args: required_components: Optional set of stack components that must exist in the stack. custom_validation_function: Optional function that returns whether a stack is valid. \"\"\" self . _required_components = required_components or set () self . _custom_validation_function = custom_validation_function def validate ( self , stack : \"Stack\" ) -> None : \"\"\"Validates the given stack. Checks if the stack contains all the required components and passes the custom validation function of the validator. Raises: StackValidationError: If the stack does not meet all the validation criteria. \"\"\" missing_components = self . _required_components - set ( stack . components ) if missing_components : raise StackValidationError ( f \"Missing stack components { missing_components } for \" f \"stack: { stack } \" ) if ( self . _custom_validation_function and not self . _custom_validation_function ( stack ) ): raise StackValidationError ( f \"Custom validation function failed to validate \" f \"stack: { stack } \" ) __init__ ( self , required_components = None , custom_validation_function = None ) special Initializes a StackValidator instance. Parameters: Name Type Description Default required_components Optional[AbstractSet[zenml.enums.StackComponentType]] Optional set of stack components that must exist in the stack. None custom_validation_function Optional[Callable[[Stack], bool]] Optional function that returns whether a stack is valid. None Source code in zenml/stack/stack_validator.py def __init__ ( self , required_components : Optional [ AbstractSet [ StackComponentType ]] = None , custom_validation_function : Optional [ Callable [[ \"Stack\" ], bool ]] = None , ): \"\"\"Initializes a `StackValidator` instance. Args: required_components: Optional set of stack components that must exist in the stack. custom_validation_function: Optional function that returns whether a stack is valid. \"\"\" self . _required_components = required_components or set () self . _custom_validation_function = custom_validation_function validate ( self , stack ) Validates the given stack. Checks if the stack contains all the required components and passes the custom validation function of the validator. Exceptions: Type Description StackValidationError If the stack does not meet all the validation criteria. Source code in zenml/stack/stack_validator.py def validate ( self , stack : \"Stack\" ) -> None : \"\"\"Validates the given stack. Checks if the stack contains all the required components and passes the custom validation function of the validator. Raises: StackValidationError: If the stack does not meet all the validation criteria. \"\"\" missing_components = self . _required_components - set ( stack . components ) if missing_components : raise StackValidationError ( f \"Missing stack components { missing_components } for \" f \"stack: { stack } \" ) if ( self . _custom_validation_function and not self . _custom_validation_function ( stack ) ): raise StackValidationError ( f \"Custom validation function failed to validate \" f \"stack: { stack } \" )","title":"Stack"},{"location":"api_docs/stack/#stack","text":"","title":"Stack"},{"location":"api_docs/stack/#zenml.stack","text":"","title":"stack"},{"location":"api_docs/stack/#zenml.stack.stack","text":"","title":"stack"},{"location":"api_docs/stack/#zenml.stack.stack.Stack","text":"ZenML stack class. A ZenML stack is a collection of multiple stack components that are required to run ZenML pipelines. Some of these components (orchestrator, metadata store and artifact store) are required to run any kind of pipeline, other components like the container registry are only required if other stack components depend on them. Source code in zenml/stack/stack.py class Stack : \"\"\"ZenML stack class. A ZenML stack is a collection of multiple stack components that are required to run ZenML pipelines. Some of these components (orchestrator, metadata store and artifact store) are required to run any kind of pipeline, other components like the container registry are only required if other stack components depend on them. \"\"\" def __init__ ( self , name : str , * , orchestrator : \"BaseOrchestrator\" , metadata_store : \"BaseMetadataStore\" , artifact_store : \"BaseArtifactStore\" , container_registry : Optional [ \"BaseContainerRegistry\" ] = None , ): \"\"\"Initializes and validates a stack instance. Raises: StackValidationError: If the stack configuration is not valid. \"\"\" self . _name = name self . _orchestrator = orchestrator self . _metadata_store = metadata_store self . _artifact_store = artifact_store self . _container_registry = container_registry self . validate () @classmethod def from_components ( cls , name : str , components : Dict [ StackComponentType , \"StackComponent\" ] ) -> \"Stack\" : \"\"\"Creates a stack instance from a dict of stack components. Args: name: The name of the stack. components: The components of the stack. Returns: A stack instance consisting of the given components. Raises: TypeError: If a required component is missing or a component doesn't inherit from the expected base class. \"\"\" from zenml.artifact_stores import BaseArtifactStore from zenml.container_registries import BaseContainerRegistry from zenml.metadata_stores import BaseMetadataStore from zenml.orchestrators import BaseOrchestrator def _raise_type_error ( component : Optional [ \"StackComponent\" ], expected_class : Type [ Any ] ) -> NoReturn : \"\"\"Raises a TypeError that the component has an unexpected type.\"\"\" raise TypeError ( f \"Unable to create stack: Wrong stack component type \" f \"` { component . __class__ . __name__ } ` (expected: subclass \" f \"of ` { expected_class . __name__ } `)\" ) orchestrator = components . get ( StackComponentType . ORCHESTRATOR ) if not isinstance ( orchestrator , BaseOrchestrator ): _raise_type_error ( orchestrator , BaseOrchestrator ) metadata_store = components . get ( StackComponentType . METADATA_STORE ) if not isinstance ( metadata_store , BaseMetadataStore ): _raise_type_error ( metadata_store , BaseMetadataStore ) artifact_store = components . get ( StackComponentType . ARTIFACT_STORE ) if not isinstance ( artifact_store , BaseArtifactStore ): _raise_type_error ( artifact_store , BaseArtifactStore ) container_registry = components . get ( StackComponentType . CONTAINER_REGISTRY ) if container_registry is not None and not isinstance ( container_registry , BaseContainerRegistry ): _raise_type_error ( container_registry , BaseContainerRegistry ) return Stack ( name = name , orchestrator = orchestrator , metadata_store = metadata_store , artifact_store = artifact_store , container_registry = container_registry , ) @classmethod def default_local_stack ( cls ) -> \"Stack\" : \"\"\"Creates a stack instance which is configured to run locally.\"\"\" from zenml.artifact_stores import LocalArtifactStore from zenml.metadata_stores import SQLiteMetadataStore from zenml.orchestrators import LocalOrchestrator orchestrator = LocalOrchestrator ( name = \"local_orchestrator\" ) artifact_store_uuid = uuid . uuid4 () artifact_store_path = os . path . join ( GlobalConfig . config_directory (), \"local_stores\" , str ( artifact_store_uuid ), ) artifact_store = LocalArtifactStore ( name = \"local_artifact_store\" , uuid = artifact_store_uuid , path = artifact_store_path , ) metadata_store_path = os . path . join ( artifact_store_path , \"metadata.db\" ) metadata_store = SQLiteMetadataStore ( name = \"local_metadata_store\" , uri = metadata_store_path ) return cls ( name = \"local_stack\" , orchestrator = orchestrator , metadata_store = metadata_store , artifact_store = artifact_store , ) @property def components ( self ) -> Dict [ StackComponentType , \"StackComponent\" ]: \"\"\"All components of the stack.\"\"\" return { component . type : component for component in [ self . orchestrator , self . metadata_store , self . artifact_store , self . container_registry , ] if component is not None } @property def name ( self ) -> str : \"\"\"The name of the stack.\"\"\" return self . _name @property def orchestrator ( self ) -> \"BaseOrchestrator\" : \"\"\"The orchestrator of the stack.\"\"\" return self . _orchestrator @property def metadata_store ( self ) -> \"BaseMetadataStore\" : \"\"\"The metadata store of the stack.\"\"\" return self . _metadata_store @property def artifact_store ( self ) -> \"BaseArtifactStore\" : \"\"\"The artifact store of the stack.\"\"\" return self . _artifact_store @property def container_registry ( self ) -> Optional [ \"BaseContainerRegistry\" ]: \"\"\"The container registry of the stack.\"\"\" return self . _container_registry # TODO [ENG-371]: Implement CLI method to generate configuration file from a # stack's available runtime options @property def runtime_options ( self ) -> Dict [ str , Any ]: \"\"\"Runtime options that are available to configure this stack. This method combines the available runtime options for all components of this stack. See `StackComponent.runtime_options()` for more information. \"\"\" runtime_options : Dict [ str , Any ] = {} for component in self . components . values (): duplicate_runtime_options = ( runtime_options . keys () & component . runtime_options . keys () ) if duplicate_runtime_options : logger . warning ( \"Found duplicate runtime options %s .\" , duplicate_runtime_options , ) runtime_options . update ( component . runtime_options ) return runtime_options def requirements ( self , exclude_components : Optional [ AbstractSet [ StackComponentType ]] = None , ) -> Set [ str ]: \"\"\"Set of PyPI requirements for the stack. This method combines the requirements of all stack components (except the ones specified in `exclude_components`). Args: exclude_components: Set of component types for which the requirements should not be included in the output. \"\"\" exclude_components = exclude_components or set () requirements = [ component . requirements for component in self . components . values () if component . type not in exclude_components ] return set . union ( * requirements ) if requirements else set () def validate ( self ) -> None : \"\"\"Checks whether the stack configuration is valid. To check if a stack configuration is valid, the following criteria must be met: - all components must support the execution mode (either local or remote execution) specified by the orchestrator of the stack - the `StackValidator` of each stack component has to validate the stack to make sure all the components are compatible with each other Raises: StackValidationError: If the stack configuration is not valid. \"\"\" # TODO [ENG-372]: Differentiate between orchestrators running a pipeline # locally and remotely (potentially using subclasses or an # `orchestrator.mode` property?) and make sure all components support # either local/remote execution for component in self . components . values (): if component . validator : component . validator . validate ( stack = self ) def deploy_pipeline ( self , pipeline : \"BasePipeline\" , runtime_configuration : RuntimeConfiguration , ) -> Any : \"\"\"Deploys a pipeline on this stack. Args: pipeline: The pipeline to deploy. runtime_configuration: Contains all the runtime configuration options specified for the pipeline run. Returns: The return value of the call to `orchestrator.run_pipeline(...)`. \"\"\" for component in self . components . values (): component . prepare_pipeline_deployment ( pipeline = pipeline , stack = self , runtime_configuration = runtime_configuration , ) for component in self . components . values (): component . prepare_pipeline_run () run_name = runtime_configuration . run_name or ( f \" { pipeline . name } -\" f ' { datetime . now () . strftime ( \" %d _%h_%y-%H_%M_%S_ %f \" ) } ' ) logger . info ( \"Using stack ` %s ` to run pipeline ` %s `...\" , self . name , pipeline . name , ) start_time = time . time () return_value = self . orchestrator . run_pipeline ( pipeline , stack = self , run_name = run_name ) run_duration = time . time () - start_time logger . info ( \"Pipeline run ` %s ` has finished in %s .\" , run_name , string_utils . get_human_readable_time ( run_duration ), ) for component in self . components . values (): component . cleanup_pipeline_run () return return_value # TODO [ENG-373]: Include provisioning status in CLI `zenml stack describe` # and `zenml stack-component describe` commands @property def is_provisioned ( self ) -> bool : \"\"\"If the stack provisioned resources to run locally.\"\"\" return all ( component . is_provisioned for component in self . components . values () ) @property def is_running ( self ) -> bool : \"\"\"If the stack is running locally.\"\"\" return all ( component . is_running for component in self . components . values () ) def provision ( self ) -> None : \"\"\"Provisions resources to run the stack locally. Raises: NotImplementedError: If any unprovisioned component does not implement provisioning. \"\"\" logger . info ( \"Provisioning resources for stack ' %s '.\" , self . name ) for component in self . components . values (): if not component . is_provisioned : component . provision () logger . info ( \"Provisioned resources for %s .\" , component ) def deprovision ( self ) -> None : \"\"\"Deprovisions all local resources of the stack. Raises: NotImplementedError: If any provisioned component does not implement deprovisioning. \"\"\" logger . info ( \"Deprovisioning resources for stack ' %s '.\" , self . name ) for component in self . components . values (): if component . is_provisioned : try : component . deprovision () logger . info ( \"Deprovisioned resources for %s .\" , component ) except NotImplementedError as e : logger . warning ( e ) def resume ( self ) -> None : \"\"\"Resumes the provisioned local resources of the stack. Raises: ProvisioningError: If any stack component is missing provisioned resources. \"\"\" logger . info ( \"Resuming provisioned resources for stack ' %s '.\" , self . name ) for component in self . components . values (): if component . is_running : # the component is already running, no need to resume anything pass elif component . is_provisioned : component . resume () logger . info ( \"Resumed resources for %s .\" , component ) else : raise ProvisioningError ( f \"Unable to resume resources for { component } : No \" f \"resources have been provisioned for this component.\" ) def suspend ( self ) -> None : \"\"\"Suspends the provisioned local resources of the stack.\"\"\" logger . info ( \"Suspending provisioned resources for stack ' %s '.\" , self . name ) for component in self . components . values (): if component . is_running : try : component . suspend () logger . info ( \"Suspended resources for %s .\" , component ) except NotImplementedError : logger . warning ( \"Suspending provisioned resources not implemented \" \"for %s . Continuing without suspending resources...\" , component , )","title":"Stack"},{"location":"api_docs/stack/#zenml.stack.stack.Stack.artifact_store","text":"The artifact store of the stack.","title":"artifact_store"},{"location":"api_docs/stack/#zenml.stack.stack.Stack.components","text":"All components of the stack.","title":"components"},{"location":"api_docs/stack/#zenml.stack.stack.Stack.container_registry","text":"The container registry of the stack.","title":"container_registry"},{"location":"api_docs/stack/#zenml.stack.stack.Stack.is_provisioned","text":"If the stack provisioned resources to run locally.","title":"is_provisioned"},{"location":"api_docs/stack/#zenml.stack.stack.Stack.is_running","text":"If the stack is running locally.","title":"is_running"},{"location":"api_docs/stack/#zenml.stack.stack.Stack.metadata_store","text":"The metadata store of the stack.","title":"metadata_store"},{"location":"api_docs/stack/#zenml.stack.stack.Stack.name","text":"The name of the stack.","title":"name"},{"location":"api_docs/stack/#zenml.stack.stack.Stack.orchestrator","text":"The orchestrator of the stack.","title":"orchestrator"},{"location":"api_docs/stack/#zenml.stack.stack.Stack.runtime_options","text":"Runtime options that are available to configure this stack. This method combines the available runtime options for all components of this stack. See StackComponent.runtime_options() for more information.","title":"runtime_options"},{"location":"api_docs/stack/#zenml.stack.stack.Stack.__init__","text":"Initializes and validates a stack instance. Exceptions: Type Description StackValidationError If the stack configuration is not valid. Source code in zenml/stack/stack.py def __init__ ( self , name : str , * , orchestrator : \"BaseOrchestrator\" , metadata_store : \"BaseMetadataStore\" , artifact_store : \"BaseArtifactStore\" , container_registry : Optional [ \"BaseContainerRegistry\" ] = None , ): \"\"\"Initializes and validates a stack instance. Raises: StackValidationError: If the stack configuration is not valid. \"\"\" self . _name = name self . _orchestrator = orchestrator self . _metadata_store = metadata_store self . _artifact_store = artifact_store self . _container_registry = container_registry self . validate ()","title":"__init__()"},{"location":"api_docs/stack/#zenml.stack.stack.Stack.default_local_stack","text":"Creates a stack instance which is configured to run locally. Source code in zenml/stack/stack.py @classmethod def default_local_stack ( cls ) -> \"Stack\" : \"\"\"Creates a stack instance which is configured to run locally.\"\"\" from zenml.artifact_stores import LocalArtifactStore from zenml.metadata_stores import SQLiteMetadataStore from zenml.orchestrators import LocalOrchestrator orchestrator = LocalOrchestrator ( name = \"local_orchestrator\" ) artifact_store_uuid = uuid . uuid4 () artifact_store_path = os . path . join ( GlobalConfig . config_directory (), \"local_stores\" , str ( artifact_store_uuid ), ) artifact_store = LocalArtifactStore ( name = \"local_artifact_store\" , uuid = artifact_store_uuid , path = artifact_store_path , ) metadata_store_path = os . path . join ( artifact_store_path , \"metadata.db\" ) metadata_store = SQLiteMetadataStore ( name = \"local_metadata_store\" , uri = metadata_store_path ) return cls ( name = \"local_stack\" , orchestrator = orchestrator , metadata_store = metadata_store , artifact_store = artifact_store , )","title":"default_local_stack()"},{"location":"api_docs/stack/#zenml.stack.stack.Stack.deploy_pipeline","text":"Deploys a pipeline on this stack. Parameters: Name Type Description Default pipeline BasePipeline The pipeline to deploy. required runtime_configuration RuntimeConfiguration Contains all the runtime configuration options specified for the pipeline run. required Returns: Type Description Any The return value of the call to orchestrator.run_pipeline(...) . Source code in zenml/stack/stack.py def deploy_pipeline ( self , pipeline : \"BasePipeline\" , runtime_configuration : RuntimeConfiguration , ) -> Any : \"\"\"Deploys a pipeline on this stack. Args: pipeline: The pipeline to deploy. runtime_configuration: Contains all the runtime configuration options specified for the pipeline run. Returns: The return value of the call to `orchestrator.run_pipeline(...)`. \"\"\" for component in self . components . values (): component . prepare_pipeline_deployment ( pipeline = pipeline , stack = self , runtime_configuration = runtime_configuration , ) for component in self . components . values (): component . prepare_pipeline_run () run_name = runtime_configuration . run_name or ( f \" { pipeline . name } -\" f ' { datetime . now () . strftime ( \" %d _%h_%y-%H_%M_%S_ %f \" ) } ' ) logger . info ( \"Using stack ` %s ` to run pipeline ` %s `...\" , self . name , pipeline . name , ) start_time = time . time () return_value = self . orchestrator . run_pipeline ( pipeline , stack = self , run_name = run_name ) run_duration = time . time () - start_time logger . info ( \"Pipeline run ` %s ` has finished in %s .\" , run_name , string_utils . get_human_readable_time ( run_duration ), ) for component in self . components . values (): component . cleanup_pipeline_run () return return_value","title":"deploy_pipeline()"},{"location":"api_docs/stack/#zenml.stack.stack.Stack.deprovision","text":"Deprovisions all local resources of the stack. Exceptions: Type Description NotImplementedError If any provisioned component does not implement deprovisioning. Source code in zenml/stack/stack.py def deprovision ( self ) -> None : \"\"\"Deprovisions all local resources of the stack. Raises: NotImplementedError: If any provisioned component does not implement deprovisioning. \"\"\" logger . info ( \"Deprovisioning resources for stack ' %s '.\" , self . name ) for component in self . components . values (): if component . is_provisioned : try : component . deprovision () logger . info ( \"Deprovisioned resources for %s .\" , component ) except NotImplementedError as e : logger . warning ( e )","title":"deprovision()"},{"location":"api_docs/stack/#zenml.stack.stack.Stack.from_components","text":"Creates a stack instance from a dict of stack components. Parameters: Name Type Description Default name str The name of the stack. required components Dict[zenml.enums.StackComponentType, StackComponent] The components of the stack. required Returns: Type Description Stack A stack instance consisting of the given components. Exceptions: Type Description TypeError If a required component is missing or a component doesn't inherit from the expected base class. Source code in zenml/stack/stack.py @classmethod def from_components ( cls , name : str , components : Dict [ StackComponentType , \"StackComponent\" ] ) -> \"Stack\" : \"\"\"Creates a stack instance from a dict of stack components. Args: name: The name of the stack. components: The components of the stack. Returns: A stack instance consisting of the given components. Raises: TypeError: If a required component is missing or a component doesn't inherit from the expected base class. \"\"\" from zenml.artifact_stores import BaseArtifactStore from zenml.container_registries import BaseContainerRegistry from zenml.metadata_stores import BaseMetadataStore from zenml.orchestrators import BaseOrchestrator def _raise_type_error ( component : Optional [ \"StackComponent\" ], expected_class : Type [ Any ] ) -> NoReturn : \"\"\"Raises a TypeError that the component has an unexpected type.\"\"\" raise TypeError ( f \"Unable to create stack: Wrong stack component type \" f \"` { component . __class__ . __name__ } ` (expected: subclass \" f \"of ` { expected_class . __name__ } `)\" ) orchestrator = components . get ( StackComponentType . ORCHESTRATOR ) if not isinstance ( orchestrator , BaseOrchestrator ): _raise_type_error ( orchestrator , BaseOrchestrator ) metadata_store = components . get ( StackComponentType . METADATA_STORE ) if not isinstance ( metadata_store , BaseMetadataStore ): _raise_type_error ( metadata_store , BaseMetadataStore ) artifact_store = components . get ( StackComponentType . ARTIFACT_STORE ) if not isinstance ( artifact_store , BaseArtifactStore ): _raise_type_error ( artifact_store , BaseArtifactStore ) container_registry = components . get ( StackComponentType . CONTAINER_REGISTRY ) if container_registry is not None and not isinstance ( container_registry , BaseContainerRegistry ): _raise_type_error ( container_registry , BaseContainerRegistry ) return Stack ( name = name , orchestrator = orchestrator , metadata_store = metadata_store , artifact_store = artifact_store , container_registry = container_registry , )","title":"from_components()"},{"location":"api_docs/stack/#zenml.stack.stack.Stack.provision","text":"Provisions resources to run the stack locally. Exceptions: Type Description NotImplementedError If any unprovisioned component does not implement provisioning. Source code in zenml/stack/stack.py def provision ( self ) -> None : \"\"\"Provisions resources to run the stack locally. Raises: NotImplementedError: If any unprovisioned component does not implement provisioning. \"\"\" logger . info ( \"Provisioning resources for stack ' %s '.\" , self . name ) for component in self . components . values (): if not component . is_provisioned : component . provision () logger . info ( \"Provisioned resources for %s .\" , component )","title":"provision()"},{"location":"api_docs/stack/#zenml.stack.stack.Stack.requirements","text":"Set of PyPI requirements for the stack. This method combines the requirements of all stack components (except the ones specified in exclude_components ). Parameters: Name Type Description Default exclude_components Optional[AbstractSet[zenml.enums.StackComponentType]] Set of component types for which the requirements should not be included in the output. None Source code in zenml/stack/stack.py def requirements ( self , exclude_components : Optional [ AbstractSet [ StackComponentType ]] = None , ) -> Set [ str ]: \"\"\"Set of PyPI requirements for the stack. This method combines the requirements of all stack components (except the ones specified in `exclude_components`). Args: exclude_components: Set of component types for which the requirements should not be included in the output. \"\"\" exclude_components = exclude_components or set () requirements = [ component . requirements for component in self . components . values () if component . type not in exclude_components ] return set . union ( * requirements ) if requirements else set ()","title":"requirements()"},{"location":"api_docs/stack/#zenml.stack.stack.Stack.resume","text":"Resumes the provisioned local resources of the stack. Exceptions: Type Description ProvisioningError If any stack component is missing provisioned resources. Source code in zenml/stack/stack.py def resume ( self ) -> None : \"\"\"Resumes the provisioned local resources of the stack. Raises: ProvisioningError: If any stack component is missing provisioned resources. \"\"\" logger . info ( \"Resuming provisioned resources for stack ' %s '.\" , self . name ) for component in self . components . values (): if component . is_running : # the component is already running, no need to resume anything pass elif component . is_provisioned : component . resume () logger . info ( \"Resumed resources for %s .\" , component ) else : raise ProvisioningError ( f \"Unable to resume resources for { component } : No \" f \"resources have been provisioned for this component.\" )","title":"resume()"},{"location":"api_docs/stack/#zenml.stack.stack.Stack.suspend","text":"Suspends the provisioned local resources of the stack. Source code in zenml/stack/stack.py def suspend ( self ) -> None : \"\"\"Suspends the provisioned local resources of the stack.\"\"\" logger . info ( \"Suspending provisioned resources for stack ' %s '.\" , self . name ) for component in self . components . values (): if component . is_running : try : component . suspend () logger . info ( \"Suspended resources for %s .\" , component ) except NotImplementedError : logger . warning ( \"Suspending provisioned resources not implemented \" \"for %s . Continuing without suspending resources...\" , component , )","title":"suspend()"},{"location":"api_docs/stack/#zenml.stack.stack.Stack.validate","text":"Checks whether the stack configuration is valid. To check if a stack configuration is valid, the following criteria must be met: - all components must support the execution mode (either local or remote execution) specified by the orchestrator of the stack - the StackValidator of each stack component has to validate the stack to make sure all the components are compatible with each other Exceptions: Type Description StackValidationError If the stack configuration is not valid. Source code in zenml/stack/stack.py def validate ( self ) -> None : \"\"\"Checks whether the stack configuration is valid. To check if a stack configuration is valid, the following criteria must be met: - all components must support the execution mode (either local or remote execution) specified by the orchestrator of the stack - the `StackValidator` of each stack component has to validate the stack to make sure all the components are compatible with each other Raises: StackValidationError: If the stack configuration is not valid. \"\"\" # TODO [ENG-372]: Differentiate between orchestrators running a pipeline # locally and remotely (potentially using subclasses or an # `orchestrator.mode` property?) and make sure all components support # either local/remote execution for component in self . components . values (): if component . validator : component . validator . validate ( stack = self )","title":"validate()"},{"location":"api_docs/stack/#zenml.stack.stack_component","text":"","title":"stack_component"},{"location":"api_docs/stack/#zenml.stack.stack_component.StackComponent","text":"Abstract StackComponent class for all components of a ZenML stack. Attributes: Name Type Description name str The name of the component. uuid UUID Unique identifier of the component. supports_local_execution bool If the component supports running locally. supports_remote_execution bool If the component supports running remotely. Source code in zenml/stack/stack_component.py class StackComponent ( BaseModel , ABC ): \"\"\"Abstract StackComponent class for all components of a ZenML stack. Attributes: name: The name of the component. uuid: Unique identifier of the component. supports_local_execution: If the component supports running locally. supports_remote_execution: If the component supports running remotely. \"\"\" name : str uuid : UUID = Field ( default_factory = uuid4 ) supports_local_execution : bool supports_remote_execution : bool @property @abstractmethod def type ( self ) -> StackComponentType : \"\"\"The component type.\"\"\" @property @abstractmethod def flavor ( self ) -> StackComponentFlavor : \"\"\"The component flavor.\"\"\" @property def log_file ( self ) -> Optional [ str ]: \"\"\"Optional path to a log file for the stack component.\"\"\" # TODO [ENG-136]: Add support for multiple log files for a stack # component. E.g. let each component return a generator that yields # logs instead of specifying a single file path. return None @property def runtime_options ( self ) -> Dict [ str , Any ]: \"\"\"Runtime options that are available to configure this component. The items of the dictionary should map option names (which can be used to configure the option in the `RuntimeConfiguration`) to default values for the option (or `None` if there is no default value). \"\"\" return {} @property def requirements ( self ) -> Set [ str ]: \"\"\"Set of PyPI requirements for the component.\"\"\" return set ( get_requirements_for_module ( self . __module__ )) def prepare_pipeline_deployment ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , runtime_configuration : \"RuntimeConfiguration\" , ) -> None : \"\"\"Prepares deploying the pipeline. This method gets called immediately before a pipeline is deployed. Subclasses should override it if they require runtime configuration options or if they need to run code before the pipeline deployment. Args: pipeline: The pipeline that will be deployed. stack: The stack on which the pipeline will be deployed. runtime_configuration: Contains all the runtime configuration options specified for the pipeline run. \"\"\" def prepare_pipeline_run ( self ) -> None : \"\"\"Prepares running the pipeline.\"\"\" def cleanup_pipeline_run ( self ) -> None : \"\"\"Cleans up resources after the pipeline run is finished.\"\"\" @property def validator ( self ) -> Optional [ \"StackValidator\" ]: \"\"\"The optional validator of the stack component. This validator will be called each time a stack with the stack component is initialized. Subclasses should override this property and return a `StackValidator` that makes sure they're not included in any stack that they're not compatible with. \"\"\" return None @property def is_provisioned ( self ) -> bool : \"\"\"If the component provisioned resources to run locally.\"\"\" return True @property def is_running ( self ) -> bool : \"\"\"If the component is running locally.\"\"\" return True def provision ( self ) -> None : \"\"\"Provisions resources to run the component locally.\"\"\" raise NotImplementedError ( f \"Provisioning local resources not implemented for { self } .\" ) def deprovision ( self ) -> None : \"\"\"Deprovisions all local resources of the component.\"\"\" raise NotImplementedError ( f \"Deprovisioning local resource not implemented for { self } .\" ) def resume ( self ) -> None : \"\"\"Resumes the provisioned local resources of the component.\"\"\" raise NotImplementedError ( f \"Resuming provisioned resources not implemented for { self } .\" ) def suspend ( self ) -> None : \"\"\"Suspends the provisioned local resources of the component.\"\"\" raise NotImplementedError ( f \"Suspending provisioned resources not implemented for { self } .\" ) def __repr__ ( self ) -> str : \"\"\"String representation of the stack component.\"\"\" attribute_representation = \", \" . join ( f \" { key } = { value } \" for key , value in self . dict () . items () ) return ( f \" { self . __class__ . __qualname__ } (type= { self . type } , \" f \"flavor= { self . flavor } , { attribute_representation } )\" ) def __str__ ( self ) -> str : \"\"\"String representation of the stack component.\"\"\" return self . __repr__ () class Config : \"\"\"Pydantic configuration class.\"\"\" # public attributes are immutable allow_mutation = False # all attributes with leading underscore are private and therefore # are mutable and not included in serialization underscore_attrs_are_private = True # exclude these two fields from being serialized fields = { \"supports_local_execution\" : { \"exclude\" : True }, \"supports_remote_execution\" : { \"exclude\" : True }, }","title":"StackComponent"},{"location":"api_docs/stack/#zenml.stack.stack_component.StackComponent.flavor","text":"The component flavor.","title":"flavor"},{"location":"api_docs/stack/#zenml.stack.stack_component.StackComponent.is_provisioned","text":"If the component provisioned resources to run locally.","title":"is_provisioned"},{"location":"api_docs/stack/#zenml.stack.stack_component.StackComponent.is_running","text":"If the component is running locally.","title":"is_running"},{"location":"api_docs/stack/#zenml.stack.stack_component.StackComponent.log_file","text":"Optional path to a log file for the stack component.","title":"log_file"},{"location":"api_docs/stack/#zenml.stack.stack_component.StackComponent.requirements","text":"Set of PyPI requirements for the component.","title":"requirements"},{"location":"api_docs/stack/#zenml.stack.stack_component.StackComponent.runtime_options","text":"Runtime options that are available to configure this component. The items of the dictionary should map option names (which can be used to configure the option in the RuntimeConfiguration ) to default values for the option (or None if there is no default value).","title":"runtime_options"},{"location":"api_docs/stack/#zenml.stack.stack_component.StackComponent.type","text":"The component type.","title":"type"},{"location":"api_docs/stack/#zenml.stack.stack_component.StackComponent.validator","text":"The optional validator of the stack component. This validator will be called each time a stack with the stack component is initialized. Subclasses should override this property and return a StackValidator that makes sure they're not included in any stack that they're not compatible with.","title":"validator"},{"location":"api_docs/stack/#zenml.stack.stack_component.StackComponent.Config","text":"Pydantic configuration class. Source code in zenml/stack/stack_component.py class Config : \"\"\"Pydantic configuration class.\"\"\" # public attributes are immutable allow_mutation = False # all attributes with leading underscore are private and therefore # are mutable and not included in serialization underscore_attrs_are_private = True # exclude these two fields from being serialized fields = { \"supports_local_execution\" : { \"exclude\" : True }, \"supports_remote_execution\" : { \"exclude\" : True }, }","title":"Config"},{"location":"api_docs/stack/#zenml.stack.stack_component.StackComponent.__repr__","text":"String representation of the stack component. Source code in zenml/stack/stack_component.py def __repr__ ( self ) -> str : \"\"\"String representation of the stack component.\"\"\" attribute_representation = \", \" . join ( f \" { key } = { value } \" for key , value in self . dict () . items () ) return ( f \" { self . __class__ . __qualname__ } (type= { self . type } , \" f \"flavor= { self . flavor } , { attribute_representation } )\" )","title":"__repr__()"},{"location":"api_docs/stack/#zenml.stack.stack_component.StackComponent.__str__","text":"String representation of the stack component. Source code in zenml/stack/stack_component.py def __str__ ( self ) -> str : \"\"\"String representation of the stack component.\"\"\" return self . __repr__ ()","title":"__str__()"},{"location":"api_docs/stack/#zenml.stack.stack_component.StackComponent.cleanup_pipeline_run","text":"Cleans up resources after the pipeline run is finished. Source code in zenml/stack/stack_component.py def cleanup_pipeline_run ( self ) -> None : \"\"\"Cleans up resources after the pipeline run is finished.\"\"\"","title":"cleanup_pipeline_run()"},{"location":"api_docs/stack/#zenml.stack.stack_component.StackComponent.deprovision","text":"Deprovisions all local resources of the component. Source code in zenml/stack/stack_component.py def deprovision ( self ) -> None : \"\"\"Deprovisions all local resources of the component.\"\"\" raise NotImplementedError ( f \"Deprovisioning local resource not implemented for { self } .\" )","title":"deprovision()"},{"location":"api_docs/stack/#zenml.stack.stack_component.StackComponent.prepare_pipeline_deployment","text":"Prepares deploying the pipeline. This method gets called immediately before a pipeline is deployed. Subclasses should override it if they require runtime configuration options or if they need to run code before the pipeline deployment. Parameters: Name Type Description Default pipeline BasePipeline The pipeline that will be deployed. required stack Stack The stack on which the pipeline will be deployed. required runtime_configuration RuntimeConfiguration Contains all the runtime configuration options specified for the pipeline run. required Source code in zenml/stack/stack_component.py def prepare_pipeline_deployment ( self , pipeline : \"BasePipeline\" , stack : \"Stack\" , runtime_configuration : \"RuntimeConfiguration\" , ) -> None : \"\"\"Prepares deploying the pipeline. This method gets called immediately before a pipeline is deployed. Subclasses should override it if they require runtime configuration options or if they need to run code before the pipeline deployment. Args: pipeline: The pipeline that will be deployed. stack: The stack on which the pipeline will be deployed. runtime_configuration: Contains all the runtime configuration options specified for the pipeline run. \"\"\"","title":"prepare_pipeline_deployment()"},{"location":"api_docs/stack/#zenml.stack.stack_component.StackComponent.prepare_pipeline_run","text":"Prepares running the pipeline. Source code in zenml/stack/stack_component.py def prepare_pipeline_run ( self ) -> None : \"\"\"Prepares running the pipeline.\"\"\"","title":"prepare_pipeline_run()"},{"location":"api_docs/stack/#zenml.stack.stack_component.StackComponent.provision","text":"Provisions resources to run the component locally. Source code in zenml/stack/stack_component.py def provision ( self ) -> None : \"\"\"Provisions resources to run the component locally.\"\"\" raise NotImplementedError ( f \"Provisioning local resources not implemented for { self } .\" )","title":"provision()"},{"location":"api_docs/stack/#zenml.stack.stack_component.StackComponent.resume","text":"Resumes the provisioned local resources of the component. Source code in zenml/stack/stack_component.py def resume ( self ) -> None : \"\"\"Resumes the provisioned local resources of the component.\"\"\" raise NotImplementedError ( f \"Resuming provisioned resources not implemented for { self } .\" )","title":"resume()"},{"location":"api_docs/stack/#zenml.stack.stack_component.StackComponent.suspend","text":"Suspends the provisioned local resources of the component. Source code in zenml/stack/stack_component.py def suspend ( self ) -> None : \"\"\"Suspends the provisioned local resources of the component.\"\"\" raise NotImplementedError ( f \"Suspending provisioned resources not implemented for { self } .\" )","title":"suspend()"},{"location":"api_docs/stack/#zenml.stack.stack_component_class_registry","text":"","title":"stack_component_class_registry"},{"location":"api_docs/stack/#zenml.stack.stack_component_class_registry.StackComponentClassRegistry","text":"Registry for stack component classes. All stack component classes must be registered here so they can be instantiated from the component type and flavor specified inside the ZenML repository configuration. Source code in zenml/stack/stack_component_class_registry.py class StackComponentClassRegistry : \"\"\"Registry for stack component classes. All stack component classes must be registered here so they can be instantiated from the component type and flavor specified inside the ZenML repository configuration. \"\"\" component_classes : ClassVar [ DefaultDict [ StackComponentType , Dict [ str , Type [ StackComponent ]]] ] = defaultdict ( dict ) @classmethod def register_class ( cls , component_type : StackComponentType , component_flavor : StackComponentFlavor , component_class : Type [ StackComponent ], ) -> None : \"\"\"Registers a stack component class. Args: component_type: The type of the component class to register. component_flavor: The flavor of the component class to register. component_class: The component class to register. \"\"\" component_flavor = component_flavor . value flavors = cls . component_classes [ component_type ] if component_flavor in flavors : logger . warning ( \"Overwriting previously registered stack component class ` %s ` \" \"for type ' %s ' and flavor ' %s '.\" , flavors [ component_flavor ] . __class__ . __name__ , component_type . value , component_flavor , ) flavors [ component_flavor ] = component_class logger . debug ( \"Registered stack component class for type ' %s ' and flavor ' %s '.\" , component_type . value , component_flavor , ) @classmethod def get_class ( cls , component_type : StackComponentType , component_flavor : Union [ StackComponentFlavor , str ], ) -> Type [ StackComponent ]: \"\"\"Returns the stack component class for the given type and flavor. Args: component_type: The type of the component class to return. component_flavor: The flavor of the component class to return. Raises: KeyError: If no component class is registered for the given type and flavor. \"\"\" # TODO [ENG-374]: Think about activating the integrations here to make # sure all potential StackComponent classes are registered if isinstance ( component_flavor , StackComponentFlavor ): component_flavor = component_flavor . value available_flavors = cls . component_classes [ component_type ] try : return available_flavors [ component_flavor ] except KeyError : raise KeyError ( f \"No stack component class found for type { component_type } \" f \"and flavor { component_flavor } . Registered flavors for this \" f \"type: { set ( available_flavors ) } . If your stack component \" f \"class is part of a ZenML integration, make sure to active \" f \"them by calling \" f \"`IntegrationRegistry.activate_integrations()`.\" ) from None","title":"StackComponentClassRegistry"},{"location":"api_docs/stack/#zenml.stack.stack_component_class_registry.StackComponentClassRegistry.get_class","text":"Returns the stack component class for the given type and flavor. Parameters: Name Type Description Default component_type StackComponentType The type of the component class to return. required component_flavor Union[zenml.enums.StackComponentFlavor, str] The flavor of the component class to return. required Exceptions: Type Description KeyError If no component class is registered for the given type and flavor. Source code in zenml/stack/stack_component_class_registry.py @classmethod def get_class ( cls , component_type : StackComponentType , component_flavor : Union [ StackComponentFlavor , str ], ) -> Type [ StackComponent ]: \"\"\"Returns the stack component class for the given type and flavor. Args: component_type: The type of the component class to return. component_flavor: The flavor of the component class to return. Raises: KeyError: If no component class is registered for the given type and flavor. \"\"\" # TODO [ENG-374]: Think about activating the integrations here to make # sure all potential StackComponent classes are registered if isinstance ( component_flavor , StackComponentFlavor ): component_flavor = component_flavor . value available_flavors = cls . component_classes [ component_type ] try : return available_flavors [ component_flavor ] except KeyError : raise KeyError ( f \"No stack component class found for type { component_type } \" f \"and flavor { component_flavor } . Registered flavors for this \" f \"type: { set ( available_flavors ) } . If your stack component \" f \"class is part of a ZenML integration, make sure to active \" f \"them by calling \" f \"`IntegrationRegistry.activate_integrations()`.\" ) from None","title":"get_class()"},{"location":"api_docs/stack/#zenml.stack.stack_component_class_registry.StackComponentClassRegistry.register_class","text":"Registers a stack component class. Parameters: Name Type Description Default component_type StackComponentType The type of the component class to register. required component_flavor StackComponentFlavor The flavor of the component class to register. required component_class Type[zenml.stack.stack_component.StackComponent] The component class to register. required Source code in zenml/stack/stack_component_class_registry.py @classmethod def register_class ( cls , component_type : StackComponentType , component_flavor : StackComponentFlavor , component_class : Type [ StackComponent ], ) -> None : \"\"\"Registers a stack component class. Args: component_type: The type of the component class to register. component_flavor: The flavor of the component class to register. component_class: The component class to register. \"\"\" component_flavor = component_flavor . value flavors = cls . component_classes [ component_type ] if component_flavor in flavors : logger . warning ( \"Overwriting previously registered stack component class ` %s ` \" \"for type ' %s ' and flavor ' %s '.\" , flavors [ component_flavor ] . __class__ . __name__ , component_type . value , component_flavor , ) flavors [ component_flavor ] = component_class logger . debug ( \"Registered stack component class for type ' %s ' and flavor ' %s '.\" , component_type . value , component_flavor , )","title":"register_class()"},{"location":"api_docs/stack/#zenml.stack.stack_component_class_registry.register_stack_component_class","text":"Parametrized decorator function to register stack component classes. Parameters: Name Type Description Default component_type StackComponentType The type of the component class to register. required component_flavor StackComponentFlavor The flavor of the component class to register. required Returns: Type Description Callable[[Type[~C]], Type[~C]] A decorator function that registers and returns the decorated stack component class. Source code in zenml/stack/stack_component_class_registry.py def register_stack_component_class ( component_type : StackComponentType , component_flavor : StackComponentFlavor ) -> Callable [[ Type [ C ]], Type [ C ]]: \"\"\"Parametrized decorator function to register stack component classes. Args: component_type: The type of the component class to register. component_flavor: The flavor of the component class to register. Returns: A decorator function that registers and returns the decorated stack component class. \"\"\" def decorator_function ( cls : Type [ C ]) -> Type [ C ]: \"\"\"Registers the stack component class and returns it unmodified.\"\"\" StackComponentClassRegistry . register_class ( component_type = component_type , component_flavor = component_flavor , component_class = cls , ) return cls return decorator_function","title":"register_stack_component_class()"},{"location":"api_docs/stack/#zenml.stack.stack_validator","text":"","title":"stack_validator"},{"location":"api_docs/stack/#zenml.stack.stack_validator.StackValidator","text":"A StackValidator is used to validate a stack configuration. Each StackComponent can provide a StackValidator to make sure it is compatible with all components of the stack. The KubeflowOrchestrator for example will always require the stack to have a container registry in order to push the docker images that are required to run a pipeline in Kubeflow Pipelines. Source code in zenml/stack/stack_validator.py class StackValidator : \"\"\"A `StackValidator` is used to validate a stack configuration. Each `StackComponent` can provide a `StackValidator` to make sure it is compatible with all components of the stack. The `KubeflowOrchestrator` for example will always require the stack to have a container registry in order to push the docker images that are required to run a pipeline in Kubeflow Pipelines. \"\"\" def __init__ ( self , required_components : Optional [ AbstractSet [ StackComponentType ]] = None , custom_validation_function : Optional [ Callable [[ \"Stack\" ], bool ]] = None , ): \"\"\"Initializes a `StackValidator` instance. Args: required_components: Optional set of stack components that must exist in the stack. custom_validation_function: Optional function that returns whether a stack is valid. \"\"\" self . _required_components = required_components or set () self . _custom_validation_function = custom_validation_function def validate ( self , stack : \"Stack\" ) -> None : \"\"\"Validates the given stack. Checks if the stack contains all the required components and passes the custom validation function of the validator. Raises: StackValidationError: If the stack does not meet all the validation criteria. \"\"\" missing_components = self . _required_components - set ( stack . components ) if missing_components : raise StackValidationError ( f \"Missing stack components { missing_components } for \" f \"stack: { stack } \" ) if ( self . _custom_validation_function and not self . _custom_validation_function ( stack ) ): raise StackValidationError ( f \"Custom validation function failed to validate \" f \"stack: { stack } \" )","title":"StackValidator"},{"location":"api_docs/stack/#zenml.stack.stack_validator.StackValidator.__init__","text":"Initializes a StackValidator instance. Parameters: Name Type Description Default required_components Optional[AbstractSet[zenml.enums.StackComponentType]] Optional set of stack components that must exist in the stack. None custom_validation_function Optional[Callable[[Stack], bool]] Optional function that returns whether a stack is valid. None Source code in zenml/stack/stack_validator.py def __init__ ( self , required_components : Optional [ AbstractSet [ StackComponentType ]] = None , custom_validation_function : Optional [ Callable [[ \"Stack\" ], bool ]] = None , ): \"\"\"Initializes a `StackValidator` instance. Args: required_components: Optional set of stack components that must exist in the stack. custom_validation_function: Optional function that returns whether a stack is valid. \"\"\" self . _required_components = required_components or set () self . _custom_validation_function = custom_validation_function","title":"__init__()"},{"location":"api_docs/stack/#zenml.stack.stack_validator.StackValidator.validate","text":"Validates the given stack. Checks if the stack contains all the required components and passes the custom validation function of the validator. Exceptions: Type Description StackValidationError If the stack does not meet all the validation criteria. Source code in zenml/stack/stack_validator.py def validate ( self , stack : \"Stack\" ) -> None : \"\"\"Validates the given stack. Checks if the stack contains all the required components and passes the custom validation function of the validator. Raises: StackValidationError: If the stack does not meet all the validation criteria. \"\"\" missing_components = self . _required_components - set ( stack . components ) if missing_components : raise StackValidationError ( f \"Missing stack components { missing_components } for \" f \"stack: { stack } \" ) if ( self . _custom_validation_function and not self . _custom_validation_function ( stack ) ): raise StackValidationError ( f \"Custom validation function failed to validate \" f \"stack: { stack } \" )","title":"validate()"},{"location":"api_docs/steps/","text":"Steps zenml.steps special A step is a single piece or stage of a ZenML pipeline. Think of each step as being one of the nodes of a Directed Acyclic Graph (or DAG). Steps are responsible for one aspect of processing or interacting with the data / artifacts in the pipeline. ZenML currently implements a basic step interface, but there will be other more customized interfaces (layered in a hierarchy) for specialized implementations. Conceptually, a Step is a discrete and independent part of a pipeline that is responsible for one particular aspect of data manipulation inside a ZenML pipeline. Steps can be subclassed from the BaseStep class, or used via our @step decorator. base_step BaseStep Abstract base class for all ZenML steps. Attributes: Name Type Description name The name of this step. pipeline_parameter_name Optional[str] The name of the pipeline parameter for which this step was passed as an argument. enable_cache A boolean indicating if caching is enabled for this step. requires_context A boolean indicating if this step requires a StepContext object during execution. Source code in zenml/steps/base_step.py class BaseStep ( metaclass = BaseStepMeta ): \"\"\"Abstract base class for all ZenML steps. Attributes: name: The name of this step. pipeline_parameter_name: The name of the pipeline parameter for which this step was passed as an argument. enable_cache: A boolean indicating if caching is enabled for this step. requires_context: A boolean indicating if this step requires a `StepContext` object during execution. \"\"\" # TODO [ENG-156]: Ensure these are ordered INPUT_SIGNATURE : ClassVar [ Dict [ str , Type [ Any ]]] = None # type: ignore[assignment] # noqa OUTPUT_SIGNATURE : ClassVar [ Dict [ str , Type [ Any ]]] = None # type: ignore[assignment] # noqa CONFIG_PARAMETER_NAME : ClassVar [ Optional [ str ]] = None CONFIG_CLASS : ClassVar [ Optional [ Type [ BaseStepConfig ]]] = None CONTEXT_PARAMETER_NAME : ClassVar [ Optional [ str ]] = None PARAM_SPEC : Dict [ str , Any ] = {} INPUT_SPEC : Dict [ str , Type [ BaseArtifact ]] = {} OUTPUT_SPEC : Dict [ str , Type [ BaseArtifact ]] = {} INSTANCE_CONFIGURATION : Dict [ str , Any ] = {} def __init__ ( self , * args : Any , ** kwargs : Any ) -> None : self . name = self . __class__ . __name__ self . pipeline_parameter_name : Optional [ str ] = None kwargs . update ( getattr ( self , INSTANCE_CONFIGURATION )) self . requires_context = bool ( self . CONTEXT_PARAMETER_NAME ) self . _created_by_functional_api = kwargs . pop ( PARAM_CREATED_BY_FUNCTIONAL_API , False ) enable_cache = kwargs . pop ( PARAM_ENABLE_CACHE , None ) if enable_cache is None : if self . requires_context : # Using the StepContext inside a step provides access to # external resources which might influence the step execution. # We therefore disable caching unless it is explicitly enabled enable_cache = False logger . debug ( \"Step ' %s ': Step context required and caching not \" \"explicitly enabled.\" , self . name , ) else : # Default to cache enabled if not explicitly set enable_cache = True logger . debug ( \"Step ' %s ': Caching %s .\" , self . name , \"enabled\" if enable_cache else \"disabled\" , ) self . enable_cache = enable_cache self . _explicit_materializers : Dict [ str , Type [ BaseMaterializer ]] = {} self . _component : Optional [ _ZenMLSimpleComponent ] = None self . _verify_init_arguments ( * args , ** kwargs ) self . _verify_output_spec () @abstractmethod def entrypoint ( self , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Abstract method for core step logic.\"\"\" def get_materializers ( self , ensure_complete : bool = False ) -> Dict [ str , Type [ BaseMaterializer ]]: \"\"\"Returns available materializers for the outputs of this step. Args: ensure_complete: If set to `True`, this method will raise a `StepInterfaceError` if no materializer can be found for an output. Returns: A dictionary mapping output names to `BaseMaterializer` subclasses. If no explicit materializer was set using `step.with_return_materializers(...)`, this checks the default materializer registry to find a materializer for the type of the output. If no materializer is registered, the output of this method will not contain an entry for this output. Raises: StepInterfaceError: (Only if `ensure_complete` is set to `True`) If an output does not have an explicit materializer assigned to it and there is no default materializer registered for the output type. \"\"\" materializers = self . _explicit_materializers for output_name , output_type in self . OUTPUT_SIGNATURE . items (): if output_name in materializers : # Materializer for this output was set explicitly pass elif default_materializer_registry . is_registered ( output_type ): materializer = default_materializer_registry [ output_type ] materializers [ output_name ] = materializer else : if ensure_complete : raise StepInterfaceError ( f \"Unable to find materializer for output \" f \"' { output_name } ' of type ` { output_type } ` in step \" f \"' { self . name } '. Please make sure to either \" f \"explicitly set a materializer for step outputs \" f \"using `step.with_return_materializers(...)` or \" f \"registering a default materializer for specific \" f \"types by subclassing `BaseMaterializer` and setting \" f \"its `ASSOCIATED_TYPES` class variable.\" ) return materializers @property def _internal_execution_parameters ( self ) -> Dict [ str , Any ]: \"\"\"ZenML internal execution parameters for this step.\"\"\" parameters = { PARAM_PIPELINE_PARAMETER_NAME : self . pipeline_parameter_name } if self . enable_cache : # Caching is enabled so we compute a hash of the step function code # and materializers to catch changes in the step behavior def _get_hashed_source ( value : Any ) -> str : \"\"\"Returns a hash of the objects source code.\"\"\" source_code = inspect . getsource ( value ) return hashlib . sha256 ( source_code . encode ( \"utf-8\" )) . hexdigest () # If the step was defined using the functional api, only track # changes to the entrypoint function. Otherwise track changes to # the entire step class. source_object = ( self . entrypoint if self . _created_by_functional_api else self . __class__ ) parameters [ \"step_source\" ] = _get_hashed_source ( source_object ) for name , materializer in self . get_materializers () . items (): key = f \" { name } _materializer_source\" parameters [ key ] = _get_hashed_source ( materializer ) else : # Add a random string to the execution properties to disable caching random_string = f \" { random . getrandbits ( 128 ) : 032x } \" parameters [ \"disable_cache\" ] = random_string return { INTERNAL_EXECUTION_PARAMETER_PREFIX + key : value for key , value in parameters . items () } def _verify_init_arguments ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Verifies the initialization args and kwargs of this step. This method makes sure that there is only a config object passed at initialization and that it was passed using the correct name and type specified in the step declaration. If the correct config object was found, additionally saves the config parameters to `self.PARAM_SPEC`. Args: *args: The args passed to the init method of this step. **kwargs: The kwargs passed to the init method of this step. Raises: StepInterfaceError: If there are too many arguments or arguments with a wrong name/type. \"\"\" maximum_arg_count = 1 if self . CONFIG_CLASS else 0 arg_count = len ( args ) + len ( kwargs ) if arg_count > maximum_arg_count : raise StepInterfaceError ( f \"Too many arguments ( { arg_count } , expected: \" f \" { maximum_arg_count } ) passed when creating a \" f \"' { self . name } ' step.\" ) if self . CONFIG_PARAMETER_NAME and self . CONFIG_CLASS : if args : config = args [ 0 ] elif kwargs : key , config = kwargs . popitem () if key != self . CONFIG_PARAMETER_NAME : raise StepInterfaceError ( f \"Unknown keyword argument ' { key } ' when creating a \" f \"' { self . name } ' step, only expected a single \" f \"argument with key ' { self . CONFIG_PARAMETER_NAME } '.\" ) else : # This step requires configuration parameters but no config # object was passed as an argument. The parameters might be # set via default values in the config class or in a # configuration file, so we continue for now and verify # that all parameters are set before running the step return if not isinstance ( config , self . CONFIG_CLASS ): raise StepInterfaceError ( f \"` { config } ` object passed when creating a \" f \"' { self . name } ' step is not a \" f \"` { self . CONFIG_CLASS . __name__ } ` instance.\" ) self . PARAM_SPEC = config . dict () def _verify_output_spec ( self ) -> None : \"\"\"Verifies the explicitly set output artifact types of this step. Raises: StepInterfaceError: If an output artifact type is specified for a non-existent step output or the artifact type is not allowed for the corresponding output type. \"\"\" for output_name , artifact_type in self . OUTPUT_SPEC . items (): if output_name not in self . OUTPUT_SIGNATURE : raise StepInterfaceError ( f \"Found explicit artifact type for unrecognized output \" f \"' { output_name } ' in step ' { self . name } '. Output \" f \"artifact types can only be specified for the outputs \" f \"of this step: { set ( self . OUTPUT_SIGNATURE ) } .\" ) if not issubclass ( artifact_type , BaseArtifact ): raise StepInterfaceError ( f \"Invalid artifact type ( { artifact_type } ) for output \" f \"' { output_name } ' of step ' { self . name } '. Only \" f \"`BaseArtifact` subclasses are allowed as artifact types.\" ) output_type = self . OUTPUT_SIGNATURE [ output_name ] allowed_artifact_types = set ( type_registry . get_artifact_type ( output_type ) ) if artifact_type not in allowed_artifact_types : raise StepInterfaceError ( f \"Artifact type ` { artifact_type } ` for output \" f \"' { output_name } ' of step ' { self . name } ' is not an \" f \"allowed artifact type for the defined output type \" f \"` { output_type } `. Allowed artifact types: \" f \" { allowed_artifact_types } . If you want to extend the \" f \"allowed artifact types, implement a custom \" f \"`BaseMaterializer` subclass and set its \" f \"`ASSOCIATED_ARTIFACT_TYPES` and `ASSOCIATED_TYPES` \" f \"accordingly.\" ) def _update_and_verify_parameter_spec ( self ) -> None : \"\"\"Verifies and prepares the config parameters for running this step. When the step requires config parameters, this method: - checks if config parameters were set via a config object or file - tries to set missing config parameters from default values of the config class Raises: MissingStepParameterError: If no value could be found for one or more config parameters. StepInterfaceError: If a config parameter value couldn't be serialized to json. \"\"\" if self . CONFIG_CLASS : # we need to store a value for all config keys inside the # metadata store to make sure caching works as expected missing_keys = [] for name , field in self . CONFIG_CLASS . __fields__ . items (): if name in self . PARAM_SPEC : # a value for this parameter has been set already continue if field . required : # this field has no default value set and therefore needs # to be passed via an initialized config object missing_keys . append ( name ) else : # use default value from the pydantic config class self . PARAM_SPEC [ name ] = field . default if missing_keys : raise MissingStepParameterError ( self . name , missing_keys , self . CONFIG_CLASS ) def _prepare_input_artifacts ( self , * artifacts : Channel , ** kw_artifacts : Channel ) -> Dict [ str , Channel ]: \"\"\"Verifies and prepares the input artifacts for running this step. Args: *artifacts: Positional input artifacts passed to the __call__ method. **kw_artifacts: Keyword input artifacts passed to the __call__ method. Returns: Dictionary containing both the positional and keyword input artifacts. Raises: StepInterfaceError: If there are too many or too few artifacts. \"\"\" input_artifact_keys = list ( self . INPUT_SIGNATURE . keys ()) if len ( artifacts ) > len ( input_artifact_keys ): raise StepInterfaceError ( f \"Too many input artifacts for step ' { self . name } '. \" f \"This step expects { len ( input_artifact_keys ) } artifact(s) \" f \"but got { len ( artifacts ) + len ( kw_artifacts ) } .\" ) combined_artifacts = {} for i , artifact in enumerate ( artifacts ): if not isinstance ( artifact , Channel ): raise StepInterfaceError ( f \"Wrong argument type (` { type ( artifact ) } `) for positional \" f \"argument { i } of step ' { self . name } '. Only outputs \" f \"from previous steps can be used as arguments when \" f \"connecting steps.\" ) key = input_artifact_keys [ i ] combined_artifacts [ key ] = artifact for key , artifact in kw_artifacts . items (): if key in combined_artifacts : # an artifact for this key was already set by # the positional input artifacts raise StepInterfaceError ( f \"Unexpected keyword argument ' { key } ' for step \" f \"' { self . name } '. An artifact for this key was \" f \"already passed as a positional argument.\" ) if not isinstance ( artifact , Channel ): raise StepInterfaceError ( f \"Wrong argument type (` { type ( artifact ) } `) for argument \" f \"' { key } ' of step ' { self . name } '. Only outputs from \" f \"previous steps can be used as arguments when \" f \"connecting steps.\" ) combined_artifacts [ key ] = artifact # check if there are any missing or unexpected artifacts expected_artifacts = set ( self . INPUT_SIGNATURE . keys ()) actual_artifacts = set ( combined_artifacts . keys ()) missing_artifacts = expected_artifacts - actual_artifacts unexpected_artifacts = actual_artifacts - expected_artifacts if missing_artifacts : raise StepInterfaceError ( f \"Missing input artifact(s) for step \" f \"' { self . name } ': { missing_artifacts } .\" ) if unexpected_artifacts : raise StepInterfaceError ( f \"Unexpected input artifact(s) for step \" f \"' { self . name } ': { unexpected_artifacts } . This step \" f \"only requires the following artifacts: { expected_artifacts } .\" ) return combined_artifacts def __call__ ( self , * artifacts : Channel , ** kw_artifacts : Channel ) -> Union [ Channel , List [ Channel ]]: \"\"\"Generates a component when called.\"\"\" # TODO [ENG-157]: replaces Channels with ZenML class (BaseArtifact?) self . _update_and_verify_parameter_spec () # Prepare the input artifacts and spec input_artifacts = self . _prepare_input_artifacts ( * artifacts , ** kw_artifacts ) self . INPUT_SPEC = { arg_name : artifact_type . type # type:ignore[misc] for arg_name , artifact_type in input_artifacts . items () } # make sure we have registered materializers for each output materializers = self . get_materializers ( ensure_complete = True ) # Prepare the output artifacts and spec for key , value in self . OUTPUT_SIGNATURE . items (): verified_types = type_registry . get_artifact_type ( value ) if key not in self . OUTPUT_SPEC : self . OUTPUT_SPEC [ key ] = verified_types [ 0 ] execution_parameters = { ** self . PARAM_SPEC , ** self . _internal_execution_parameters , } # Convert execution parameter values to strings try : execution_parameters = { k : json . dumps ( v ) for k , v in execution_parameters . items () } except TypeError as e : raise StepInterfaceError ( f \"Failed to serialize execution parameters for step \" f \"' { self . name } '. Please make sure to only use \" f \"json serializable parameter values.\" ) from e source_fn = getattr ( self , STEP_INNER_FUNC_NAME ) component_class = generate_component_class ( step_name = self . name , step_module = self . __module__ , input_spec = self . INPUT_SPEC , output_spec = self . OUTPUT_SPEC , execution_parameter_names = set ( execution_parameters ), step_function = source_fn , materializers = materializers , ) self . _component = component_class ( ** input_artifacts , ** execution_parameters ) # Resolve the returns in the right order. returns = [ self . component . outputs [ key ] for key in self . OUTPUT_SPEC ] # If its one return we just return the one channel not as a list if len ( returns ) == 1 : return returns [ 0 ] else : return returns @property def component ( self ) -> _ZenMLSimpleComponent : \"\"\"Returns a TFX component.\"\"\" if not self . _component : raise StepInterfaceError ( \"Trying to access the step component \" \"before creating it via calling the step.\" ) return self . _component def with_return_materializers ( self : T , materializers : Union [ Type [ BaseMaterializer ], Dict [ str , Type [ BaseMaterializer ]] ], ) -> T : \"\"\"Register materializers for step outputs. If a single materializer is passed, it will be used for all step outputs. Otherwise, the dictionary keys specify the output names for which the materializers will be used. Args: materializers: The materializers for the outputs of this step. Returns: The object that this method was called on. Raises: StepInterfaceError: If a materializer is not a `BaseMaterializer` subclass or a materializer for a non-existent output is given. \"\"\" def _is_materializer_class ( value : Any ) -> bool : \"\"\"Checks whether the given object is a `BaseMaterializer` subclass.\"\"\" is_class = isinstance ( value , type ) return is_class and issubclass ( value , BaseMaterializer ) if isinstance ( materializers , dict ): allowed_output_names = set ( self . OUTPUT_SIGNATURE ) for output_name , materializer in materializers . items (): if output_name not in allowed_output_names : raise StepInterfaceError ( f \"Got unexpected materializers for non-existent \" f \"output ' { output_name } ' in step ' { self . name } '. \" f \"Only materializers for the outputs \" f \" { allowed_output_names } of this step can\" f \" be registered.\" ) if not _is_materializer_class ( materializer ): raise StepInterfaceError ( f \"Got unexpected object ` { materializer } ` as \" f \"materializer for output ' { output_name } ' of step \" f \"' { self . name } '. Only `BaseMaterializer` \" f \"subclasses are allowed.\" ) self . _explicit_materializers [ output_name ] = materializer elif _is_materializer_class ( materializers ): # Set the materializer for all outputs of this step self . _explicit_materializers = { key : materializers for key in self . OUTPUT_SIGNATURE } else : raise StepInterfaceError ( f \"Got unexpected object ` { materializers } ` as output \" f \"materializer for step ' { self . name } '. Only \" f \"`BaseMaterializer` subclasses or dictionaries mapping \" f \"output names to `BaseMaterializer` subclasses are allowed \" f \"as input when specifying return materializers.\" ) return self component : _ZenMLSimpleComponent property readonly Returns a TFX component. __call__ ( self , * artifacts , ** kw_artifacts ) special Generates a component when called. Source code in zenml/steps/base_step.py def __call__ ( self , * artifacts : Channel , ** kw_artifacts : Channel ) -> Union [ Channel , List [ Channel ]]: \"\"\"Generates a component when called.\"\"\" # TODO [ENG-157]: replaces Channels with ZenML class (BaseArtifact?) self . _update_and_verify_parameter_spec () # Prepare the input artifacts and spec input_artifacts = self . _prepare_input_artifacts ( * artifacts , ** kw_artifacts ) self . INPUT_SPEC = { arg_name : artifact_type . type # type:ignore[misc] for arg_name , artifact_type in input_artifacts . items () } # make sure we have registered materializers for each output materializers = self . get_materializers ( ensure_complete = True ) # Prepare the output artifacts and spec for key , value in self . OUTPUT_SIGNATURE . items (): verified_types = type_registry . get_artifact_type ( value ) if key not in self . OUTPUT_SPEC : self . OUTPUT_SPEC [ key ] = verified_types [ 0 ] execution_parameters = { ** self . PARAM_SPEC , ** self . _internal_execution_parameters , } # Convert execution parameter values to strings try : execution_parameters = { k : json . dumps ( v ) for k , v in execution_parameters . items () } except TypeError as e : raise StepInterfaceError ( f \"Failed to serialize execution parameters for step \" f \"' { self . name } '. Please make sure to only use \" f \"json serializable parameter values.\" ) from e source_fn = getattr ( self , STEP_INNER_FUNC_NAME ) component_class = generate_component_class ( step_name = self . name , step_module = self . __module__ , input_spec = self . INPUT_SPEC , output_spec = self . OUTPUT_SPEC , execution_parameter_names = set ( execution_parameters ), step_function = source_fn , materializers = materializers , ) self . _component = component_class ( ** input_artifacts , ** execution_parameters ) # Resolve the returns in the right order. returns = [ self . component . outputs [ key ] for key in self . OUTPUT_SPEC ] # If its one return we just return the one channel not as a list if len ( returns ) == 1 : return returns [ 0 ] else : return returns entrypoint ( self , * args , ** kwargs ) Abstract method for core step logic. Source code in zenml/steps/base_step.py @abstractmethod def entrypoint ( self , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Abstract method for core step logic.\"\"\" get_materializers ( self , ensure_complete = False ) Returns available materializers for the outputs of this step. Parameters: Name Type Description Default ensure_complete bool If set to True , this method will raise a StepInterfaceError if no materializer can be found for an output. False Returns: Type Description Dict[str, Type[zenml.materializers.base_materializer.BaseMaterializer]] A dictionary mapping output names to BaseMaterializer subclasses. If no explicit materializer was set using step.with_return_materializers(...) , this checks the default materializer registry to find a materializer for the type of the output. If no materializer is registered, the output of this method will not contain an entry for this output. Exceptions: Type Description StepInterfaceError (Only if ensure_complete is set to True ) If an output does not have an explicit materializer assigned to it and there is no default materializer registered for the output type. Source code in zenml/steps/base_step.py def get_materializers ( self , ensure_complete : bool = False ) -> Dict [ str , Type [ BaseMaterializer ]]: \"\"\"Returns available materializers for the outputs of this step. Args: ensure_complete: If set to `True`, this method will raise a `StepInterfaceError` if no materializer can be found for an output. Returns: A dictionary mapping output names to `BaseMaterializer` subclasses. If no explicit materializer was set using `step.with_return_materializers(...)`, this checks the default materializer registry to find a materializer for the type of the output. If no materializer is registered, the output of this method will not contain an entry for this output. Raises: StepInterfaceError: (Only if `ensure_complete` is set to `True`) If an output does not have an explicit materializer assigned to it and there is no default materializer registered for the output type. \"\"\" materializers = self . _explicit_materializers for output_name , output_type in self . OUTPUT_SIGNATURE . items (): if output_name in materializers : # Materializer for this output was set explicitly pass elif default_materializer_registry . is_registered ( output_type ): materializer = default_materializer_registry [ output_type ] materializers [ output_name ] = materializer else : if ensure_complete : raise StepInterfaceError ( f \"Unable to find materializer for output \" f \"' { output_name } ' of type ` { output_type } ` in step \" f \"' { self . name } '. Please make sure to either \" f \"explicitly set a materializer for step outputs \" f \"using `step.with_return_materializers(...)` or \" f \"registering a default materializer for specific \" f \"types by subclassing `BaseMaterializer` and setting \" f \"its `ASSOCIATED_TYPES` class variable.\" ) return materializers with_return_materializers ( self , materializers ) Register materializers for step outputs. If a single materializer is passed, it will be used for all step outputs. Otherwise, the dictionary keys specify the output names for which the materializers will be used. Parameters: Name Type Description Default materializers Union[Type[zenml.materializers.base_materializer.BaseMaterializer], Dict[str, Type[zenml.materializers.base_materializer.BaseMaterializer]]] The materializers for the outputs of this step. required Returns: Type Description ~T The object that this method was called on. Exceptions: Type Description StepInterfaceError If a materializer is not a BaseMaterializer subclass or a materializer for a non-existent output is given. Source code in zenml/steps/base_step.py def with_return_materializers ( self : T , materializers : Union [ Type [ BaseMaterializer ], Dict [ str , Type [ BaseMaterializer ]] ], ) -> T : \"\"\"Register materializers for step outputs. If a single materializer is passed, it will be used for all step outputs. Otherwise, the dictionary keys specify the output names for which the materializers will be used. Args: materializers: The materializers for the outputs of this step. Returns: The object that this method was called on. Raises: StepInterfaceError: If a materializer is not a `BaseMaterializer` subclass or a materializer for a non-existent output is given. \"\"\" def _is_materializer_class ( value : Any ) -> bool : \"\"\"Checks whether the given object is a `BaseMaterializer` subclass.\"\"\" is_class = isinstance ( value , type ) return is_class and issubclass ( value , BaseMaterializer ) if isinstance ( materializers , dict ): allowed_output_names = set ( self . OUTPUT_SIGNATURE ) for output_name , materializer in materializers . items (): if output_name not in allowed_output_names : raise StepInterfaceError ( f \"Got unexpected materializers for non-existent \" f \"output ' { output_name } ' in step ' { self . name } '. \" f \"Only materializers for the outputs \" f \" { allowed_output_names } of this step can\" f \" be registered.\" ) if not _is_materializer_class ( materializer ): raise StepInterfaceError ( f \"Got unexpected object ` { materializer } ` as \" f \"materializer for output ' { output_name } ' of step \" f \"' { self . name } '. Only `BaseMaterializer` \" f \"subclasses are allowed.\" ) self . _explicit_materializers [ output_name ] = materializer elif _is_materializer_class ( materializers ): # Set the materializer for all outputs of this step self . _explicit_materializers = { key : materializers for key in self . OUTPUT_SIGNATURE } else : raise StepInterfaceError ( f \"Got unexpected object ` { materializers } ` as output \" f \"materializer for step ' { self . name } '. Only \" f \"`BaseMaterializer` subclasses or dictionaries mapping \" f \"output names to `BaseMaterializer` subclasses are allowed \" f \"as input when specifying return materializers.\" ) return self BaseStepMeta ( type ) Metaclass for BaseStep . Checks whether everything passed in: * Has a matching materializer. * Is a subclass of the Config class Source code in zenml/steps/base_step.py class BaseStepMeta ( type ): \"\"\"Metaclass for `BaseStep`. Checks whether everything passed in: * Has a matching materializer. * Is a subclass of the Config class \"\"\" def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BaseStepMeta\" : \"\"\"Set up a new class with a qualified spec.\"\"\" dct . setdefault ( \"PARAM_SPEC\" , {}) dct . setdefault ( \"INPUT_SPEC\" , {}) dct . setdefault ( \"OUTPUT_SPEC\" , {}) cls = cast ( Type [ \"BaseStep\" ], super () . __new__ ( mcs , name , bases , dct )) cls . INPUT_SIGNATURE = {} cls . OUTPUT_SIGNATURE = {} cls . CONFIG_PARAMETER_NAME = None cls . CONFIG_CLASS = None cls . CONTEXT_PARAMETER_NAME = None # Get the signature of the step function step_function_signature = inspect . getfullargspec ( getattr ( cls , STEP_INNER_FUNC_NAME ) ) if bases : # We're not creating the abstract `BaseStep` class # but a concrete implementation. Make sure the step function # signature does not contain variable *args or **kwargs variable_arguments = None if step_function_signature . varargs : variable_arguments = f \"* { step_function_signature . varargs } \" elif step_function_signature . varkw : variable_arguments = f \"** { step_function_signature . varkw } \" if variable_arguments : raise StepInterfaceError ( f \"Unable to create step ' { name } ' with variable arguments \" f \"' { variable_arguments } '. Please make sure your step \" f \"functions are defined with a fixed amount of arguments.\" ) step_function_args = ( step_function_signature . args + step_function_signature . kwonlyargs ) # Remove 'self' from the signature if it exists if step_function_args and step_function_args [ 0 ] == \"self\" : step_function_args . pop ( 0 ) # Verify the input arguments of the step function for arg in step_function_args : arg_type = step_function_signature . annotations . get ( arg , None ) if not arg_type : raise StepInterfaceError ( f \"Missing type annotation for argument ' { arg } ' when \" f \"trying to create step ' { name } '. Please make sure to \" f \"include type annotations for all your step inputs \" f \"and outputs.\" ) if issubclass ( arg_type , BaseStepConfig ): # Raise an error if we already found a config in the signature if cls . CONFIG_CLASS is not None : raise StepInterfaceError ( f \"Found multiple configuration arguments \" f \"(' { cls . CONFIG_PARAMETER_NAME } ' and ' { arg } ') when \" f \"trying to create step ' { name } '. Please make sure to \" f \"only have one `BaseStepConfig` subclass as input \" f \"argument for a step.\" ) cls . CONFIG_PARAMETER_NAME = arg cls . CONFIG_CLASS = arg_type elif issubclass ( arg_type , StepContext ): if cls . CONTEXT_PARAMETER_NAME is not None : raise StepInterfaceError ( f \"Found multiple context arguments \" f \"(' { cls . CONTEXT_PARAMETER_NAME } ' and ' { arg } ') when \" f \"trying to create step ' { name } '. Please make sure to \" f \"only have one `StepContext` as input \" f \"argument for a step.\" ) cls . CONTEXT_PARAMETER_NAME = arg else : # Can't do any check for existing materializers right now # as they might get be defined later, so we simply store the # argument name and type for later use. cls . INPUT_SIGNATURE . update ({ arg : arg_type }) # Parse the returns of the step function return_type = step_function_signature . annotations . get ( \"return\" , None ) if return_type is not None : if isinstance ( return_type , Output ): cls . OUTPUT_SIGNATURE = dict ( return_type . items ()) else : cls . OUTPUT_SIGNATURE [ SINGLE_RETURN_OUT_NAME ] = return_type # Raise an exception if input and output names of a step overlap as # tfx requires them to be unique # TODO [ENG-155]: Can we prefix inputs and outputs to avoid this # restriction? counter : Counter [ str ] = collections . Counter () counter . update ( list ( cls . INPUT_SIGNATURE )) counter . update ( list ( cls . OUTPUT_SIGNATURE )) if cls . CONFIG_CLASS : counter . update ( list ( cls . CONFIG_CLASS . __fields__ . keys ())) shared_keys = { k for k in counter . elements () if counter [ k ] > 1 } if shared_keys : raise StepInterfaceError ( f \"The following keys are overlapping in the input, output and \" f \"config parameter names of step ' { name } ': { shared_keys } . \" f \"Please make sure that your input, output and config \" f \"parameter names are unique.\" ) return cls __new__ ( mcs , name , bases , dct ) special staticmethod Set up a new class with a qualified spec. Source code in zenml/steps/base_step.py def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BaseStepMeta\" : \"\"\"Set up a new class with a qualified spec.\"\"\" dct . setdefault ( \"PARAM_SPEC\" , {}) dct . setdefault ( \"INPUT_SPEC\" , {}) dct . setdefault ( \"OUTPUT_SPEC\" , {}) cls = cast ( Type [ \"BaseStep\" ], super () . __new__ ( mcs , name , bases , dct )) cls . INPUT_SIGNATURE = {} cls . OUTPUT_SIGNATURE = {} cls . CONFIG_PARAMETER_NAME = None cls . CONFIG_CLASS = None cls . CONTEXT_PARAMETER_NAME = None # Get the signature of the step function step_function_signature = inspect . getfullargspec ( getattr ( cls , STEP_INNER_FUNC_NAME ) ) if bases : # We're not creating the abstract `BaseStep` class # but a concrete implementation. Make sure the step function # signature does not contain variable *args or **kwargs variable_arguments = None if step_function_signature . varargs : variable_arguments = f \"* { step_function_signature . varargs } \" elif step_function_signature . varkw : variable_arguments = f \"** { step_function_signature . varkw } \" if variable_arguments : raise StepInterfaceError ( f \"Unable to create step ' { name } ' with variable arguments \" f \"' { variable_arguments } '. Please make sure your step \" f \"functions are defined with a fixed amount of arguments.\" ) step_function_args = ( step_function_signature . args + step_function_signature . kwonlyargs ) # Remove 'self' from the signature if it exists if step_function_args and step_function_args [ 0 ] == \"self\" : step_function_args . pop ( 0 ) # Verify the input arguments of the step function for arg in step_function_args : arg_type = step_function_signature . annotations . get ( arg , None ) if not arg_type : raise StepInterfaceError ( f \"Missing type annotation for argument ' { arg } ' when \" f \"trying to create step ' { name } '. Please make sure to \" f \"include type annotations for all your step inputs \" f \"and outputs.\" ) if issubclass ( arg_type , BaseStepConfig ): # Raise an error if we already found a config in the signature if cls . CONFIG_CLASS is not None : raise StepInterfaceError ( f \"Found multiple configuration arguments \" f \"(' { cls . CONFIG_PARAMETER_NAME } ' and ' { arg } ') when \" f \"trying to create step ' { name } '. Please make sure to \" f \"only have one `BaseStepConfig` subclass as input \" f \"argument for a step.\" ) cls . CONFIG_PARAMETER_NAME = arg cls . CONFIG_CLASS = arg_type elif issubclass ( arg_type , StepContext ): if cls . CONTEXT_PARAMETER_NAME is not None : raise StepInterfaceError ( f \"Found multiple context arguments \" f \"(' { cls . CONTEXT_PARAMETER_NAME } ' and ' { arg } ') when \" f \"trying to create step ' { name } '. Please make sure to \" f \"only have one `StepContext` as input \" f \"argument for a step.\" ) cls . CONTEXT_PARAMETER_NAME = arg else : # Can't do any check for existing materializers right now # as they might get be defined later, so we simply store the # argument name and type for later use. cls . INPUT_SIGNATURE . update ({ arg : arg_type }) # Parse the returns of the step function return_type = step_function_signature . annotations . get ( \"return\" , None ) if return_type is not None : if isinstance ( return_type , Output ): cls . OUTPUT_SIGNATURE = dict ( return_type . items ()) else : cls . OUTPUT_SIGNATURE [ SINGLE_RETURN_OUT_NAME ] = return_type # Raise an exception if input and output names of a step overlap as # tfx requires them to be unique # TODO [ENG-155]: Can we prefix inputs and outputs to avoid this # restriction? counter : Counter [ str ] = collections . Counter () counter . update ( list ( cls . INPUT_SIGNATURE )) counter . update ( list ( cls . OUTPUT_SIGNATURE )) if cls . CONFIG_CLASS : counter . update ( list ( cls . CONFIG_CLASS . __fields__ . keys ())) shared_keys = { k for k in counter . elements () if counter [ k ] > 1 } if shared_keys : raise StepInterfaceError ( f \"The following keys are overlapping in the input, output and \" f \"config parameter names of step ' { name } ': { shared_keys } . \" f \"Please make sure that your input, output and config \" f \"parameter names are unique.\" ) return cls base_step_config BaseStepConfig ( BaseModel ) pydantic-model Base configuration class to pass execution params into a step. Source code in zenml/steps/base_step_config.py class BaseStepConfig ( BaseModel ): \"\"\"Base configuration class to pass execution params into a step.\"\"\" builtin_steps special pandas_analyzer PandasAnalyzer ( BaseAnalyzerStep ) Simple step implementation which analyzes a given pd.DataFrame Source code in zenml/steps/builtin_steps/pandas_analyzer.py class PandasAnalyzer ( BaseAnalyzerStep ): \"\"\"Simple step implementation which analyzes a given pd.DataFrame\"\"\" # Manually defining the type of the output artifacts OUTPUT_SPEC = { \"statistics\" : StatisticsArtifact , \"schema\" : SchemaArtifact } def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , config : PandasAnalyzerConfig , ) -> Output ( # type:ignore[valid-type] statistics = pd . DataFrame , schema = pd . DataFrame ): \"\"\"Main entrypoint function for the pandas analyzer Args: dataset: pd.DataFrame, the given dataset config: the configuration of the step Returns: the statistics and the schema of the given dataframe \"\"\" statistics = dataset . describe ( percentiles = config . percentiles , include = config . include , exclude = config . exclude , ) . T schema = dataset . dtypes . to_frame () . T . astype ( str ) return statistics , schema CONFIG_CLASS ( BaseAnalyzerConfig ) pydantic-model Config class for the PandasAnalyzer Config Source code in zenml/steps/builtin_steps/pandas_analyzer.py class PandasAnalyzerConfig ( BaseAnalyzerConfig ): \"\"\"Config class for the PandasAnalyzer Config\"\"\" percentiles : List [ float ] = [ 0.25 , 0.5 , 0.75 ] include : Optional [ Union [ str , List [ Type [ Any ]]]] = None exclude : Optional [ Union [ str , List [ Type [ Any ]]]] = None entrypoint ( self , dataset , config ) Main entrypoint function for the pandas analyzer Parameters: Name Type Description Default dataset DataFrame pd.DataFrame, the given dataset required config PandasAnalyzerConfig the configuration of the step required Returns: Type Description <zenml.steps.step_output.Output object at 0x7f2d93bd5b20> the statistics and the schema of the given dataframe Source code in zenml/steps/builtin_steps/pandas_analyzer.py def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , config : PandasAnalyzerConfig , ) -> Output ( # type:ignore[valid-type] statistics = pd . DataFrame , schema = pd . DataFrame ): \"\"\"Main entrypoint function for the pandas analyzer Args: dataset: pd.DataFrame, the given dataset config: the configuration of the step Returns: the statistics and the schema of the given dataframe \"\"\" statistics = dataset . describe ( percentiles = config . percentiles , include = config . include , exclude = config . exclude , ) . T schema = dataset . dtypes . to_frame () . T . astype ( str ) return statistics , schema PandasAnalyzerConfig ( BaseAnalyzerConfig ) pydantic-model Config class for the PandasAnalyzer Config Source code in zenml/steps/builtin_steps/pandas_analyzer.py class PandasAnalyzerConfig ( BaseAnalyzerConfig ): \"\"\"Config class for the PandasAnalyzer Config\"\"\" percentiles : List [ float ] = [ 0.25 , 0.5 , 0.75 ] include : Optional [ Union [ str , List [ Type [ Any ]]]] = None exclude : Optional [ Union [ str , List [ Type [ Any ]]]] = None pandas_datasource PandasDatasource ( BaseDatasourceStep ) Simple step implementation to ingest from a csv file using pandas Source code in zenml/steps/builtin_steps/pandas_datasource.py class PandasDatasource ( BaseDatasourceStep ): \"\"\"Simple step implementation to ingest from a csv file using pandas\"\"\" def entrypoint ( # type: ignore[override] self , config : PandasDatasourceConfig , ) -> pd . DataFrame : \"\"\"Main entrypoint method for the PandasDatasource Args: config: the configuration of the step Returns: the resulting dataframe \"\"\" return pd . read_csv ( filepath_or_buffer = config . path , sep = config . sep , header = config . header , names = config . names , index_col = config . index_col , ) CONFIG_CLASS ( BaseDatasourceConfig ) pydantic-model Config class for the pandas csv datasource Source code in zenml/steps/builtin_steps/pandas_datasource.py class PandasDatasourceConfig ( BaseDatasourceConfig ): \"\"\"Config class for the pandas csv datasource\"\"\" path : str sep : str = \",\" header : Union [ int , List [ int ], str ] = \"infer\" names : Optional [ List [ str ]] = None index_col : Optional [ Union [ int , str , List [ Union [ int , str ]], bool ]] = None entrypoint ( self , config ) Main entrypoint method for the PandasDatasource Parameters: Name Type Description Default config PandasDatasourceConfig the configuration of the step required Returns: Type Description DataFrame the resulting dataframe Source code in zenml/steps/builtin_steps/pandas_datasource.py def entrypoint ( # type: ignore[override] self , config : PandasDatasourceConfig , ) -> pd . DataFrame : \"\"\"Main entrypoint method for the PandasDatasource Args: config: the configuration of the step Returns: the resulting dataframe \"\"\" return pd . read_csv ( filepath_or_buffer = config . path , sep = config . sep , header = config . header , names = config . names , index_col = config . index_col , ) PandasDatasourceConfig ( BaseDatasourceConfig ) pydantic-model Config class for the pandas csv datasource Source code in zenml/steps/builtin_steps/pandas_datasource.py class PandasDatasourceConfig ( BaseDatasourceConfig ): \"\"\"Config class for the pandas csv datasource\"\"\" path : str sep : str = \",\" header : Union [ int , List [ int ], str ] = \"infer\" names : Optional [ List [ str ]] = None index_col : Optional [ Union [ int , str , List [ Union [ int , str ]], bool ]] = None step_context StepContext Provides additional context inside a step function. This class is used to access the metadata store, materializers and artifacts inside a step function. To use it, add a StepContext object to the signature of your step function like this: @step def my_step(context: StepContext, ...) context.get_output_materializer(...) You do not need to create a StepContext object yourself and pass it when creating the step, as long as you specify it in the signature ZenML will create the StepContext and automatically pass it when executing your step. Note : When using a StepContext inside a step, ZenML disables caching for this step by default as the context provides access to external resources which might influence the result of your step execution. To enable caching anyway, explicitly enable it in the @step decorator or when initializing your custom step class. Source code in zenml/steps/step_context.py class StepContext : \"\"\"Provides additional context inside a step function. This class is used to access the metadata store, materializers and artifacts inside a step function. To use it, add a `StepContext` object to the signature of your step function like this: @step def my_step(context: StepContext, ...) context.get_output_materializer(...) You do not need to create a `StepContext` object yourself and pass it when creating the step, as long as you specify it in the signature ZenML will create the `StepContext` and automatically pass it when executing your step. **Note**: When using a `StepContext` inside a step, ZenML disables caching for this step by default as the context provides access to external resources which might influence the result of your step execution. To enable caching anyway, explicitly enable it in the `@step` decorator or when initializing your custom step class. \"\"\" def __init__ ( self , step_name : str , output_materializers : Dict [ str , Type [ \"BaseMaterializer\" ]], output_artifacts : Dict [ str , \"BaseArtifact\" ], ): \"\"\"Initializes a StepContext instance. Args: step_name: The name of the step that this context is used in. output_materializers: The output materializers of the step that this context is used in. output_artifacts: The output artifacts of the step that this context is used in. Raises: StepContextError: If the keys of the output materializers and output artifacts do not match. \"\"\" if output_materializers . keys () != output_artifacts . keys (): raise StepContextError ( f \"Mismatched keys in output materializers and output \" f \"artifacts for step ' { step_name } '. Output materializer \" f \"keys: { set ( output_materializers ) } , output artifact \" f \"keys: { set ( output_artifacts ) } \" ) self . step_name = step_name self . _outputs = { key : StepContextOutput ( output_materializers [ key ], output_artifacts [ key ] ) for key in output_materializers . keys () } self . _metadata_store = Repository () . active_stack . metadata_store def _get_output ( self , output_name : Optional [ str ] = None ) -> StepContextOutput : \"\"\"Returns the materializer and artifact URI for a given step output. Args: output_name: Optional name of the output for which to get the materializer and URI. Returns: Tuple containing the materializer and artifact URI for the given output. Raises: StepContextError: If the step has no outputs, no output for the given `output_name` or if no `output_name` was given but the step has multiple outputs. \"\"\" output_count = len ( self . _outputs ) if output_count == 0 : raise StepContextError ( f \"Unable to get step output for step ' { self . step_name } ': \" f \"This step does not have any outputs.\" ) if not output_name and output_count > 1 : raise StepContextError ( f \"Unable to get step output for step ' { self . step_name } ': \" f \"This step has multiple outputs ( { set ( self . _outputs ) } ), \" f \"please specify which output to return.\" ) if output_name : if output_name not in self . _outputs : raise StepContextError ( f \"Unable to get step output ' { output_name } ' for \" f \"step ' { self . step_name } '. This step does not have an \" f \"output with the given name, please specify one of the \" f \"available outputs: { set ( self . _outputs ) } .\" ) return self . _outputs [ output_name ] else : return next ( iter ( self . _outputs . values ())) @property def metadata_store ( self ) -> \"BaseMetadataStore\" : \"\"\"Returns an instance of the metadata store that is used to store metadata about the step (and the corresponding pipeline) which is being executed.\"\"\" return self . _metadata_store def get_output_materializer ( self , output_name : Optional [ str ] = None , custom_materializer_class : Optional [ Type [ \"BaseMaterializer\" ]] = None , ) -> \"BaseMaterializer\" : \"\"\"Returns a materializer for a given step output. Args: output_name: Optional name of the output for which to get the materializer. If no name is given and the step only has a single output, the materializer of this output will be returned. If the step has multiple outputs, an exception will be raised. custom_materializer_class: If given, this `BaseMaterializer` subclass will be initialized with the output artifact instead of the materializer that was registered for this step output. Returns: A materializer initialized with the output artifact for the given output. Raises: StepContextError: If the step has no outputs, no output for the given `output_name` or if no `output_name` was given but the step has multiple outputs. \"\"\" materializer_class , artifact = self . _get_output ( output_name ) # use custom materializer class if provided or fallback to default # materializer for output materializer_class = custom_materializer_class or materializer_class return materializer_class ( artifact ) def get_output_artifact_uri ( self , output_name : Optional [ str ] = None ) -> str : \"\"\"Returns the artifact URI for a given step output. Args: output_name: Optional name of the output for which to get the URI. If no name is given and the step only has a single output, the URI of this output will be returned. If the step has multiple outputs, an exception will be raised. Returns: Artifact URI for the given output. Raises: StepContextError: If the step has no outputs, no output for the given `output_name` or if no `output_name` was given but the step has multiple outputs. \"\"\" return cast ( str , self . _get_output ( output_name ) . artifact . uri ) metadata_store : BaseMetadataStore property readonly Returns an instance of the metadata store that is used to store metadata about the step (and the corresponding pipeline) which is being executed. __init__ ( self , step_name , output_materializers , output_artifacts ) special Initializes a StepContext instance. Parameters: Name Type Description Default step_name str The name of the step that this context is used in. required output_materializers Dict[str, Type[BaseMaterializer]] The output materializers of the step that this context is used in. required output_artifacts Dict[str, BaseArtifact] The output artifacts of the step that this context is used in. required Exceptions: Type Description StepContextError If the keys of the output materializers and Source code in zenml/steps/step_context.py def __init__ ( self , step_name : str , output_materializers : Dict [ str , Type [ \"BaseMaterializer\" ]], output_artifacts : Dict [ str , \"BaseArtifact\" ], ): \"\"\"Initializes a StepContext instance. Args: step_name: The name of the step that this context is used in. output_materializers: The output materializers of the step that this context is used in. output_artifacts: The output artifacts of the step that this context is used in. Raises: StepContextError: If the keys of the output materializers and output artifacts do not match. \"\"\" if output_materializers . keys () != output_artifacts . keys (): raise StepContextError ( f \"Mismatched keys in output materializers and output \" f \"artifacts for step ' { step_name } '. Output materializer \" f \"keys: { set ( output_materializers ) } , output artifact \" f \"keys: { set ( output_artifacts ) } \" ) self . step_name = step_name self . _outputs = { key : StepContextOutput ( output_materializers [ key ], output_artifacts [ key ] ) for key in output_materializers . keys () } self . _metadata_store = Repository () . active_stack . metadata_store get_output_artifact_uri ( self , output_name = None ) Returns the artifact URI for a given step output. Parameters: Name Type Description Default output_name Optional[str] Optional name of the output for which to get the URI. If no name is given and the step only has a single output, the URI of this output will be returned. If the step has multiple outputs, an exception will be raised. None Returns: Type Description str Artifact URI for the given output. Exceptions: Type Description StepContextError If the step has no outputs, no output for the given output_name or if no output_name was given but the step has multiple outputs. Source code in zenml/steps/step_context.py def get_output_artifact_uri ( self , output_name : Optional [ str ] = None ) -> str : \"\"\"Returns the artifact URI for a given step output. Args: output_name: Optional name of the output for which to get the URI. If no name is given and the step only has a single output, the URI of this output will be returned. If the step has multiple outputs, an exception will be raised. Returns: Artifact URI for the given output. Raises: StepContextError: If the step has no outputs, no output for the given `output_name` or if no `output_name` was given but the step has multiple outputs. \"\"\" return cast ( str , self . _get_output ( output_name ) . artifact . uri ) get_output_materializer ( self , output_name = None , custom_materializer_class = None ) Returns a materializer for a given step output. Parameters: Name Type Description Default output_name Optional[str] Optional name of the output for which to get the materializer. If no name is given and the step only has a single output, the materializer of this output will be returned. If the step has multiple outputs, an exception will be raised. None custom_materializer_class Optional[Type[BaseMaterializer]] If given, this BaseMaterializer subclass will be initialized with the output artifact instead of the materializer that was registered for this step output. None Returns: Type Description BaseMaterializer A materializer initialized with the output artifact for the given output. Exceptions: Type Description StepContextError If the step has no outputs, no output for the given output_name or if no output_name was given but the step has multiple outputs. Source code in zenml/steps/step_context.py def get_output_materializer ( self , output_name : Optional [ str ] = None , custom_materializer_class : Optional [ Type [ \"BaseMaterializer\" ]] = None , ) -> \"BaseMaterializer\" : \"\"\"Returns a materializer for a given step output. Args: output_name: Optional name of the output for which to get the materializer. If no name is given and the step only has a single output, the materializer of this output will be returned. If the step has multiple outputs, an exception will be raised. custom_materializer_class: If given, this `BaseMaterializer` subclass will be initialized with the output artifact instead of the materializer that was registered for this step output. Returns: A materializer initialized with the output artifact for the given output. Raises: StepContextError: If the step has no outputs, no output for the given `output_name` or if no `output_name` was given but the step has multiple outputs. \"\"\" materializer_class , artifact = self . _get_output ( output_name ) # use custom materializer class if provided or fallback to default # materializer for output materializer_class = custom_materializer_class or materializer_class return materializer_class ( artifact ) StepContextOutput ( tuple ) Tuple containing materializer class and artifact for a step output. Source code in zenml/steps/step_context.py class StepContextOutput ( NamedTuple ): \"\"\"Tuple containing materializer class and artifact for a step output.\"\"\" materializer_class : Type [ \"BaseMaterializer\" ] artifact : \"BaseArtifact\" __getnewargs__ ( self ) special Return self as a plain tuple. Used by copy and pickle. Source code in zenml/steps/step_context.py def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self ) __new__ ( _cls , materializer_class , artifact ) special staticmethod Create new instance of StepContextOutput(materializer_class, artifact) __repr__ ( self ) special Return a nicely formatted representation string Source code in zenml/steps/step_context.py def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self step_decorator step ( _func = None , * , name = None , enable_cache = None , output_types = None ) Outer decorator function for the creation of a ZenML step In order to be able to work with parameters such as name , it features a nested decorator structure. Parameters: Name Type Description Default _func Optional[~F] The decorated function. None name Optional[str] The name of the step. If left empty, the name of the decorated function will be used as a fallback. None enable_cache Optional[bool] Specify whether caching is enabled for this step. If no value is passed, caching is enabled by default unless the step requires a StepContext (see :class: zenml.steps.step_context.StepContext for more information). None output_types Optional[Dict[str, Type[BaseArtifact]]] A dictionary which sets different outputs to non-default artifact types None Returns: Type Description Union[Type[zenml.steps.base_step.BaseStep], Callable[[~F], Type[zenml.steps.base_step.BaseStep]]] the inner decorator which creates the step class based on the ZenML BaseStep Source code in zenml/steps/step_decorator.py def step ( _func : Optional [ F ] = None , * , name : Optional [ str ] = None , enable_cache : Optional [ bool ] = None , output_types : Optional [ Dict [ str , Type [ \"BaseArtifact\" ]]] = None , ) -> Union [ Type [ BaseStep ], Callable [[ F ], Type [ BaseStep ]]]: \"\"\"Outer decorator function for the creation of a ZenML step In order to be able to work with parameters such as `name`, it features a nested decorator structure. Args: _func: The decorated function. name: The name of the step. If left empty, the name of the decorated function will be used as a fallback. enable_cache: Specify whether caching is enabled for this step. If no value is passed, caching is enabled by default unless the step requires a `StepContext` (see :class:`zenml.steps.step_context.StepContext` for more information). output_types: A dictionary which sets different outputs to non-default artifact types Returns: the inner decorator which creates the step class based on the ZenML BaseStep \"\"\" def inner_decorator ( func : F ) -> Type [ BaseStep ]: \"\"\"Inner decorator function for the creation of a ZenML Step Args: func: types.FunctionType, this function will be used as the \"process\" method of the generated Step Returns: The class of a newly generated ZenML Step. \"\"\" step_name = name or func . __name__ output_spec = output_types or {} return type ( # noqa step_name , ( BaseStep ,), { STEP_INNER_FUNC_NAME : staticmethod ( func ), INSTANCE_CONFIGURATION : { PARAM_ENABLE_CACHE : enable_cache , PARAM_CREATED_BY_FUNCTIONAL_API : True , }, OUTPUT_SPEC : output_spec , \"__module__\" : func . __module__ , }, ) if _func is None : return inner_decorator else : return inner_decorator ( _func ) step_interfaces special base_analyzer_step BaseAnalyzerConfig ( BaseStepConfig ) pydantic-model Base class for analyzer step configurations Source code in zenml/steps/step_interfaces/base_analyzer_step.py class BaseAnalyzerConfig ( BaseStepConfig ): \"\"\"Base class for analyzer step configurations\"\"\" BaseAnalyzerStep ( BaseStep ) Base step implementation for any analyzer step implementation on ZenML Source code in zenml/steps/step_interfaces/base_analyzer_step.py class BaseAnalyzerStep ( BaseStep ): \"\"\"Base step implementation for any analyzer step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , config : BaseAnalyzerConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] statistics = StatisticsArtifact , schema = SchemaArtifact ): \"\"\"Base entrypoint for any analyzer implementation\"\"\" CONFIG_CLASS ( BaseStepConfig ) pydantic-model Base class for analyzer step configurations Source code in zenml/steps/step_interfaces/base_analyzer_step.py class BaseAnalyzerConfig ( BaseStepConfig ): \"\"\"Base class for analyzer step configurations\"\"\" entrypoint ( self , dataset , config , context ) Base entrypoint for any analyzer implementation Source code in zenml/steps/step_interfaces/base_analyzer_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , config : BaseAnalyzerConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] statistics = StatisticsArtifact , schema = SchemaArtifact ): \"\"\"Base entrypoint for any analyzer implementation\"\"\" base_datasource_step BaseDatasourceConfig ( BaseStepConfig ) pydantic-model Base class for datasource configs to inherit from Source code in zenml/steps/step_interfaces/base_datasource_step.py class BaseDatasourceConfig ( BaseStepConfig ): \"\"\"Base class for datasource configs to inherit from\"\"\" BaseDatasourceStep ( BaseStep ) Base step implementation for any datasource step implementation on ZenML Source code in zenml/steps/step_interfaces/base_datasource_step.py class BaseDatasourceStep ( BaseStep ): \"\"\"Base step implementation for any datasource step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , config : BaseDatasourceConfig , context : StepContext , ) -> DataArtifact : \"\"\"Base entrypoint for any datasource implementation\"\"\" CONFIG_CLASS ( BaseStepConfig ) pydantic-model Base class for datasource configs to inherit from Source code in zenml/steps/step_interfaces/base_datasource_step.py class BaseDatasourceConfig ( BaseStepConfig ): \"\"\"Base class for datasource configs to inherit from\"\"\" entrypoint ( self , config , context ) Base entrypoint for any datasource implementation Source code in zenml/steps/step_interfaces/base_datasource_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , config : BaseDatasourceConfig , context : StepContext , ) -> DataArtifact : \"\"\"Base entrypoint for any datasource implementation\"\"\" base_drift_detection_step BaseDriftDetectionConfig ( BaseStepConfig ) pydantic-model Base class for drift detection step configurations Source code in zenml/steps/step_interfaces/base_drift_detection_step.py class BaseDriftDetectionConfig ( BaseStepConfig ): \"\"\"Base class for drift detection step configurations\"\"\" BaseDriftDetectionStep ( BaseStep ) Base step implementation for any drift detection step implementation on ZenML Source code in zenml/steps/step_interfaces/base_drift_detection_step.py class BaseDriftDetectionStep ( BaseStep ): \"\"\"Base step implementation for any drift detection step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , reference_dataset : DataArtifact , comparison_dataset : DataArtifact , config : BaseDriftDetectionConfig , context : StepContext , ) -> Any : \"\"\"Base entrypoint for any drift detection implementation\"\"\" CONFIG_CLASS ( BaseStepConfig ) pydantic-model Base class for drift detection step configurations Source code in zenml/steps/step_interfaces/base_drift_detection_step.py class BaseDriftDetectionConfig ( BaseStepConfig ): \"\"\"Base class for drift detection step configurations\"\"\" entrypoint ( self , reference_dataset , comparison_dataset , config , context ) Base entrypoint for any drift detection implementation Source code in zenml/steps/step_interfaces/base_drift_detection_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , reference_dataset : DataArtifact , comparison_dataset : DataArtifact , config : BaseDriftDetectionConfig , context : StepContext , ) -> Any : \"\"\"Base entrypoint for any drift detection implementation\"\"\" base_evaluator_step BaseEvaluatorConfig ( BaseStepConfig ) pydantic-model Base class for evaluator step configurations Source code in zenml/steps/step_interfaces/base_evaluator_step.py class BaseEvaluatorConfig ( BaseStepConfig ): \"\"\"Base class for evaluator step configurations\"\"\" BaseEvaluatorStep ( BaseStep ) Base step implementation for any evaluator step implementation on ZenML Source code in zenml/steps/step_interfaces/base_evaluator_step.py class BaseEvaluatorStep ( BaseStep ): \"\"\"Base step implementation for any evaluator step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , model : ModelArtifact , config : BaseEvaluatorConfig , context : StepContext , ) -> DataArtifact : \"\"\"Base entrypoint for any evaluator implementation\"\"\" CONFIG_CLASS ( BaseStepConfig ) pydantic-model Base class for evaluator step configurations Source code in zenml/steps/step_interfaces/base_evaluator_step.py class BaseEvaluatorConfig ( BaseStepConfig ): \"\"\"Base class for evaluator step configurations\"\"\" entrypoint ( self , dataset , model , config , context ) Base entrypoint for any evaluator implementation Source code in zenml/steps/step_interfaces/base_evaluator_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , model : ModelArtifact , config : BaseEvaluatorConfig , context : StepContext , ) -> DataArtifact : \"\"\"Base entrypoint for any evaluator implementation\"\"\" base_preprocessor_step BasePreprocessorConfig ( BaseStepConfig ) pydantic-model Base class for Preprocessor step configurations Source code in zenml/steps/step_interfaces/base_preprocessor_step.py class BasePreprocessorConfig ( BaseStepConfig ): \"\"\"Base class for Preprocessor step configurations\"\"\" BasePreprocessorStep ( BaseStep ) Base step implementation for any Preprocessor step implementation on ZenML Source code in zenml/steps/step_interfaces/base_preprocessor_step.py class BasePreprocessorStep ( BaseStep ): \"\"\"Base step implementation for any Preprocessor step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , train_dataset : DataArtifact , test_dataset : DataArtifact , validation_dataset : DataArtifact , statistics : StatisticsArtifact , schema : SchemaArtifact , config : BasePreprocessorConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] train_transformed = DataArtifact , test_transformed = DataArtifact , validation_transformed = DataArtifact , ): \"\"\"Base entrypoint for any Preprocessor implementation\"\"\" CONFIG_CLASS ( BaseStepConfig ) pydantic-model Base class for Preprocessor step configurations Source code in zenml/steps/step_interfaces/base_preprocessor_step.py class BasePreprocessorConfig ( BaseStepConfig ): \"\"\"Base class for Preprocessor step configurations\"\"\" entrypoint ( self , train_dataset , test_dataset , validation_dataset , statistics , schema , config , context ) Base entrypoint for any Preprocessor implementation Source code in zenml/steps/step_interfaces/base_preprocessor_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , train_dataset : DataArtifact , test_dataset : DataArtifact , validation_dataset : DataArtifact , statistics : StatisticsArtifact , schema : SchemaArtifact , config : BasePreprocessorConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] train_transformed = DataArtifact , test_transformed = DataArtifact , validation_transformed = DataArtifact , ): \"\"\"Base entrypoint for any Preprocessor implementation\"\"\" base_split_step BaseSplitStep ( BaseStep ) Base step implementation for any split step implementation on ZenML Source code in zenml/steps/step_interfaces/base_split_step.py class BaseSplitStep ( BaseStep ): \"\"\"Base step implementation for any split step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , config : BaseSplitStepConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] train = DataArtifact , test = DataArtifact , validation = DataArtifact ): \"\"\"Entrypoint for a function for the split steps to run\"\"\" CONFIG_CLASS ( BaseStepConfig ) pydantic-model Base class for split configs to inherit from Source code in zenml/steps/step_interfaces/base_split_step.py class BaseSplitStepConfig ( BaseStepConfig ): \"\"\"Base class for split configs to inherit from\"\"\" entrypoint ( self , dataset , config , context ) Entrypoint for a function for the split steps to run Source code in zenml/steps/step_interfaces/base_split_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , config : BaseSplitStepConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] train = DataArtifact , test = DataArtifact , validation = DataArtifact ): \"\"\"Entrypoint for a function for the split steps to run\"\"\" BaseSplitStepConfig ( BaseStepConfig ) pydantic-model Base class for split configs to inherit from Source code in zenml/steps/step_interfaces/base_split_step.py class BaseSplitStepConfig ( BaseStepConfig ): \"\"\"Base class for split configs to inherit from\"\"\" base_trainer_step BaseTrainerConfig ( BaseStepConfig ) pydantic-model Base class for Trainer step configurations Source code in zenml/steps/step_interfaces/base_trainer_step.py class BaseTrainerConfig ( BaseStepConfig ): \"\"\"Base class for Trainer step configurations\"\"\" BaseTrainerStep ( BaseStep ) Base step implementation for any Trainer step implementation on ZenML Source code in zenml/steps/step_interfaces/base_trainer_step.py class BaseTrainerStep ( BaseStep ): \"\"\"Base step implementation for any Trainer step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , train_dataset : DataArtifact , validation_dataset : DataArtifact , config : BaseTrainerConfig , context : StepContext , ) -> ModelArtifact : \"\"\"Base entrypoint for any Trainer implementation\"\"\" CONFIG_CLASS ( BaseStepConfig ) pydantic-model Base class for Trainer step configurations Source code in zenml/steps/step_interfaces/base_trainer_step.py class BaseTrainerConfig ( BaseStepConfig ): \"\"\"Base class for Trainer step configurations\"\"\" entrypoint ( self , train_dataset , validation_dataset , config , context ) Base entrypoint for any Trainer implementation Source code in zenml/steps/step_interfaces/base_trainer_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , train_dataset : DataArtifact , validation_dataset : DataArtifact , config : BaseTrainerConfig , context : StepContext , ) -> ModelArtifact : \"\"\"Base entrypoint for any Trainer implementation\"\"\" step_output Output A named tuple with a default name that cannot be overridden. Source code in zenml/steps/step_output.py class Output ( object ): \"\"\"A named tuple with a default name that cannot be overridden.\"\"\" def __init__ ( self , ** kwargs : Type [ Any ]): # TODO [ENG-161]: do we even need the named tuple here or is # a list of tuples (name, Type) sufficient? self . outputs = NamedTuple ( \"ZenOutput\" , ** kwargs ) # type: ignore[misc] def items ( self ) -> Iterator [ Tuple [ str , Type [ Any ]]]: \"\"\"Yields a tuple of type (output_name, output_type).\"\"\" yield from self . outputs . __annotations__ . items () items ( self ) Yields a tuple of type (output_name, output_type). Source code in zenml/steps/step_output.py def items ( self ) -> Iterator [ Tuple [ str , Type [ Any ]]]: \"\"\"Yields a tuple of type (output_name, output_type).\"\"\" yield from self . outputs . __annotations__ . items () utils The collection of utility functions/classes are inspired by their original implementation of the Tensorflow Extended team, which can be found here: https://github.com/tensorflow/tfx/blob/master/tfx/dsl/component/experimental /decorators.py This version is heavily adjusted to work with the Pipeline-Step paradigm which is proposed by ZenML. do_types_match ( type_a , type_b ) Check whether type_a and type_b match. Parameters: Name Type Description Default type_a Type[Any] First Type to check. required type_b Type[Any] Second Type to check. required Returns: Type Description bool True if types match, otherwise False. Source code in zenml/steps/utils.py def do_types_match ( type_a : Type [ Any ], type_b : Type [ Any ]) -> bool : \"\"\"Check whether type_a and type_b match. Args: type_a: First Type to check. type_b: Second Type to check. Returns: True if types match, otherwise False. \"\"\" # TODO [ENG-158]: Check more complicated cases where type_a can be a sub-type # of type_b return type_a == type_b generate_component_class ( step_name , step_module , input_spec , output_spec , execution_parameter_names , step_function , materializers ) Generates a TFX component class for a ZenML step. Parameters: Name Type Description Default step_name str Name of the step for which the component will be created. required step_module str Module in which the step class is defined. required input_spec Dict[str, Type[zenml.artifacts.base_artifact.BaseArtifact]] Input artifacts of the step. required output_spec Dict[str, Type[zenml.artifacts.base_artifact.BaseArtifact]] Output artifacts of the step required execution_parameter_names Set[str] Execution parameter names of the step. required step_function Callable[..., Any] The actual function to execute when running the step. required materializers Dict[str, Type[zenml.materializers.base_materializer.BaseMaterializer]] Materializer classes for all outputs of the step. required Returns: Type Description Type[_ZenMLSimpleComponent] A TFX component class. Source code in zenml/steps/utils.py def generate_component_class ( step_name : str , step_module : str , input_spec : Dict [ str , Type [ BaseArtifact ]], output_spec : Dict [ str , Type [ BaseArtifact ]], execution_parameter_names : Set [ str ], step_function : Callable [ ... , Any ], materializers : Dict [ str , Type [ BaseMaterializer ]], ) -> Type [ \"_ZenMLSimpleComponent\" ]: \"\"\"Generates a TFX component class for a ZenML step. Args: step_name: Name of the step for which the component will be created. step_module: Module in which the step class is defined. input_spec: Input artifacts of the step. output_spec: Output artifacts of the step execution_parameter_names: Execution parameter names of the step. step_function: The actual function to execute when running the step. materializers: Materializer classes for all outputs of the step. Returns: A TFX component class. \"\"\" component_spec_class = generate_component_spec_class ( step_name = step_name , input_spec = input_spec , output_spec = output_spec , execution_parameter_names = execution_parameter_names , ) # Create executor class executor_class_name = f \" { step_name } _Executor\" executor_class = type ( executor_class_name , ( _FunctionExecutor ,), { \"_FUNCTION\" : staticmethod ( step_function ), \"__module__\" : step_module , \"materializers\" : materializers , PARAM_STEP_NAME : step_name , }, ) # Add the executor class to the module in which the step was defined module = sys . modules [ step_module ] setattr ( module , executor_class_name , executor_class ) return type ( step_name , ( _ZenMLSimpleComponent ,), { \"SPEC_CLASS\" : component_spec_class , \"EXECUTOR_SPEC\" : ExecutorClassSpec ( executor_class = executor_class ), \"__module__\" : step_module , }, ) generate_component_spec_class ( step_name , input_spec , output_spec , execution_parameter_names ) Generates a TFX component spec class for a ZenML step. Parameters: Name Type Description Default step_name str Name of the step for which the component will be created. required input_spec Dict[str, Type[zenml.artifacts.base_artifact.BaseArtifact]] Input artifacts of the step. required output_spec Dict[str, Type[zenml.artifacts.base_artifact.BaseArtifact]] Output artifacts of the step required execution_parameter_names Set[str] Execution parameter names of the step. required Returns: Type Description Type[tfx.types.component_spec.ComponentSpec] A TFX component spec class. Source code in zenml/steps/utils.py def generate_component_spec_class ( step_name : str , input_spec : Dict [ str , Type [ BaseArtifact ]], output_spec : Dict [ str , Type [ BaseArtifact ]], execution_parameter_names : Set [ str ], ) -> Type [ component_spec . ComponentSpec ]: \"\"\"Generates a TFX component spec class for a ZenML step. Args: step_name: Name of the step for which the component will be created. input_spec: Input artifacts of the step. output_spec: Output artifacts of the step execution_parameter_names: Execution parameter names of the step. Returns: A TFX component spec class. \"\"\" inputs = { key : component_spec . ChannelParameter ( type = artifact_type ) for key , artifact_type in input_spec . items () } outputs = { key : component_spec . ChannelParameter ( type = artifact_type ) for key , artifact_type in output_spec . items () } parameters = { key : component_spec . ExecutionParameter ( type = str ) # type: ignore[no-untyped-call] # noqa for key in execution_parameter_names } return type ( f \" { step_name } _Spec\" , ( component_spec . ComponentSpec ,), { \"INPUTS\" : inputs , \"OUTPUTS\" : outputs , \"PARAMETERS\" : parameters , }, )","title":"Steps"},{"location":"api_docs/steps/#steps","text":"","title":"Steps"},{"location":"api_docs/steps/#zenml.steps","text":"A step is a single piece or stage of a ZenML pipeline. Think of each step as being one of the nodes of a Directed Acyclic Graph (or DAG). Steps are responsible for one aspect of processing or interacting with the data / artifacts in the pipeline. ZenML currently implements a basic step interface, but there will be other more customized interfaces (layered in a hierarchy) for specialized implementations. Conceptually, a Step is a discrete and independent part of a pipeline that is responsible for one particular aspect of data manipulation inside a ZenML pipeline. Steps can be subclassed from the BaseStep class, or used via our @step decorator.","title":"steps"},{"location":"api_docs/steps/#zenml.steps.base_step","text":"","title":"base_step"},{"location":"api_docs/steps/#zenml.steps.base_step.BaseStep","text":"Abstract base class for all ZenML steps. Attributes: Name Type Description name The name of this step. pipeline_parameter_name Optional[str] The name of the pipeline parameter for which this step was passed as an argument. enable_cache A boolean indicating if caching is enabled for this step. requires_context A boolean indicating if this step requires a StepContext object during execution. Source code in zenml/steps/base_step.py class BaseStep ( metaclass = BaseStepMeta ): \"\"\"Abstract base class for all ZenML steps. Attributes: name: The name of this step. pipeline_parameter_name: The name of the pipeline parameter for which this step was passed as an argument. enable_cache: A boolean indicating if caching is enabled for this step. requires_context: A boolean indicating if this step requires a `StepContext` object during execution. \"\"\" # TODO [ENG-156]: Ensure these are ordered INPUT_SIGNATURE : ClassVar [ Dict [ str , Type [ Any ]]] = None # type: ignore[assignment] # noqa OUTPUT_SIGNATURE : ClassVar [ Dict [ str , Type [ Any ]]] = None # type: ignore[assignment] # noqa CONFIG_PARAMETER_NAME : ClassVar [ Optional [ str ]] = None CONFIG_CLASS : ClassVar [ Optional [ Type [ BaseStepConfig ]]] = None CONTEXT_PARAMETER_NAME : ClassVar [ Optional [ str ]] = None PARAM_SPEC : Dict [ str , Any ] = {} INPUT_SPEC : Dict [ str , Type [ BaseArtifact ]] = {} OUTPUT_SPEC : Dict [ str , Type [ BaseArtifact ]] = {} INSTANCE_CONFIGURATION : Dict [ str , Any ] = {} def __init__ ( self , * args : Any , ** kwargs : Any ) -> None : self . name = self . __class__ . __name__ self . pipeline_parameter_name : Optional [ str ] = None kwargs . update ( getattr ( self , INSTANCE_CONFIGURATION )) self . requires_context = bool ( self . CONTEXT_PARAMETER_NAME ) self . _created_by_functional_api = kwargs . pop ( PARAM_CREATED_BY_FUNCTIONAL_API , False ) enable_cache = kwargs . pop ( PARAM_ENABLE_CACHE , None ) if enable_cache is None : if self . requires_context : # Using the StepContext inside a step provides access to # external resources which might influence the step execution. # We therefore disable caching unless it is explicitly enabled enable_cache = False logger . debug ( \"Step ' %s ': Step context required and caching not \" \"explicitly enabled.\" , self . name , ) else : # Default to cache enabled if not explicitly set enable_cache = True logger . debug ( \"Step ' %s ': Caching %s .\" , self . name , \"enabled\" if enable_cache else \"disabled\" , ) self . enable_cache = enable_cache self . _explicit_materializers : Dict [ str , Type [ BaseMaterializer ]] = {} self . _component : Optional [ _ZenMLSimpleComponent ] = None self . _verify_init_arguments ( * args , ** kwargs ) self . _verify_output_spec () @abstractmethod def entrypoint ( self , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Abstract method for core step logic.\"\"\" def get_materializers ( self , ensure_complete : bool = False ) -> Dict [ str , Type [ BaseMaterializer ]]: \"\"\"Returns available materializers for the outputs of this step. Args: ensure_complete: If set to `True`, this method will raise a `StepInterfaceError` if no materializer can be found for an output. Returns: A dictionary mapping output names to `BaseMaterializer` subclasses. If no explicit materializer was set using `step.with_return_materializers(...)`, this checks the default materializer registry to find a materializer for the type of the output. If no materializer is registered, the output of this method will not contain an entry for this output. Raises: StepInterfaceError: (Only if `ensure_complete` is set to `True`) If an output does not have an explicit materializer assigned to it and there is no default materializer registered for the output type. \"\"\" materializers = self . _explicit_materializers for output_name , output_type in self . OUTPUT_SIGNATURE . items (): if output_name in materializers : # Materializer for this output was set explicitly pass elif default_materializer_registry . is_registered ( output_type ): materializer = default_materializer_registry [ output_type ] materializers [ output_name ] = materializer else : if ensure_complete : raise StepInterfaceError ( f \"Unable to find materializer for output \" f \"' { output_name } ' of type ` { output_type } ` in step \" f \"' { self . name } '. Please make sure to either \" f \"explicitly set a materializer for step outputs \" f \"using `step.with_return_materializers(...)` or \" f \"registering a default materializer for specific \" f \"types by subclassing `BaseMaterializer` and setting \" f \"its `ASSOCIATED_TYPES` class variable.\" ) return materializers @property def _internal_execution_parameters ( self ) -> Dict [ str , Any ]: \"\"\"ZenML internal execution parameters for this step.\"\"\" parameters = { PARAM_PIPELINE_PARAMETER_NAME : self . pipeline_parameter_name } if self . enable_cache : # Caching is enabled so we compute a hash of the step function code # and materializers to catch changes in the step behavior def _get_hashed_source ( value : Any ) -> str : \"\"\"Returns a hash of the objects source code.\"\"\" source_code = inspect . getsource ( value ) return hashlib . sha256 ( source_code . encode ( \"utf-8\" )) . hexdigest () # If the step was defined using the functional api, only track # changes to the entrypoint function. Otherwise track changes to # the entire step class. source_object = ( self . entrypoint if self . _created_by_functional_api else self . __class__ ) parameters [ \"step_source\" ] = _get_hashed_source ( source_object ) for name , materializer in self . get_materializers () . items (): key = f \" { name } _materializer_source\" parameters [ key ] = _get_hashed_source ( materializer ) else : # Add a random string to the execution properties to disable caching random_string = f \" { random . getrandbits ( 128 ) : 032x } \" parameters [ \"disable_cache\" ] = random_string return { INTERNAL_EXECUTION_PARAMETER_PREFIX + key : value for key , value in parameters . items () } def _verify_init_arguments ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Verifies the initialization args and kwargs of this step. This method makes sure that there is only a config object passed at initialization and that it was passed using the correct name and type specified in the step declaration. If the correct config object was found, additionally saves the config parameters to `self.PARAM_SPEC`. Args: *args: The args passed to the init method of this step. **kwargs: The kwargs passed to the init method of this step. Raises: StepInterfaceError: If there are too many arguments or arguments with a wrong name/type. \"\"\" maximum_arg_count = 1 if self . CONFIG_CLASS else 0 arg_count = len ( args ) + len ( kwargs ) if arg_count > maximum_arg_count : raise StepInterfaceError ( f \"Too many arguments ( { arg_count } , expected: \" f \" { maximum_arg_count } ) passed when creating a \" f \"' { self . name } ' step.\" ) if self . CONFIG_PARAMETER_NAME and self . CONFIG_CLASS : if args : config = args [ 0 ] elif kwargs : key , config = kwargs . popitem () if key != self . CONFIG_PARAMETER_NAME : raise StepInterfaceError ( f \"Unknown keyword argument ' { key } ' when creating a \" f \"' { self . name } ' step, only expected a single \" f \"argument with key ' { self . CONFIG_PARAMETER_NAME } '.\" ) else : # This step requires configuration parameters but no config # object was passed as an argument. The parameters might be # set via default values in the config class or in a # configuration file, so we continue for now and verify # that all parameters are set before running the step return if not isinstance ( config , self . CONFIG_CLASS ): raise StepInterfaceError ( f \"` { config } ` object passed when creating a \" f \"' { self . name } ' step is not a \" f \"` { self . CONFIG_CLASS . __name__ } ` instance.\" ) self . PARAM_SPEC = config . dict () def _verify_output_spec ( self ) -> None : \"\"\"Verifies the explicitly set output artifact types of this step. Raises: StepInterfaceError: If an output artifact type is specified for a non-existent step output or the artifact type is not allowed for the corresponding output type. \"\"\" for output_name , artifact_type in self . OUTPUT_SPEC . items (): if output_name not in self . OUTPUT_SIGNATURE : raise StepInterfaceError ( f \"Found explicit artifact type for unrecognized output \" f \"' { output_name } ' in step ' { self . name } '. Output \" f \"artifact types can only be specified for the outputs \" f \"of this step: { set ( self . OUTPUT_SIGNATURE ) } .\" ) if not issubclass ( artifact_type , BaseArtifact ): raise StepInterfaceError ( f \"Invalid artifact type ( { artifact_type } ) for output \" f \"' { output_name } ' of step ' { self . name } '. Only \" f \"`BaseArtifact` subclasses are allowed as artifact types.\" ) output_type = self . OUTPUT_SIGNATURE [ output_name ] allowed_artifact_types = set ( type_registry . get_artifact_type ( output_type ) ) if artifact_type not in allowed_artifact_types : raise StepInterfaceError ( f \"Artifact type ` { artifact_type } ` for output \" f \"' { output_name } ' of step ' { self . name } ' is not an \" f \"allowed artifact type for the defined output type \" f \"` { output_type } `. Allowed artifact types: \" f \" { allowed_artifact_types } . If you want to extend the \" f \"allowed artifact types, implement a custom \" f \"`BaseMaterializer` subclass and set its \" f \"`ASSOCIATED_ARTIFACT_TYPES` and `ASSOCIATED_TYPES` \" f \"accordingly.\" ) def _update_and_verify_parameter_spec ( self ) -> None : \"\"\"Verifies and prepares the config parameters for running this step. When the step requires config parameters, this method: - checks if config parameters were set via a config object or file - tries to set missing config parameters from default values of the config class Raises: MissingStepParameterError: If no value could be found for one or more config parameters. StepInterfaceError: If a config parameter value couldn't be serialized to json. \"\"\" if self . CONFIG_CLASS : # we need to store a value for all config keys inside the # metadata store to make sure caching works as expected missing_keys = [] for name , field in self . CONFIG_CLASS . __fields__ . items (): if name in self . PARAM_SPEC : # a value for this parameter has been set already continue if field . required : # this field has no default value set and therefore needs # to be passed via an initialized config object missing_keys . append ( name ) else : # use default value from the pydantic config class self . PARAM_SPEC [ name ] = field . default if missing_keys : raise MissingStepParameterError ( self . name , missing_keys , self . CONFIG_CLASS ) def _prepare_input_artifacts ( self , * artifacts : Channel , ** kw_artifacts : Channel ) -> Dict [ str , Channel ]: \"\"\"Verifies and prepares the input artifacts for running this step. Args: *artifacts: Positional input artifacts passed to the __call__ method. **kw_artifacts: Keyword input artifacts passed to the __call__ method. Returns: Dictionary containing both the positional and keyword input artifacts. Raises: StepInterfaceError: If there are too many or too few artifacts. \"\"\" input_artifact_keys = list ( self . INPUT_SIGNATURE . keys ()) if len ( artifacts ) > len ( input_artifact_keys ): raise StepInterfaceError ( f \"Too many input artifacts for step ' { self . name } '. \" f \"This step expects { len ( input_artifact_keys ) } artifact(s) \" f \"but got { len ( artifacts ) + len ( kw_artifacts ) } .\" ) combined_artifacts = {} for i , artifact in enumerate ( artifacts ): if not isinstance ( artifact , Channel ): raise StepInterfaceError ( f \"Wrong argument type (` { type ( artifact ) } `) for positional \" f \"argument { i } of step ' { self . name } '. Only outputs \" f \"from previous steps can be used as arguments when \" f \"connecting steps.\" ) key = input_artifact_keys [ i ] combined_artifacts [ key ] = artifact for key , artifact in kw_artifacts . items (): if key in combined_artifacts : # an artifact for this key was already set by # the positional input artifacts raise StepInterfaceError ( f \"Unexpected keyword argument ' { key } ' for step \" f \"' { self . name } '. An artifact for this key was \" f \"already passed as a positional argument.\" ) if not isinstance ( artifact , Channel ): raise StepInterfaceError ( f \"Wrong argument type (` { type ( artifact ) } `) for argument \" f \"' { key } ' of step ' { self . name } '. Only outputs from \" f \"previous steps can be used as arguments when \" f \"connecting steps.\" ) combined_artifacts [ key ] = artifact # check if there are any missing or unexpected artifacts expected_artifacts = set ( self . INPUT_SIGNATURE . keys ()) actual_artifacts = set ( combined_artifacts . keys ()) missing_artifacts = expected_artifacts - actual_artifacts unexpected_artifacts = actual_artifacts - expected_artifacts if missing_artifacts : raise StepInterfaceError ( f \"Missing input artifact(s) for step \" f \"' { self . name } ': { missing_artifacts } .\" ) if unexpected_artifacts : raise StepInterfaceError ( f \"Unexpected input artifact(s) for step \" f \"' { self . name } ': { unexpected_artifacts } . This step \" f \"only requires the following artifacts: { expected_artifacts } .\" ) return combined_artifacts def __call__ ( self , * artifacts : Channel , ** kw_artifacts : Channel ) -> Union [ Channel , List [ Channel ]]: \"\"\"Generates a component when called.\"\"\" # TODO [ENG-157]: replaces Channels with ZenML class (BaseArtifact?) self . _update_and_verify_parameter_spec () # Prepare the input artifacts and spec input_artifacts = self . _prepare_input_artifacts ( * artifacts , ** kw_artifacts ) self . INPUT_SPEC = { arg_name : artifact_type . type # type:ignore[misc] for arg_name , artifact_type in input_artifacts . items () } # make sure we have registered materializers for each output materializers = self . get_materializers ( ensure_complete = True ) # Prepare the output artifacts and spec for key , value in self . OUTPUT_SIGNATURE . items (): verified_types = type_registry . get_artifact_type ( value ) if key not in self . OUTPUT_SPEC : self . OUTPUT_SPEC [ key ] = verified_types [ 0 ] execution_parameters = { ** self . PARAM_SPEC , ** self . _internal_execution_parameters , } # Convert execution parameter values to strings try : execution_parameters = { k : json . dumps ( v ) for k , v in execution_parameters . items () } except TypeError as e : raise StepInterfaceError ( f \"Failed to serialize execution parameters for step \" f \"' { self . name } '. Please make sure to only use \" f \"json serializable parameter values.\" ) from e source_fn = getattr ( self , STEP_INNER_FUNC_NAME ) component_class = generate_component_class ( step_name = self . name , step_module = self . __module__ , input_spec = self . INPUT_SPEC , output_spec = self . OUTPUT_SPEC , execution_parameter_names = set ( execution_parameters ), step_function = source_fn , materializers = materializers , ) self . _component = component_class ( ** input_artifacts , ** execution_parameters ) # Resolve the returns in the right order. returns = [ self . component . outputs [ key ] for key in self . OUTPUT_SPEC ] # If its one return we just return the one channel not as a list if len ( returns ) == 1 : return returns [ 0 ] else : return returns @property def component ( self ) -> _ZenMLSimpleComponent : \"\"\"Returns a TFX component.\"\"\" if not self . _component : raise StepInterfaceError ( \"Trying to access the step component \" \"before creating it via calling the step.\" ) return self . _component def with_return_materializers ( self : T , materializers : Union [ Type [ BaseMaterializer ], Dict [ str , Type [ BaseMaterializer ]] ], ) -> T : \"\"\"Register materializers for step outputs. If a single materializer is passed, it will be used for all step outputs. Otherwise, the dictionary keys specify the output names for which the materializers will be used. Args: materializers: The materializers for the outputs of this step. Returns: The object that this method was called on. Raises: StepInterfaceError: If a materializer is not a `BaseMaterializer` subclass or a materializer for a non-existent output is given. \"\"\" def _is_materializer_class ( value : Any ) -> bool : \"\"\"Checks whether the given object is a `BaseMaterializer` subclass.\"\"\" is_class = isinstance ( value , type ) return is_class and issubclass ( value , BaseMaterializer ) if isinstance ( materializers , dict ): allowed_output_names = set ( self . OUTPUT_SIGNATURE ) for output_name , materializer in materializers . items (): if output_name not in allowed_output_names : raise StepInterfaceError ( f \"Got unexpected materializers for non-existent \" f \"output ' { output_name } ' in step ' { self . name } '. \" f \"Only materializers for the outputs \" f \" { allowed_output_names } of this step can\" f \" be registered.\" ) if not _is_materializer_class ( materializer ): raise StepInterfaceError ( f \"Got unexpected object ` { materializer } ` as \" f \"materializer for output ' { output_name } ' of step \" f \"' { self . name } '. Only `BaseMaterializer` \" f \"subclasses are allowed.\" ) self . _explicit_materializers [ output_name ] = materializer elif _is_materializer_class ( materializers ): # Set the materializer for all outputs of this step self . _explicit_materializers = { key : materializers for key in self . OUTPUT_SIGNATURE } else : raise StepInterfaceError ( f \"Got unexpected object ` { materializers } ` as output \" f \"materializer for step ' { self . name } '. Only \" f \"`BaseMaterializer` subclasses or dictionaries mapping \" f \"output names to `BaseMaterializer` subclasses are allowed \" f \"as input when specifying return materializers.\" ) return self","title":"BaseStep"},{"location":"api_docs/steps/#zenml.steps.base_step.BaseStep.component","text":"Returns a TFX component.","title":"component"},{"location":"api_docs/steps/#zenml.steps.base_step.BaseStep.__call__","text":"Generates a component when called. Source code in zenml/steps/base_step.py def __call__ ( self , * artifacts : Channel , ** kw_artifacts : Channel ) -> Union [ Channel , List [ Channel ]]: \"\"\"Generates a component when called.\"\"\" # TODO [ENG-157]: replaces Channels with ZenML class (BaseArtifact?) self . _update_and_verify_parameter_spec () # Prepare the input artifacts and spec input_artifacts = self . _prepare_input_artifacts ( * artifacts , ** kw_artifacts ) self . INPUT_SPEC = { arg_name : artifact_type . type # type:ignore[misc] for arg_name , artifact_type in input_artifacts . items () } # make sure we have registered materializers for each output materializers = self . get_materializers ( ensure_complete = True ) # Prepare the output artifacts and spec for key , value in self . OUTPUT_SIGNATURE . items (): verified_types = type_registry . get_artifact_type ( value ) if key not in self . OUTPUT_SPEC : self . OUTPUT_SPEC [ key ] = verified_types [ 0 ] execution_parameters = { ** self . PARAM_SPEC , ** self . _internal_execution_parameters , } # Convert execution parameter values to strings try : execution_parameters = { k : json . dumps ( v ) for k , v in execution_parameters . items () } except TypeError as e : raise StepInterfaceError ( f \"Failed to serialize execution parameters for step \" f \"' { self . name } '. Please make sure to only use \" f \"json serializable parameter values.\" ) from e source_fn = getattr ( self , STEP_INNER_FUNC_NAME ) component_class = generate_component_class ( step_name = self . name , step_module = self . __module__ , input_spec = self . INPUT_SPEC , output_spec = self . OUTPUT_SPEC , execution_parameter_names = set ( execution_parameters ), step_function = source_fn , materializers = materializers , ) self . _component = component_class ( ** input_artifacts , ** execution_parameters ) # Resolve the returns in the right order. returns = [ self . component . outputs [ key ] for key in self . OUTPUT_SPEC ] # If its one return we just return the one channel not as a list if len ( returns ) == 1 : return returns [ 0 ] else : return returns","title":"__call__()"},{"location":"api_docs/steps/#zenml.steps.base_step.BaseStep.entrypoint","text":"Abstract method for core step logic. Source code in zenml/steps/base_step.py @abstractmethod def entrypoint ( self , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Abstract method for core step logic.\"\"\"","title":"entrypoint()"},{"location":"api_docs/steps/#zenml.steps.base_step.BaseStep.get_materializers","text":"Returns available materializers for the outputs of this step. Parameters: Name Type Description Default ensure_complete bool If set to True , this method will raise a StepInterfaceError if no materializer can be found for an output. False Returns: Type Description Dict[str, Type[zenml.materializers.base_materializer.BaseMaterializer]] A dictionary mapping output names to BaseMaterializer subclasses. If no explicit materializer was set using step.with_return_materializers(...) , this checks the default materializer registry to find a materializer for the type of the output. If no materializer is registered, the output of this method will not contain an entry for this output. Exceptions: Type Description StepInterfaceError (Only if ensure_complete is set to True ) If an output does not have an explicit materializer assigned to it and there is no default materializer registered for the output type. Source code in zenml/steps/base_step.py def get_materializers ( self , ensure_complete : bool = False ) -> Dict [ str , Type [ BaseMaterializer ]]: \"\"\"Returns available materializers for the outputs of this step. Args: ensure_complete: If set to `True`, this method will raise a `StepInterfaceError` if no materializer can be found for an output. Returns: A dictionary mapping output names to `BaseMaterializer` subclasses. If no explicit materializer was set using `step.with_return_materializers(...)`, this checks the default materializer registry to find a materializer for the type of the output. If no materializer is registered, the output of this method will not contain an entry for this output. Raises: StepInterfaceError: (Only if `ensure_complete` is set to `True`) If an output does not have an explicit materializer assigned to it and there is no default materializer registered for the output type. \"\"\" materializers = self . _explicit_materializers for output_name , output_type in self . OUTPUT_SIGNATURE . items (): if output_name in materializers : # Materializer for this output was set explicitly pass elif default_materializer_registry . is_registered ( output_type ): materializer = default_materializer_registry [ output_type ] materializers [ output_name ] = materializer else : if ensure_complete : raise StepInterfaceError ( f \"Unable to find materializer for output \" f \"' { output_name } ' of type ` { output_type } ` in step \" f \"' { self . name } '. Please make sure to either \" f \"explicitly set a materializer for step outputs \" f \"using `step.with_return_materializers(...)` or \" f \"registering a default materializer for specific \" f \"types by subclassing `BaseMaterializer` and setting \" f \"its `ASSOCIATED_TYPES` class variable.\" ) return materializers","title":"get_materializers()"},{"location":"api_docs/steps/#zenml.steps.base_step.BaseStep.with_return_materializers","text":"Register materializers for step outputs. If a single materializer is passed, it will be used for all step outputs. Otherwise, the dictionary keys specify the output names for which the materializers will be used. Parameters: Name Type Description Default materializers Union[Type[zenml.materializers.base_materializer.BaseMaterializer], Dict[str, Type[zenml.materializers.base_materializer.BaseMaterializer]]] The materializers for the outputs of this step. required Returns: Type Description ~T The object that this method was called on. Exceptions: Type Description StepInterfaceError If a materializer is not a BaseMaterializer subclass or a materializer for a non-existent output is given. Source code in zenml/steps/base_step.py def with_return_materializers ( self : T , materializers : Union [ Type [ BaseMaterializer ], Dict [ str , Type [ BaseMaterializer ]] ], ) -> T : \"\"\"Register materializers for step outputs. If a single materializer is passed, it will be used for all step outputs. Otherwise, the dictionary keys specify the output names for which the materializers will be used. Args: materializers: The materializers for the outputs of this step. Returns: The object that this method was called on. Raises: StepInterfaceError: If a materializer is not a `BaseMaterializer` subclass or a materializer for a non-existent output is given. \"\"\" def _is_materializer_class ( value : Any ) -> bool : \"\"\"Checks whether the given object is a `BaseMaterializer` subclass.\"\"\" is_class = isinstance ( value , type ) return is_class and issubclass ( value , BaseMaterializer ) if isinstance ( materializers , dict ): allowed_output_names = set ( self . OUTPUT_SIGNATURE ) for output_name , materializer in materializers . items (): if output_name not in allowed_output_names : raise StepInterfaceError ( f \"Got unexpected materializers for non-existent \" f \"output ' { output_name } ' in step ' { self . name } '. \" f \"Only materializers for the outputs \" f \" { allowed_output_names } of this step can\" f \" be registered.\" ) if not _is_materializer_class ( materializer ): raise StepInterfaceError ( f \"Got unexpected object ` { materializer } ` as \" f \"materializer for output ' { output_name } ' of step \" f \"' { self . name } '. Only `BaseMaterializer` \" f \"subclasses are allowed.\" ) self . _explicit_materializers [ output_name ] = materializer elif _is_materializer_class ( materializers ): # Set the materializer for all outputs of this step self . _explicit_materializers = { key : materializers for key in self . OUTPUT_SIGNATURE } else : raise StepInterfaceError ( f \"Got unexpected object ` { materializers } ` as output \" f \"materializer for step ' { self . name } '. Only \" f \"`BaseMaterializer` subclasses or dictionaries mapping \" f \"output names to `BaseMaterializer` subclasses are allowed \" f \"as input when specifying return materializers.\" ) return self","title":"with_return_materializers()"},{"location":"api_docs/steps/#zenml.steps.base_step.BaseStepMeta","text":"Metaclass for BaseStep . Checks whether everything passed in: * Has a matching materializer. * Is a subclass of the Config class Source code in zenml/steps/base_step.py class BaseStepMeta ( type ): \"\"\"Metaclass for `BaseStep`. Checks whether everything passed in: * Has a matching materializer. * Is a subclass of the Config class \"\"\" def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BaseStepMeta\" : \"\"\"Set up a new class with a qualified spec.\"\"\" dct . setdefault ( \"PARAM_SPEC\" , {}) dct . setdefault ( \"INPUT_SPEC\" , {}) dct . setdefault ( \"OUTPUT_SPEC\" , {}) cls = cast ( Type [ \"BaseStep\" ], super () . __new__ ( mcs , name , bases , dct )) cls . INPUT_SIGNATURE = {} cls . OUTPUT_SIGNATURE = {} cls . CONFIG_PARAMETER_NAME = None cls . CONFIG_CLASS = None cls . CONTEXT_PARAMETER_NAME = None # Get the signature of the step function step_function_signature = inspect . getfullargspec ( getattr ( cls , STEP_INNER_FUNC_NAME ) ) if bases : # We're not creating the abstract `BaseStep` class # but a concrete implementation. Make sure the step function # signature does not contain variable *args or **kwargs variable_arguments = None if step_function_signature . varargs : variable_arguments = f \"* { step_function_signature . varargs } \" elif step_function_signature . varkw : variable_arguments = f \"** { step_function_signature . varkw } \" if variable_arguments : raise StepInterfaceError ( f \"Unable to create step ' { name } ' with variable arguments \" f \"' { variable_arguments } '. Please make sure your step \" f \"functions are defined with a fixed amount of arguments.\" ) step_function_args = ( step_function_signature . args + step_function_signature . kwonlyargs ) # Remove 'self' from the signature if it exists if step_function_args and step_function_args [ 0 ] == \"self\" : step_function_args . pop ( 0 ) # Verify the input arguments of the step function for arg in step_function_args : arg_type = step_function_signature . annotations . get ( arg , None ) if not arg_type : raise StepInterfaceError ( f \"Missing type annotation for argument ' { arg } ' when \" f \"trying to create step ' { name } '. Please make sure to \" f \"include type annotations for all your step inputs \" f \"and outputs.\" ) if issubclass ( arg_type , BaseStepConfig ): # Raise an error if we already found a config in the signature if cls . CONFIG_CLASS is not None : raise StepInterfaceError ( f \"Found multiple configuration arguments \" f \"(' { cls . CONFIG_PARAMETER_NAME } ' and ' { arg } ') when \" f \"trying to create step ' { name } '. Please make sure to \" f \"only have one `BaseStepConfig` subclass as input \" f \"argument for a step.\" ) cls . CONFIG_PARAMETER_NAME = arg cls . CONFIG_CLASS = arg_type elif issubclass ( arg_type , StepContext ): if cls . CONTEXT_PARAMETER_NAME is not None : raise StepInterfaceError ( f \"Found multiple context arguments \" f \"(' { cls . CONTEXT_PARAMETER_NAME } ' and ' { arg } ') when \" f \"trying to create step ' { name } '. Please make sure to \" f \"only have one `StepContext` as input \" f \"argument for a step.\" ) cls . CONTEXT_PARAMETER_NAME = arg else : # Can't do any check for existing materializers right now # as they might get be defined later, so we simply store the # argument name and type for later use. cls . INPUT_SIGNATURE . update ({ arg : arg_type }) # Parse the returns of the step function return_type = step_function_signature . annotations . get ( \"return\" , None ) if return_type is not None : if isinstance ( return_type , Output ): cls . OUTPUT_SIGNATURE = dict ( return_type . items ()) else : cls . OUTPUT_SIGNATURE [ SINGLE_RETURN_OUT_NAME ] = return_type # Raise an exception if input and output names of a step overlap as # tfx requires them to be unique # TODO [ENG-155]: Can we prefix inputs and outputs to avoid this # restriction? counter : Counter [ str ] = collections . Counter () counter . update ( list ( cls . INPUT_SIGNATURE )) counter . update ( list ( cls . OUTPUT_SIGNATURE )) if cls . CONFIG_CLASS : counter . update ( list ( cls . CONFIG_CLASS . __fields__ . keys ())) shared_keys = { k for k in counter . elements () if counter [ k ] > 1 } if shared_keys : raise StepInterfaceError ( f \"The following keys are overlapping in the input, output and \" f \"config parameter names of step ' { name } ': { shared_keys } . \" f \"Please make sure that your input, output and config \" f \"parameter names are unique.\" ) return cls","title":"BaseStepMeta"},{"location":"api_docs/steps/#zenml.steps.base_step.BaseStepMeta.__new__","text":"Set up a new class with a qualified spec. Source code in zenml/steps/base_step.py def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BaseStepMeta\" : \"\"\"Set up a new class with a qualified spec.\"\"\" dct . setdefault ( \"PARAM_SPEC\" , {}) dct . setdefault ( \"INPUT_SPEC\" , {}) dct . setdefault ( \"OUTPUT_SPEC\" , {}) cls = cast ( Type [ \"BaseStep\" ], super () . __new__ ( mcs , name , bases , dct )) cls . INPUT_SIGNATURE = {} cls . OUTPUT_SIGNATURE = {} cls . CONFIG_PARAMETER_NAME = None cls . CONFIG_CLASS = None cls . CONTEXT_PARAMETER_NAME = None # Get the signature of the step function step_function_signature = inspect . getfullargspec ( getattr ( cls , STEP_INNER_FUNC_NAME ) ) if bases : # We're not creating the abstract `BaseStep` class # but a concrete implementation. Make sure the step function # signature does not contain variable *args or **kwargs variable_arguments = None if step_function_signature . varargs : variable_arguments = f \"* { step_function_signature . varargs } \" elif step_function_signature . varkw : variable_arguments = f \"** { step_function_signature . varkw } \" if variable_arguments : raise StepInterfaceError ( f \"Unable to create step ' { name } ' with variable arguments \" f \"' { variable_arguments } '. Please make sure your step \" f \"functions are defined with a fixed amount of arguments.\" ) step_function_args = ( step_function_signature . args + step_function_signature . kwonlyargs ) # Remove 'self' from the signature if it exists if step_function_args and step_function_args [ 0 ] == \"self\" : step_function_args . pop ( 0 ) # Verify the input arguments of the step function for arg in step_function_args : arg_type = step_function_signature . annotations . get ( arg , None ) if not arg_type : raise StepInterfaceError ( f \"Missing type annotation for argument ' { arg } ' when \" f \"trying to create step ' { name } '. Please make sure to \" f \"include type annotations for all your step inputs \" f \"and outputs.\" ) if issubclass ( arg_type , BaseStepConfig ): # Raise an error if we already found a config in the signature if cls . CONFIG_CLASS is not None : raise StepInterfaceError ( f \"Found multiple configuration arguments \" f \"(' { cls . CONFIG_PARAMETER_NAME } ' and ' { arg } ') when \" f \"trying to create step ' { name } '. Please make sure to \" f \"only have one `BaseStepConfig` subclass as input \" f \"argument for a step.\" ) cls . CONFIG_PARAMETER_NAME = arg cls . CONFIG_CLASS = arg_type elif issubclass ( arg_type , StepContext ): if cls . CONTEXT_PARAMETER_NAME is not None : raise StepInterfaceError ( f \"Found multiple context arguments \" f \"(' { cls . CONTEXT_PARAMETER_NAME } ' and ' { arg } ') when \" f \"trying to create step ' { name } '. Please make sure to \" f \"only have one `StepContext` as input \" f \"argument for a step.\" ) cls . CONTEXT_PARAMETER_NAME = arg else : # Can't do any check for existing materializers right now # as they might get be defined later, so we simply store the # argument name and type for later use. cls . INPUT_SIGNATURE . update ({ arg : arg_type }) # Parse the returns of the step function return_type = step_function_signature . annotations . get ( \"return\" , None ) if return_type is not None : if isinstance ( return_type , Output ): cls . OUTPUT_SIGNATURE = dict ( return_type . items ()) else : cls . OUTPUT_SIGNATURE [ SINGLE_RETURN_OUT_NAME ] = return_type # Raise an exception if input and output names of a step overlap as # tfx requires them to be unique # TODO [ENG-155]: Can we prefix inputs and outputs to avoid this # restriction? counter : Counter [ str ] = collections . Counter () counter . update ( list ( cls . INPUT_SIGNATURE )) counter . update ( list ( cls . OUTPUT_SIGNATURE )) if cls . CONFIG_CLASS : counter . update ( list ( cls . CONFIG_CLASS . __fields__ . keys ())) shared_keys = { k for k in counter . elements () if counter [ k ] > 1 } if shared_keys : raise StepInterfaceError ( f \"The following keys are overlapping in the input, output and \" f \"config parameter names of step ' { name } ': { shared_keys } . \" f \"Please make sure that your input, output and config \" f \"parameter names are unique.\" ) return cls","title":"__new__()"},{"location":"api_docs/steps/#zenml.steps.base_step_config","text":"","title":"base_step_config"},{"location":"api_docs/steps/#zenml.steps.base_step_config.BaseStepConfig","text":"Base configuration class to pass execution params into a step. Source code in zenml/steps/base_step_config.py class BaseStepConfig ( BaseModel ): \"\"\"Base configuration class to pass execution params into a step.\"\"\"","title":"BaseStepConfig"},{"location":"api_docs/steps/#zenml.steps.builtin_steps","text":"","title":"builtin_steps"},{"location":"api_docs/steps/#zenml.steps.builtin_steps.pandas_analyzer","text":"","title":"pandas_analyzer"},{"location":"api_docs/steps/#zenml.steps.builtin_steps.pandas_analyzer.PandasAnalyzer","text":"Simple step implementation which analyzes a given pd.DataFrame Source code in zenml/steps/builtin_steps/pandas_analyzer.py class PandasAnalyzer ( BaseAnalyzerStep ): \"\"\"Simple step implementation which analyzes a given pd.DataFrame\"\"\" # Manually defining the type of the output artifacts OUTPUT_SPEC = { \"statistics\" : StatisticsArtifact , \"schema\" : SchemaArtifact } def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , config : PandasAnalyzerConfig , ) -> Output ( # type:ignore[valid-type] statistics = pd . DataFrame , schema = pd . DataFrame ): \"\"\"Main entrypoint function for the pandas analyzer Args: dataset: pd.DataFrame, the given dataset config: the configuration of the step Returns: the statistics and the schema of the given dataframe \"\"\" statistics = dataset . describe ( percentiles = config . percentiles , include = config . include , exclude = config . exclude , ) . T schema = dataset . dtypes . to_frame () . T . astype ( str ) return statistics , schema","title":"PandasAnalyzer"},{"location":"api_docs/steps/#zenml.steps.builtin_steps.pandas_analyzer.PandasAnalyzer.CONFIG_CLASS","text":"Config class for the PandasAnalyzer Config Source code in zenml/steps/builtin_steps/pandas_analyzer.py class PandasAnalyzerConfig ( BaseAnalyzerConfig ): \"\"\"Config class for the PandasAnalyzer Config\"\"\" percentiles : List [ float ] = [ 0.25 , 0.5 , 0.75 ] include : Optional [ Union [ str , List [ Type [ Any ]]]] = None exclude : Optional [ Union [ str , List [ Type [ Any ]]]] = None","title":"CONFIG_CLASS"},{"location":"api_docs/steps/#zenml.steps.builtin_steps.pandas_analyzer.PandasAnalyzer.entrypoint","text":"Main entrypoint function for the pandas analyzer Parameters: Name Type Description Default dataset DataFrame pd.DataFrame, the given dataset required config PandasAnalyzerConfig the configuration of the step required Returns: Type Description <zenml.steps.step_output.Output object at 0x7f2d93bd5b20> the statistics and the schema of the given dataframe Source code in zenml/steps/builtin_steps/pandas_analyzer.py def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , config : PandasAnalyzerConfig , ) -> Output ( # type:ignore[valid-type] statistics = pd . DataFrame , schema = pd . DataFrame ): \"\"\"Main entrypoint function for the pandas analyzer Args: dataset: pd.DataFrame, the given dataset config: the configuration of the step Returns: the statistics and the schema of the given dataframe \"\"\" statistics = dataset . describe ( percentiles = config . percentiles , include = config . include , exclude = config . exclude , ) . T schema = dataset . dtypes . to_frame () . T . astype ( str ) return statistics , schema","title":"entrypoint()"},{"location":"api_docs/steps/#zenml.steps.builtin_steps.pandas_analyzer.PandasAnalyzerConfig","text":"Config class for the PandasAnalyzer Config Source code in zenml/steps/builtin_steps/pandas_analyzer.py class PandasAnalyzerConfig ( BaseAnalyzerConfig ): \"\"\"Config class for the PandasAnalyzer Config\"\"\" percentiles : List [ float ] = [ 0.25 , 0.5 , 0.75 ] include : Optional [ Union [ str , List [ Type [ Any ]]]] = None exclude : Optional [ Union [ str , List [ Type [ Any ]]]] = None","title":"PandasAnalyzerConfig"},{"location":"api_docs/steps/#zenml.steps.builtin_steps.pandas_datasource","text":"","title":"pandas_datasource"},{"location":"api_docs/steps/#zenml.steps.builtin_steps.pandas_datasource.PandasDatasource","text":"Simple step implementation to ingest from a csv file using pandas Source code in zenml/steps/builtin_steps/pandas_datasource.py class PandasDatasource ( BaseDatasourceStep ): \"\"\"Simple step implementation to ingest from a csv file using pandas\"\"\" def entrypoint ( # type: ignore[override] self , config : PandasDatasourceConfig , ) -> pd . DataFrame : \"\"\"Main entrypoint method for the PandasDatasource Args: config: the configuration of the step Returns: the resulting dataframe \"\"\" return pd . read_csv ( filepath_or_buffer = config . path , sep = config . sep , header = config . header , names = config . names , index_col = config . index_col , )","title":"PandasDatasource"},{"location":"api_docs/steps/#zenml.steps.builtin_steps.pandas_datasource.PandasDatasource.CONFIG_CLASS","text":"Config class for the pandas csv datasource Source code in zenml/steps/builtin_steps/pandas_datasource.py class PandasDatasourceConfig ( BaseDatasourceConfig ): \"\"\"Config class for the pandas csv datasource\"\"\" path : str sep : str = \",\" header : Union [ int , List [ int ], str ] = \"infer\" names : Optional [ List [ str ]] = None index_col : Optional [ Union [ int , str , List [ Union [ int , str ]], bool ]] = None","title":"CONFIG_CLASS"},{"location":"api_docs/steps/#zenml.steps.builtin_steps.pandas_datasource.PandasDatasource.entrypoint","text":"Main entrypoint method for the PandasDatasource Parameters: Name Type Description Default config PandasDatasourceConfig the configuration of the step required Returns: Type Description DataFrame the resulting dataframe Source code in zenml/steps/builtin_steps/pandas_datasource.py def entrypoint ( # type: ignore[override] self , config : PandasDatasourceConfig , ) -> pd . DataFrame : \"\"\"Main entrypoint method for the PandasDatasource Args: config: the configuration of the step Returns: the resulting dataframe \"\"\" return pd . read_csv ( filepath_or_buffer = config . path , sep = config . sep , header = config . header , names = config . names , index_col = config . index_col , )","title":"entrypoint()"},{"location":"api_docs/steps/#zenml.steps.builtin_steps.pandas_datasource.PandasDatasourceConfig","text":"Config class for the pandas csv datasource Source code in zenml/steps/builtin_steps/pandas_datasource.py class PandasDatasourceConfig ( BaseDatasourceConfig ): \"\"\"Config class for the pandas csv datasource\"\"\" path : str sep : str = \",\" header : Union [ int , List [ int ], str ] = \"infer\" names : Optional [ List [ str ]] = None index_col : Optional [ Union [ int , str , List [ Union [ int , str ]], bool ]] = None","title":"PandasDatasourceConfig"},{"location":"api_docs/steps/#zenml.steps.step_context","text":"","title":"step_context"},{"location":"api_docs/steps/#zenml.steps.step_context.StepContext","text":"Provides additional context inside a step function. This class is used to access the metadata store, materializers and artifacts inside a step function. To use it, add a StepContext object to the signature of your step function like this: @step def my_step(context: StepContext, ...) context.get_output_materializer(...) You do not need to create a StepContext object yourself and pass it when creating the step, as long as you specify it in the signature ZenML will create the StepContext and automatically pass it when executing your step. Note : When using a StepContext inside a step, ZenML disables caching for this step by default as the context provides access to external resources which might influence the result of your step execution. To enable caching anyway, explicitly enable it in the @step decorator or when initializing your custom step class. Source code in zenml/steps/step_context.py class StepContext : \"\"\"Provides additional context inside a step function. This class is used to access the metadata store, materializers and artifacts inside a step function. To use it, add a `StepContext` object to the signature of your step function like this: @step def my_step(context: StepContext, ...) context.get_output_materializer(...) You do not need to create a `StepContext` object yourself and pass it when creating the step, as long as you specify it in the signature ZenML will create the `StepContext` and automatically pass it when executing your step. **Note**: When using a `StepContext` inside a step, ZenML disables caching for this step by default as the context provides access to external resources which might influence the result of your step execution. To enable caching anyway, explicitly enable it in the `@step` decorator or when initializing your custom step class. \"\"\" def __init__ ( self , step_name : str , output_materializers : Dict [ str , Type [ \"BaseMaterializer\" ]], output_artifacts : Dict [ str , \"BaseArtifact\" ], ): \"\"\"Initializes a StepContext instance. Args: step_name: The name of the step that this context is used in. output_materializers: The output materializers of the step that this context is used in. output_artifacts: The output artifacts of the step that this context is used in. Raises: StepContextError: If the keys of the output materializers and output artifacts do not match. \"\"\" if output_materializers . keys () != output_artifacts . keys (): raise StepContextError ( f \"Mismatched keys in output materializers and output \" f \"artifacts for step ' { step_name } '. Output materializer \" f \"keys: { set ( output_materializers ) } , output artifact \" f \"keys: { set ( output_artifacts ) } \" ) self . step_name = step_name self . _outputs = { key : StepContextOutput ( output_materializers [ key ], output_artifacts [ key ] ) for key in output_materializers . keys () } self . _metadata_store = Repository () . active_stack . metadata_store def _get_output ( self , output_name : Optional [ str ] = None ) -> StepContextOutput : \"\"\"Returns the materializer and artifact URI for a given step output. Args: output_name: Optional name of the output for which to get the materializer and URI. Returns: Tuple containing the materializer and artifact URI for the given output. Raises: StepContextError: If the step has no outputs, no output for the given `output_name` or if no `output_name` was given but the step has multiple outputs. \"\"\" output_count = len ( self . _outputs ) if output_count == 0 : raise StepContextError ( f \"Unable to get step output for step ' { self . step_name } ': \" f \"This step does not have any outputs.\" ) if not output_name and output_count > 1 : raise StepContextError ( f \"Unable to get step output for step ' { self . step_name } ': \" f \"This step has multiple outputs ( { set ( self . _outputs ) } ), \" f \"please specify which output to return.\" ) if output_name : if output_name not in self . _outputs : raise StepContextError ( f \"Unable to get step output ' { output_name } ' for \" f \"step ' { self . step_name } '. This step does not have an \" f \"output with the given name, please specify one of the \" f \"available outputs: { set ( self . _outputs ) } .\" ) return self . _outputs [ output_name ] else : return next ( iter ( self . _outputs . values ())) @property def metadata_store ( self ) -> \"BaseMetadataStore\" : \"\"\"Returns an instance of the metadata store that is used to store metadata about the step (and the corresponding pipeline) which is being executed.\"\"\" return self . _metadata_store def get_output_materializer ( self , output_name : Optional [ str ] = None , custom_materializer_class : Optional [ Type [ \"BaseMaterializer\" ]] = None , ) -> \"BaseMaterializer\" : \"\"\"Returns a materializer for a given step output. Args: output_name: Optional name of the output for which to get the materializer. If no name is given and the step only has a single output, the materializer of this output will be returned. If the step has multiple outputs, an exception will be raised. custom_materializer_class: If given, this `BaseMaterializer` subclass will be initialized with the output artifact instead of the materializer that was registered for this step output. Returns: A materializer initialized with the output artifact for the given output. Raises: StepContextError: If the step has no outputs, no output for the given `output_name` or if no `output_name` was given but the step has multiple outputs. \"\"\" materializer_class , artifact = self . _get_output ( output_name ) # use custom materializer class if provided or fallback to default # materializer for output materializer_class = custom_materializer_class or materializer_class return materializer_class ( artifact ) def get_output_artifact_uri ( self , output_name : Optional [ str ] = None ) -> str : \"\"\"Returns the artifact URI for a given step output. Args: output_name: Optional name of the output for which to get the URI. If no name is given and the step only has a single output, the URI of this output will be returned. If the step has multiple outputs, an exception will be raised. Returns: Artifact URI for the given output. Raises: StepContextError: If the step has no outputs, no output for the given `output_name` or if no `output_name` was given but the step has multiple outputs. \"\"\" return cast ( str , self . _get_output ( output_name ) . artifact . uri )","title":"StepContext"},{"location":"api_docs/steps/#zenml.steps.step_context.StepContext.metadata_store","text":"Returns an instance of the metadata store that is used to store metadata about the step (and the corresponding pipeline) which is being executed.","title":"metadata_store"},{"location":"api_docs/steps/#zenml.steps.step_context.StepContext.__init__","text":"Initializes a StepContext instance. Parameters: Name Type Description Default step_name str The name of the step that this context is used in. required output_materializers Dict[str, Type[BaseMaterializer]] The output materializers of the step that this context is used in. required output_artifacts Dict[str, BaseArtifact] The output artifacts of the step that this context is used in. required Exceptions: Type Description StepContextError If the keys of the output materializers and Source code in zenml/steps/step_context.py def __init__ ( self , step_name : str , output_materializers : Dict [ str , Type [ \"BaseMaterializer\" ]], output_artifacts : Dict [ str , \"BaseArtifact\" ], ): \"\"\"Initializes a StepContext instance. Args: step_name: The name of the step that this context is used in. output_materializers: The output materializers of the step that this context is used in. output_artifacts: The output artifacts of the step that this context is used in. Raises: StepContextError: If the keys of the output materializers and output artifacts do not match. \"\"\" if output_materializers . keys () != output_artifacts . keys (): raise StepContextError ( f \"Mismatched keys in output materializers and output \" f \"artifacts for step ' { step_name } '. Output materializer \" f \"keys: { set ( output_materializers ) } , output artifact \" f \"keys: { set ( output_artifacts ) } \" ) self . step_name = step_name self . _outputs = { key : StepContextOutput ( output_materializers [ key ], output_artifacts [ key ] ) for key in output_materializers . keys () } self . _metadata_store = Repository () . active_stack . metadata_store","title":"__init__()"},{"location":"api_docs/steps/#zenml.steps.step_context.StepContext.get_output_artifact_uri","text":"Returns the artifact URI for a given step output. Parameters: Name Type Description Default output_name Optional[str] Optional name of the output for which to get the URI. If no name is given and the step only has a single output, the URI of this output will be returned. If the step has multiple outputs, an exception will be raised. None Returns: Type Description str Artifact URI for the given output. Exceptions: Type Description StepContextError If the step has no outputs, no output for the given output_name or if no output_name was given but the step has multiple outputs. Source code in zenml/steps/step_context.py def get_output_artifact_uri ( self , output_name : Optional [ str ] = None ) -> str : \"\"\"Returns the artifact URI for a given step output. Args: output_name: Optional name of the output for which to get the URI. If no name is given and the step only has a single output, the URI of this output will be returned. If the step has multiple outputs, an exception will be raised. Returns: Artifact URI for the given output. Raises: StepContextError: If the step has no outputs, no output for the given `output_name` or if no `output_name` was given but the step has multiple outputs. \"\"\" return cast ( str , self . _get_output ( output_name ) . artifact . uri )","title":"get_output_artifact_uri()"},{"location":"api_docs/steps/#zenml.steps.step_context.StepContext.get_output_materializer","text":"Returns a materializer for a given step output. Parameters: Name Type Description Default output_name Optional[str] Optional name of the output for which to get the materializer. If no name is given and the step only has a single output, the materializer of this output will be returned. If the step has multiple outputs, an exception will be raised. None custom_materializer_class Optional[Type[BaseMaterializer]] If given, this BaseMaterializer subclass will be initialized with the output artifact instead of the materializer that was registered for this step output. None Returns: Type Description BaseMaterializer A materializer initialized with the output artifact for the given output. Exceptions: Type Description StepContextError If the step has no outputs, no output for the given output_name or if no output_name was given but the step has multiple outputs. Source code in zenml/steps/step_context.py def get_output_materializer ( self , output_name : Optional [ str ] = None , custom_materializer_class : Optional [ Type [ \"BaseMaterializer\" ]] = None , ) -> \"BaseMaterializer\" : \"\"\"Returns a materializer for a given step output. Args: output_name: Optional name of the output for which to get the materializer. If no name is given and the step only has a single output, the materializer of this output will be returned. If the step has multiple outputs, an exception will be raised. custom_materializer_class: If given, this `BaseMaterializer` subclass will be initialized with the output artifact instead of the materializer that was registered for this step output. Returns: A materializer initialized with the output artifact for the given output. Raises: StepContextError: If the step has no outputs, no output for the given `output_name` or if no `output_name` was given but the step has multiple outputs. \"\"\" materializer_class , artifact = self . _get_output ( output_name ) # use custom materializer class if provided or fallback to default # materializer for output materializer_class = custom_materializer_class or materializer_class return materializer_class ( artifact )","title":"get_output_materializer()"},{"location":"api_docs/steps/#zenml.steps.step_context.StepContextOutput","text":"Tuple containing materializer class and artifact for a step output. Source code in zenml/steps/step_context.py class StepContextOutput ( NamedTuple ): \"\"\"Tuple containing materializer class and artifact for a step output.\"\"\" materializer_class : Type [ \"BaseMaterializer\" ] artifact : \"BaseArtifact\"","title":"StepContextOutput"},{"location":"api_docs/steps/#zenml.steps.step_context.StepContextOutput.__getnewargs__","text":"Return self as a plain tuple. Used by copy and pickle. Source code in zenml/steps/step_context.py def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self )","title":"__getnewargs__()"},{"location":"api_docs/steps/#zenml.steps.step_context.StepContextOutput.__new__","text":"Create new instance of StepContextOutput(materializer_class, artifact)","title":"__new__()"},{"location":"api_docs/steps/#zenml.steps.step_context.StepContextOutput.__repr__","text":"Return a nicely formatted representation string Source code in zenml/steps/step_context.py def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self","title":"__repr__()"},{"location":"api_docs/steps/#zenml.steps.step_decorator","text":"","title":"step_decorator"},{"location":"api_docs/steps/#zenml.steps.step_decorator.step","text":"Outer decorator function for the creation of a ZenML step In order to be able to work with parameters such as name , it features a nested decorator structure. Parameters: Name Type Description Default _func Optional[~F] The decorated function. None name Optional[str] The name of the step. If left empty, the name of the decorated function will be used as a fallback. None enable_cache Optional[bool] Specify whether caching is enabled for this step. If no value is passed, caching is enabled by default unless the step requires a StepContext (see :class: zenml.steps.step_context.StepContext for more information). None output_types Optional[Dict[str, Type[BaseArtifact]]] A dictionary which sets different outputs to non-default artifact types None Returns: Type Description Union[Type[zenml.steps.base_step.BaseStep], Callable[[~F], Type[zenml.steps.base_step.BaseStep]]] the inner decorator which creates the step class based on the ZenML BaseStep Source code in zenml/steps/step_decorator.py def step ( _func : Optional [ F ] = None , * , name : Optional [ str ] = None , enable_cache : Optional [ bool ] = None , output_types : Optional [ Dict [ str , Type [ \"BaseArtifact\" ]]] = None , ) -> Union [ Type [ BaseStep ], Callable [[ F ], Type [ BaseStep ]]]: \"\"\"Outer decorator function for the creation of a ZenML step In order to be able to work with parameters such as `name`, it features a nested decorator structure. Args: _func: The decorated function. name: The name of the step. If left empty, the name of the decorated function will be used as a fallback. enable_cache: Specify whether caching is enabled for this step. If no value is passed, caching is enabled by default unless the step requires a `StepContext` (see :class:`zenml.steps.step_context.StepContext` for more information). output_types: A dictionary which sets different outputs to non-default artifact types Returns: the inner decorator which creates the step class based on the ZenML BaseStep \"\"\" def inner_decorator ( func : F ) -> Type [ BaseStep ]: \"\"\"Inner decorator function for the creation of a ZenML Step Args: func: types.FunctionType, this function will be used as the \"process\" method of the generated Step Returns: The class of a newly generated ZenML Step. \"\"\" step_name = name or func . __name__ output_spec = output_types or {} return type ( # noqa step_name , ( BaseStep ,), { STEP_INNER_FUNC_NAME : staticmethod ( func ), INSTANCE_CONFIGURATION : { PARAM_ENABLE_CACHE : enable_cache , PARAM_CREATED_BY_FUNCTIONAL_API : True , }, OUTPUT_SPEC : output_spec , \"__module__\" : func . __module__ , }, ) if _func is None : return inner_decorator else : return inner_decorator ( _func )","title":"step()"},{"location":"api_docs/steps/#zenml.steps.step_interfaces","text":"","title":"step_interfaces"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_analyzer_step","text":"","title":"base_analyzer_step"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_analyzer_step.BaseAnalyzerConfig","text":"Base class for analyzer step configurations Source code in zenml/steps/step_interfaces/base_analyzer_step.py class BaseAnalyzerConfig ( BaseStepConfig ): \"\"\"Base class for analyzer step configurations\"\"\"","title":"BaseAnalyzerConfig"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_analyzer_step.BaseAnalyzerStep","text":"Base step implementation for any analyzer step implementation on ZenML Source code in zenml/steps/step_interfaces/base_analyzer_step.py class BaseAnalyzerStep ( BaseStep ): \"\"\"Base step implementation for any analyzer step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , config : BaseAnalyzerConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] statistics = StatisticsArtifact , schema = SchemaArtifact ): \"\"\"Base entrypoint for any analyzer implementation\"\"\"","title":"BaseAnalyzerStep"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_analyzer_step.BaseAnalyzerStep.CONFIG_CLASS","text":"Base class for analyzer step configurations Source code in zenml/steps/step_interfaces/base_analyzer_step.py class BaseAnalyzerConfig ( BaseStepConfig ): \"\"\"Base class for analyzer step configurations\"\"\"","title":"CONFIG_CLASS"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_analyzer_step.BaseAnalyzerStep.entrypoint","text":"Base entrypoint for any analyzer implementation Source code in zenml/steps/step_interfaces/base_analyzer_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , config : BaseAnalyzerConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] statistics = StatisticsArtifact , schema = SchemaArtifact ): \"\"\"Base entrypoint for any analyzer implementation\"\"\"","title":"entrypoint()"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_datasource_step","text":"","title":"base_datasource_step"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_datasource_step.BaseDatasourceConfig","text":"Base class for datasource configs to inherit from Source code in zenml/steps/step_interfaces/base_datasource_step.py class BaseDatasourceConfig ( BaseStepConfig ): \"\"\"Base class for datasource configs to inherit from\"\"\"","title":"BaseDatasourceConfig"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_datasource_step.BaseDatasourceStep","text":"Base step implementation for any datasource step implementation on ZenML Source code in zenml/steps/step_interfaces/base_datasource_step.py class BaseDatasourceStep ( BaseStep ): \"\"\"Base step implementation for any datasource step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , config : BaseDatasourceConfig , context : StepContext , ) -> DataArtifact : \"\"\"Base entrypoint for any datasource implementation\"\"\"","title":"BaseDatasourceStep"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_datasource_step.BaseDatasourceStep.CONFIG_CLASS","text":"Base class for datasource configs to inherit from Source code in zenml/steps/step_interfaces/base_datasource_step.py class BaseDatasourceConfig ( BaseStepConfig ): \"\"\"Base class for datasource configs to inherit from\"\"\"","title":"CONFIG_CLASS"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_datasource_step.BaseDatasourceStep.entrypoint","text":"Base entrypoint for any datasource implementation Source code in zenml/steps/step_interfaces/base_datasource_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , config : BaseDatasourceConfig , context : StepContext , ) -> DataArtifact : \"\"\"Base entrypoint for any datasource implementation\"\"\"","title":"entrypoint()"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_drift_detection_step","text":"","title":"base_drift_detection_step"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_drift_detection_step.BaseDriftDetectionConfig","text":"Base class for drift detection step configurations Source code in zenml/steps/step_interfaces/base_drift_detection_step.py class BaseDriftDetectionConfig ( BaseStepConfig ): \"\"\"Base class for drift detection step configurations\"\"\"","title":"BaseDriftDetectionConfig"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_drift_detection_step.BaseDriftDetectionStep","text":"Base step implementation for any drift detection step implementation on ZenML Source code in zenml/steps/step_interfaces/base_drift_detection_step.py class BaseDriftDetectionStep ( BaseStep ): \"\"\"Base step implementation for any drift detection step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , reference_dataset : DataArtifact , comparison_dataset : DataArtifact , config : BaseDriftDetectionConfig , context : StepContext , ) -> Any : \"\"\"Base entrypoint for any drift detection implementation\"\"\"","title":"BaseDriftDetectionStep"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_drift_detection_step.BaseDriftDetectionStep.CONFIG_CLASS","text":"Base class for drift detection step configurations Source code in zenml/steps/step_interfaces/base_drift_detection_step.py class BaseDriftDetectionConfig ( BaseStepConfig ): \"\"\"Base class for drift detection step configurations\"\"\"","title":"CONFIG_CLASS"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_drift_detection_step.BaseDriftDetectionStep.entrypoint","text":"Base entrypoint for any drift detection implementation Source code in zenml/steps/step_interfaces/base_drift_detection_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , reference_dataset : DataArtifact , comparison_dataset : DataArtifact , config : BaseDriftDetectionConfig , context : StepContext , ) -> Any : \"\"\"Base entrypoint for any drift detection implementation\"\"\"","title":"entrypoint()"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_evaluator_step","text":"","title":"base_evaluator_step"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_evaluator_step.BaseEvaluatorConfig","text":"Base class for evaluator step configurations Source code in zenml/steps/step_interfaces/base_evaluator_step.py class BaseEvaluatorConfig ( BaseStepConfig ): \"\"\"Base class for evaluator step configurations\"\"\"","title":"BaseEvaluatorConfig"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_evaluator_step.BaseEvaluatorStep","text":"Base step implementation for any evaluator step implementation on ZenML Source code in zenml/steps/step_interfaces/base_evaluator_step.py class BaseEvaluatorStep ( BaseStep ): \"\"\"Base step implementation for any evaluator step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , model : ModelArtifact , config : BaseEvaluatorConfig , context : StepContext , ) -> DataArtifact : \"\"\"Base entrypoint for any evaluator implementation\"\"\"","title":"BaseEvaluatorStep"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_evaluator_step.BaseEvaluatorStep.CONFIG_CLASS","text":"Base class for evaluator step configurations Source code in zenml/steps/step_interfaces/base_evaluator_step.py class BaseEvaluatorConfig ( BaseStepConfig ): \"\"\"Base class for evaluator step configurations\"\"\"","title":"CONFIG_CLASS"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_evaluator_step.BaseEvaluatorStep.entrypoint","text":"Base entrypoint for any evaluator implementation Source code in zenml/steps/step_interfaces/base_evaluator_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , model : ModelArtifact , config : BaseEvaluatorConfig , context : StepContext , ) -> DataArtifact : \"\"\"Base entrypoint for any evaluator implementation\"\"\"","title":"entrypoint()"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_preprocessor_step","text":"","title":"base_preprocessor_step"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_preprocessor_step.BasePreprocessorConfig","text":"Base class for Preprocessor step configurations Source code in zenml/steps/step_interfaces/base_preprocessor_step.py class BasePreprocessorConfig ( BaseStepConfig ): \"\"\"Base class for Preprocessor step configurations\"\"\"","title":"BasePreprocessorConfig"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_preprocessor_step.BasePreprocessorStep","text":"Base step implementation for any Preprocessor step implementation on ZenML Source code in zenml/steps/step_interfaces/base_preprocessor_step.py class BasePreprocessorStep ( BaseStep ): \"\"\"Base step implementation for any Preprocessor step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , train_dataset : DataArtifact , test_dataset : DataArtifact , validation_dataset : DataArtifact , statistics : StatisticsArtifact , schema : SchemaArtifact , config : BasePreprocessorConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] train_transformed = DataArtifact , test_transformed = DataArtifact , validation_transformed = DataArtifact , ): \"\"\"Base entrypoint for any Preprocessor implementation\"\"\"","title":"BasePreprocessorStep"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_preprocessor_step.BasePreprocessorStep.CONFIG_CLASS","text":"Base class for Preprocessor step configurations Source code in zenml/steps/step_interfaces/base_preprocessor_step.py class BasePreprocessorConfig ( BaseStepConfig ): \"\"\"Base class for Preprocessor step configurations\"\"\"","title":"CONFIG_CLASS"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_preprocessor_step.BasePreprocessorStep.entrypoint","text":"Base entrypoint for any Preprocessor implementation Source code in zenml/steps/step_interfaces/base_preprocessor_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , train_dataset : DataArtifact , test_dataset : DataArtifact , validation_dataset : DataArtifact , statistics : StatisticsArtifact , schema : SchemaArtifact , config : BasePreprocessorConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] train_transformed = DataArtifact , test_transformed = DataArtifact , validation_transformed = DataArtifact , ): \"\"\"Base entrypoint for any Preprocessor implementation\"\"\"","title":"entrypoint()"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_split_step","text":"","title":"base_split_step"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_split_step.BaseSplitStep","text":"Base step implementation for any split step implementation on ZenML Source code in zenml/steps/step_interfaces/base_split_step.py class BaseSplitStep ( BaseStep ): \"\"\"Base step implementation for any split step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , config : BaseSplitStepConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] train = DataArtifact , test = DataArtifact , validation = DataArtifact ): \"\"\"Entrypoint for a function for the split steps to run\"\"\"","title":"BaseSplitStep"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_split_step.BaseSplitStep.CONFIG_CLASS","text":"Base class for split configs to inherit from Source code in zenml/steps/step_interfaces/base_split_step.py class BaseSplitStepConfig ( BaseStepConfig ): \"\"\"Base class for split configs to inherit from\"\"\"","title":"CONFIG_CLASS"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_split_step.BaseSplitStep.entrypoint","text":"Entrypoint for a function for the split steps to run Source code in zenml/steps/step_interfaces/base_split_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , config : BaseSplitStepConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] train = DataArtifact , test = DataArtifact , validation = DataArtifact ): \"\"\"Entrypoint for a function for the split steps to run\"\"\"","title":"entrypoint()"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_split_step.BaseSplitStepConfig","text":"Base class for split configs to inherit from Source code in zenml/steps/step_interfaces/base_split_step.py class BaseSplitStepConfig ( BaseStepConfig ): \"\"\"Base class for split configs to inherit from\"\"\"","title":"BaseSplitStepConfig"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_trainer_step","text":"","title":"base_trainer_step"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_trainer_step.BaseTrainerConfig","text":"Base class for Trainer step configurations Source code in zenml/steps/step_interfaces/base_trainer_step.py class BaseTrainerConfig ( BaseStepConfig ): \"\"\"Base class for Trainer step configurations\"\"\"","title":"BaseTrainerConfig"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_trainer_step.BaseTrainerStep","text":"Base step implementation for any Trainer step implementation on ZenML Source code in zenml/steps/step_interfaces/base_trainer_step.py class BaseTrainerStep ( BaseStep ): \"\"\"Base step implementation for any Trainer step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , train_dataset : DataArtifact , validation_dataset : DataArtifact , config : BaseTrainerConfig , context : StepContext , ) -> ModelArtifact : \"\"\"Base entrypoint for any Trainer implementation\"\"\"","title":"BaseTrainerStep"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_trainer_step.BaseTrainerStep.CONFIG_CLASS","text":"Base class for Trainer step configurations Source code in zenml/steps/step_interfaces/base_trainer_step.py class BaseTrainerConfig ( BaseStepConfig ): \"\"\"Base class for Trainer step configurations\"\"\"","title":"CONFIG_CLASS"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_trainer_step.BaseTrainerStep.entrypoint","text":"Base entrypoint for any Trainer implementation Source code in zenml/steps/step_interfaces/base_trainer_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , train_dataset : DataArtifact , validation_dataset : DataArtifact , config : BaseTrainerConfig , context : StepContext , ) -> ModelArtifact : \"\"\"Base entrypoint for any Trainer implementation\"\"\"","title":"entrypoint()"},{"location":"api_docs/steps/#zenml.steps.step_output","text":"","title":"step_output"},{"location":"api_docs/steps/#zenml.steps.step_output.Output","text":"A named tuple with a default name that cannot be overridden. Source code in zenml/steps/step_output.py class Output ( object ): \"\"\"A named tuple with a default name that cannot be overridden.\"\"\" def __init__ ( self , ** kwargs : Type [ Any ]): # TODO [ENG-161]: do we even need the named tuple here or is # a list of tuples (name, Type) sufficient? self . outputs = NamedTuple ( \"ZenOutput\" , ** kwargs ) # type: ignore[misc] def items ( self ) -> Iterator [ Tuple [ str , Type [ Any ]]]: \"\"\"Yields a tuple of type (output_name, output_type).\"\"\" yield from self . outputs . __annotations__ . items ()","title":"Output"},{"location":"api_docs/steps/#zenml.steps.step_output.Output.items","text":"Yields a tuple of type (output_name, output_type). Source code in zenml/steps/step_output.py def items ( self ) -> Iterator [ Tuple [ str , Type [ Any ]]]: \"\"\"Yields a tuple of type (output_name, output_type).\"\"\" yield from self . outputs . __annotations__ . items ()","title":"items()"},{"location":"api_docs/steps/#zenml.steps.utils","text":"The collection of utility functions/classes are inspired by their original implementation of the Tensorflow Extended team, which can be found here: https://github.com/tensorflow/tfx/blob/master/tfx/dsl/component/experimental /decorators.py This version is heavily adjusted to work with the Pipeline-Step paradigm which is proposed by ZenML.","title":"utils"},{"location":"api_docs/steps/#zenml.steps.utils.do_types_match","text":"Check whether type_a and type_b match. Parameters: Name Type Description Default type_a Type[Any] First Type to check. required type_b Type[Any] Second Type to check. required Returns: Type Description bool True if types match, otherwise False. Source code in zenml/steps/utils.py def do_types_match ( type_a : Type [ Any ], type_b : Type [ Any ]) -> bool : \"\"\"Check whether type_a and type_b match. Args: type_a: First Type to check. type_b: Second Type to check. Returns: True if types match, otherwise False. \"\"\" # TODO [ENG-158]: Check more complicated cases where type_a can be a sub-type # of type_b return type_a == type_b","title":"do_types_match()"},{"location":"api_docs/steps/#zenml.steps.utils.generate_component_class","text":"Generates a TFX component class for a ZenML step. Parameters: Name Type Description Default step_name str Name of the step for which the component will be created. required step_module str Module in which the step class is defined. required input_spec Dict[str, Type[zenml.artifacts.base_artifact.BaseArtifact]] Input artifacts of the step. required output_spec Dict[str, Type[zenml.artifacts.base_artifact.BaseArtifact]] Output artifacts of the step required execution_parameter_names Set[str] Execution parameter names of the step. required step_function Callable[..., Any] The actual function to execute when running the step. required materializers Dict[str, Type[zenml.materializers.base_materializer.BaseMaterializer]] Materializer classes for all outputs of the step. required Returns: Type Description Type[_ZenMLSimpleComponent] A TFX component class. Source code in zenml/steps/utils.py def generate_component_class ( step_name : str , step_module : str , input_spec : Dict [ str , Type [ BaseArtifact ]], output_spec : Dict [ str , Type [ BaseArtifact ]], execution_parameter_names : Set [ str ], step_function : Callable [ ... , Any ], materializers : Dict [ str , Type [ BaseMaterializer ]], ) -> Type [ \"_ZenMLSimpleComponent\" ]: \"\"\"Generates a TFX component class for a ZenML step. Args: step_name: Name of the step for which the component will be created. step_module: Module in which the step class is defined. input_spec: Input artifacts of the step. output_spec: Output artifacts of the step execution_parameter_names: Execution parameter names of the step. step_function: The actual function to execute when running the step. materializers: Materializer classes for all outputs of the step. Returns: A TFX component class. \"\"\" component_spec_class = generate_component_spec_class ( step_name = step_name , input_spec = input_spec , output_spec = output_spec , execution_parameter_names = execution_parameter_names , ) # Create executor class executor_class_name = f \" { step_name } _Executor\" executor_class = type ( executor_class_name , ( _FunctionExecutor ,), { \"_FUNCTION\" : staticmethod ( step_function ), \"__module__\" : step_module , \"materializers\" : materializers , PARAM_STEP_NAME : step_name , }, ) # Add the executor class to the module in which the step was defined module = sys . modules [ step_module ] setattr ( module , executor_class_name , executor_class ) return type ( step_name , ( _ZenMLSimpleComponent ,), { \"SPEC_CLASS\" : component_spec_class , \"EXECUTOR_SPEC\" : ExecutorClassSpec ( executor_class = executor_class ), \"__module__\" : step_module , }, )","title":"generate_component_class()"},{"location":"api_docs/steps/#zenml.steps.utils.generate_component_spec_class","text":"Generates a TFX component spec class for a ZenML step. Parameters: Name Type Description Default step_name str Name of the step for which the component will be created. required input_spec Dict[str, Type[zenml.artifacts.base_artifact.BaseArtifact]] Input artifacts of the step. required output_spec Dict[str, Type[zenml.artifacts.base_artifact.BaseArtifact]] Output artifacts of the step required execution_parameter_names Set[str] Execution parameter names of the step. required Returns: Type Description Type[tfx.types.component_spec.ComponentSpec] A TFX component spec class. Source code in zenml/steps/utils.py def generate_component_spec_class ( step_name : str , input_spec : Dict [ str , Type [ BaseArtifact ]], output_spec : Dict [ str , Type [ BaseArtifact ]], execution_parameter_names : Set [ str ], ) -> Type [ component_spec . ComponentSpec ]: \"\"\"Generates a TFX component spec class for a ZenML step. Args: step_name: Name of the step for which the component will be created. input_spec: Input artifacts of the step. output_spec: Output artifacts of the step execution_parameter_names: Execution parameter names of the step. Returns: A TFX component spec class. \"\"\" inputs = { key : component_spec . ChannelParameter ( type = artifact_type ) for key , artifact_type in input_spec . items () } outputs = { key : component_spec . ChannelParameter ( type = artifact_type ) for key , artifact_type in output_spec . items () } parameters = { key : component_spec . ExecutionParameter ( type = str ) # type: ignore[no-untyped-call] # noqa for key in execution_parameter_names } return type ( f \" { step_name } _Spec\" , ( component_spec . ComponentSpec ,), { \"INPUTS\" : inputs , \"OUTPUTS\" : outputs , \"PARAMETERS\" : parameters , }, )","title":"generate_component_spec_class()"},{"location":"api_docs/utils/","text":"Utils zenml.utils special The utils module contains utility functions handling analytics, reading and writing YAML data as well as other general purpose functions. analytics_utils Analytics code for ZenML get_environment () Returns a string representing the execution environment of the pipeline. Currently, one of docker , paperspace , 'colab', or native Source code in zenml/utils/analytics_utils.py def get_environment () -> str : \"\"\"Returns a string representing the execution environment of the pipeline. Currently, one of `docker`, `paperspace`, 'colab', or `native`\"\"\" if in_docker (): return \"docker\" elif in_google_colab (): return \"colab\" elif in_paperspace_gradient (): return \"paperspace\" else : return \"native\" get_segment_key () Get key for authorizing to Segment backend. Returns: Type Description str Segment key as a string. Source code in zenml/utils/analytics_utils.py def get_segment_key () -> str : \"\"\"Get key for authorizing to Segment backend. Returns: Segment key as a string. \"\"\" if IS_DEBUG_ENV : return SEGMENT_KEY_DEV else : return SEGMENT_KEY_PROD get_system_info () Returns system info as a dict. Returns: Type Description Dict[str, Any] A dict of system information. Source code in zenml/utils/analytics_utils.py def get_system_info () -> Dict [ str , Any ]: \"\"\"Returns system info as a dict. Returns: A dict of system information. \"\"\" system = platform . system () if system == \"Windows\" : release , version , csd , ptype = platform . win32_ver () return { \"os\" : \"windows\" , \"windows_version_release\" : release , \"windows_version\" : version , \"windows_version_service_pack\" : csd , \"windows_version_os_type\" : ptype , } if system == \"Darwin\" : return { \"os\" : \"mac\" , \"mac_version\" : platform . mac_ver ()[ 0 ]} if system == \"Linux\" : return { \"os\" : \"linux\" , \"linux_distro\" : distro . id (), \"linux_distro_like\" : distro . like (), \"linux_distro_version\" : distro . version (), } # We don't collect data for any other system. return { \"os\" : \"unknown\" } in_docker () Returns: True if running in a Docker container, else False Source code in zenml/utils/analytics_utils.py def in_docker () -> bool : \"\"\"Returns: True if running in a Docker container, else False\"\"\" # TODO [ENG-167]: Make this more reliable and add test. try : with open ( \"/proc/1/cgroup\" , \"rt\" ) as ifh : info = ifh . read () return \"docker\" in info or \"kubepod\" in info except ( FileNotFoundError , Exception ): return False in_google_colab () Returns: True if running in a Google Colab env, else False Source code in zenml/utils/analytics_utils.py def in_google_colab () -> bool : \"\"\"Returns: True if running in a Google Colab env, else False\"\"\" if \"COLAB_GPU\" in os . environ : return True return False in_paperspace_gradient () Returns: True if running in a Paperspace Gradient env, else False Source code in zenml/utils/analytics_utils.py def in_paperspace_gradient () -> bool : \"\"\"Returns: True if running in a Paperspace Gradient env, else False\"\"\" if \"PAPERSPACE_NOTEBOOK_REPO_ID\" in os . environ : return True return False parametrized ( dec ) This is a meta-decorator, that is, a decorator for decorators. As a decorator is a function, it actually works as a regular decorator with arguments: Source code in zenml/utils/analytics_utils.py def parametrized ( dec : Callable [ ... , Callable [ ... , Any ]] ) -> Callable [ ... , Callable [[ Callable [ ... , Any ]], Callable [ ... , Any ]]]: \"\"\"This is a meta-decorator, that is, a decorator for decorators. As a decorator is a function, it actually works as a regular decorator with arguments:\"\"\" def layer ( * args : Any , ** kwargs : Any ) -> Callable [[ Callable [ ... , Any ]], Callable [ ... , Any ]]: \"\"\"Internal layer\"\"\" def repl ( f : Callable [ ... , Any ]) -> Callable [ ... , Any ]: \"\"\"Internal repl\"\"\" return dec ( f , * args , ** kwargs ) return repl return layer track ( * args , ** kwargs ) Internal layer Source code in zenml/utils/analytics_utils.py def layer ( * args : Any , ** kwargs : Any ) -> Callable [[ Callable [ ... , Any ]], Callable [ ... , Any ]]: \"\"\"Internal layer\"\"\" def repl ( f : Callable [ ... , Any ]) -> Callable [ ... , Any ]: \"\"\"Internal repl\"\"\" return dec ( f , * args , ** kwargs ) return repl track_event ( event , metadata = None ) Track segment event if user opted-in. Parameters: Name Type Description Default event str Name of event to track in segment. required metadata Optional[Dict[str, Any]] Dict of metadata to track. None Returns: Type Description bool True if event is sent successfully, False is not. Source code in zenml/utils/analytics_utils.py def track_event ( event : str , metadata : Optional [ Dict [ str , Any ]] = None ) -> bool : \"\"\" Track segment event if user opted-in. Args: event: Name of event to track in segment. metadata: Dict of metadata to track. Returns: True if event is sent successfully, False is not. \"\"\" try : import analytics from zenml.config.global_config import GlobalConfig if analytics . write_key is None : analytics . write_key = get_segment_key () assert ( analytics . write_key is not None ), \"Analytics key not set but trying to make telemetry call.\" # Set this to 1 to avoid backoff loop analytics . max_retries = 1 gc = GlobalConfig () logger . debug ( f \"Attempting analytics: User: { gc . user_id } , \" f \"Event: { event } ,\" f \"Metadata: { metadata } \" ) if not gc . analytics_opt_in and event not in [ OPT_OUT_ANALYTICS , OPT_IN_ANALYTICS , ]: return False if metadata is None : metadata = {} # add basics metadata . update ( get_system_info ()) metadata . update ( { \"environment\" : get_environment (), \"version\" : __version__ , } ) analytics . track ( str ( gc . user_id ), event , metadata ) logger . debug ( f \"Analytics sent: User: { gc . user_id } , Event: { event } , Metadata: \" f \" { metadata } \" ) return True except Exception as e : # We should never fail main thread logger . debug ( f \"Analytics failed due to: { e } \" ) return False daemon Utility functions to start/stop daemon processes. This is only implemented for UNIX systems and therefore doesn't work on Windows. Based on https://www.jejik.com/articles/2007/02/a_simple_unix_linux_daemon_in_python/ check_if_daemon_is_running ( pid_file ) Checks whether a daemon process indicated by the PID file is running. Parameters: Name Type Description Default pid_file str Path to file containing the PID of the daemon process to check. required Source code in zenml/utils/daemon.py def check_if_daemon_is_running ( pid_file : str ) -> bool : \"\"\"Checks whether a daemon process indicated by the PID file is running. Args: pid_file: Path to file containing the PID of the daemon process to check. \"\"\" try : with open ( pid_file , \"r\" ) as f : pid = int ( f . read () . strip ()) except ( IOError , FileNotFoundError ): return False return psutil . pid_exists ( pid ) run_as_daemon ( daemon_function , pid_file , log_file = None , working_directory = '/' ) Runs a function as a daemon process. Parameters: Name Type Description Default daemon_function Callable[..., Any] The function to run as a daemon. required pid_file str Path to file in which to store the PID of the daemon process. required log_file Optional[str] Optional file to which the daemons stdout/stderr will be redirected to. None working_directory str Working directory for the daemon process, defaults to the root directory. '/' Exceptions: Type Description FileExistsError If the PID file already exists. Source code in zenml/utils/daemon.py def run_as_daemon ( daemon_function : Callable [ ... , Any ], pid_file : str , log_file : Optional [ str ] = None , working_directory : str = \"/\" , ) -> None : \"\"\"Runs a function as a daemon process. Args: daemon_function: The function to run as a daemon. pid_file: Path to file in which to store the PID of the daemon process. log_file: Optional file to which the daemons stdout/stderr will be redirected to. working_directory: Working directory for the daemon process, defaults to the root directory. Raises: FileExistsError: If the PID file already exists. \"\"\" # convert to absolute path as we will change working directory later pid_file = os . path . abspath ( pid_file ) if log_file : log_file = os . path . abspath ( log_file ) # check if PID file exists if os . path . exists ( pid_file ): raise FileExistsError ( f \"The PID file ' { pid_file } ' already exists, either the daemon \" f \"process is already running or something went wrong.\" ) # first fork try : pid = os . fork () if pid > 0 : # this is the process that called `run_as_daemon` so we # simply return so it can keep running return except OSError as e : logger . error ( \"Unable to fork (error code: %d )\" , e . errno ) sys . exit ( 1 ) # decouple from parent environment os . chdir ( working_directory ) os . setsid () os . umask ( 0 ) # second fork try : pid = os . fork () if pid > 0 : # this is the parent of the future daemon process, kill it # so the daemon gets adopted by the init process sys . exit ( 0 ) except OSError as e : sys . stderr . write ( f \"Unable to fork (error code: { e . errno } )\" ) sys . exit ( 1 ) # redirect standard file descriptors to devnull (or the given logfile) devnull = \"/dev/null\" if hasattr ( os , \"devnull\" ): devnull = os . devnull devnull_fd = os . open ( devnull , os . O_RDWR ) log_fd = os . open ( log_file , os . O_CREAT | os . O_RDWR ) if log_file else None out_fd = log_fd or devnull_fd os . dup2 ( devnull_fd , sys . stdin . fileno ()) os . dup2 ( out_fd , sys . stdout . fileno ()) os . dup2 ( out_fd , sys . stderr . fileno ()) # write the PID file with open ( pid_file , \"w+\" ) as f : f . write ( f \" { os . getpid () } \\n \" ) # register actions in case this process exits/gets killed def sigterm ( signum : int , frame : Optional [ types . FrameType ]) -> None : \"\"\"Removes the PID file.\"\"\" os . remove ( pid_file ) def cleanup () -> None : \"\"\"Removes the PID file.\"\"\" os . remove ( pid_file ) signal . signal ( signal . SIGTERM , sigterm ) atexit . register ( cleanup ) # finally run the actual daemon code daemon_function () stop_daemon ( pid_file , kill_children = True ) Stops a daemon process. Parameters: Name Type Description Default pid_file str Path to file containing the PID of the daemon process to kill. required kill_children bool If True , all child processes of the daemon process will be killed as well. True Source code in zenml/utils/daemon.py def stop_daemon ( pid_file : str , kill_children : bool = True ) -> None : \"\"\"Stops a daemon process. Args: pid_file: Path to file containing the PID of the daemon process to kill. kill_children: If `True`, all child processes of the daemon process will be killed as well. \"\"\" try : with open ( pid_file , \"r\" ) as f : pid = int ( f . read () . strip ()) except ( IOError , FileNotFoundError ): logger . warning ( \"Daemon PID file ' %s ' does not exist.\" , pid_file ) return if psutil . pid_exists ( pid ): process = psutil . Process ( pid ) if kill_children : for child in process . children ( recursive = True ): child . kill () process . kill () else : logger . warning ( \"PID from ' %s ' does not exist.\" , pid_file ) networking_utils find_available_port () Finds a local unoccupied port. Source code in zenml/utils/networking_utils.py def find_available_port () -> int : \"\"\"Finds a local unoccupied port.\"\"\" with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . bind (( \"127.0.0.1\" , 0 )) _ , port = s . getsockname () return cast ( int , port ) port_available ( port ) Checks if a local port is available. Source code in zenml/utils/networking_utils.py def port_available ( port : int ) -> bool : \"\"\"Checks if a local port is available.\"\"\" try : with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . bind (( \"127.0.0.1\" , port )) except socket . error as e : logger . debug ( \"Port %d unavailable: %s \" , port , e ) return False return True source_utils These utils are predicated on the following definitions: class_source: This is a python-import type path to a class, e.g. some.mod.class module_source: This is a python-import type path to a module, e.g. some.mod file_path, relative_path, absolute_path: These are file system paths. source: This is a class_source or module_source. If it is a class_source, it can also be optionally pinned. pin: Whatever comes after the @ symbol from a source, usually the git sha or the version of zenml as a string. create_zenml_pin () Creates a ZenML pin for source pinning from release version. Source code in zenml/utils/source_utils.py def create_zenml_pin () -> str : \"\"\"Creates a ZenML pin for source pinning from release version.\"\"\" return f \" { APP_NAME } _ { __version__ } \" get_absolute_path_from_module_source ( module ) Get a directory path from module source. E.g. zenml.core.step will return full/path/to/zenml/core/step . Parameters: Name Type Description Default module str A module e.g. zenml.core.step . required Source code in zenml/utils/source_utils.py def get_absolute_path_from_module_source ( module : str ) -> str : \"\"\"Get a directory path from module source. E.g. `zenml.core.step` will return `full/path/to/zenml/core/step`. Args: module: A module e.g. `zenml.core.step`. \"\"\" mod = importlib . import_module ( module ) return mod . __path__ [ 0 ] # type: ignore[no-any-return, attr-defined] get_class_source_from_source ( source ) Gets class source from source, i.e. module.path@version, returns version. Parameters: Name Type Description Default source str source pointing to potentially pinned sha. required Source code in zenml/utils/source_utils.py def get_class_source_from_source ( source : str ) -> str : \"\"\"Gets class source from source, i.e. module.path@version, returns version. Args: source: source pointing to potentially pinned sha. \"\"\" # source need not even be pinned return source . split ( \"@\" )[ 0 ] get_module_source_from_class ( class_ ) Takes class input and returns module_source. If class is already string then returns the same. Parameters: Name Type Description Default class_ Union[Type[Any], str] object of type class. required Source code in zenml/utils/source_utils.py def get_module_source_from_class ( class_ : Union [ Type [ Any ], str ] ) -> Optional [ str ]: \"\"\"Takes class input and returns module_source. If class is already string then returns the same. Args: class_: object of type class. \"\"\" if isinstance ( class_ , str ): module_source = class_ else : # Infer it from the class provided if not inspect . isclass ( class_ ): raise AssertionError ( \"step_type is neither string nor class.\" ) module_source = class_ . __module__ + \".\" + class_ . __name__ return module_source get_module_source_from_file_path ( file_path ) Gets module_source from a file_path. E.g. /home/myrepo/step/trainer.py returns myrepo.step.trainer if myrepo is the root of the repo. Parameters: Name Type Description Default file_path str Absolute file path to a file within the module. required Source code in zenml/utils/source_utils.py def get_module_source_from_file_path ( file_path : str ) -> str : \"\"\"Gets module_source from a file_path. E.g. `/home/myrepo/step/trainer.py` returns `myrepo.step.trainer` if `myrepo` is the root of the repo. Args: file_path: Absolute file path to a file within the module. \"\"\" from zenml.repository import Repository repo_path = str ( Repository () . root ) # Replace repo_path with file_path to get relative path left over relative_file_path = file_path . replace ( repo_path , \"\" )[ 1 :] # Kick out the .py and replace `/` with `.` to get the module source relative_file_path = relative_file_path . replace ( \".py\" , \"\" ) module_source = relative_file_path . replace ( \"/\" , \".\" ) return module_source get_module_source_from_source ( source ) Gets module source from source. E.g. some.module.file.class@version , returns some.module . Parameters: Name Type Description Default source str source pointing to potentially pinned sha. required Source code in zenml/utils/source_utils.py def get_module_source_from_source ( source : str ) -> str : \"\"\"Gets module source from source. E.g. `some.module.file.class@version`, returns `some.module`. Args: source: source pointing to potentially pinned sha. \"\"\" class_source = get_class_source_from_source ( source ) return \".\" . join ( class_source . split ( \".\" )[: - 2 ]) get_relative_path_from_module_source ( module_source ) Get a directory path from module, relative to root of repository. E.g. zenml.core.step will return zenml/core/step. Parameters: Name Type Description Default module_source str A module e.g. zenml.core.step required Source code in zenml/utils/source_utils.py def get_relative_path_from_module_source ( module_source : str ) -> str : \"\"\"Get a directory path from module, relative to root of repository. E.g. zenml.core.step will return zenml/core/step. Args: module_source: A module e.g. zenml.core.step \"\"\" return module_source . replace ( \".\" , \"/\" ) import_class_by_path ( class_path ) Imports a class based on a given path Parameters: Name Type Description Default class_path str str, class_source e.g. this.module.Class required Returns: the given class Source code in zenml/utils/source_utils.py def import_class_by_path ( class_path : str ) -> Type [ Any ]: \"\"\"Imports a class based on a given path Args: class_path: str, class_source e.g. this.module.Class Returns: the given class \"\"\" classname = class_path . split ( \".\" )[ - 1 ] modulename = \".\" . join ( class_path . split ( \".\" )[ 0 : - 1 ]) mod = importlib . import_module ( modulename ) return getattr ( mod , classname ) # type: ignore[no-any-return] import_python_file ( file_path ) Imports a python file. Parameters: Name Type Description Default file_path str Path to python file that should be imported. required Returns: Type Description module The imported module. Source code in zenml/utils/source_utils.py def import_python_file ( file_path : str ) -> types . ModuleType : \"\"\"Imports a python file. Args: file_path: Path to python file that should be imported. Returns: The imported module. \"\"\" # Add directory of python file to PYTHONPATH so we can import it file_path = os . path . abspath ( file_path ) sys . path . append ( os . path . dirname ( file_path )) module_name = os . path . splitext ( os . path . basename ( file_path ))[ 0 ] return importlib . import_module ( module_name ) is_inside_repository ( file_path ) Returns whether a file is inside a zenml repository. Source code in zenml/utils/source_utils.py def is_inside_repository ( file_path : str ) -> bool : \"\"\"Returns whether a file is inside a zenml repository.\"\"\" from zenml.repository import Repository repo_path = Repository () . root . resolve () absolute_file_path = pathlib . Path ( file_path ) . resolve () return repo_path in absolute_file_path . parents is_standard_pin ( pin ) Returns True if pin is valid ZenML pin, else False. Parameters: Name Type Description Default pin str potential ZenML pin like 'zenml_0.1.1' required Source code in zenml/utils/source_utils.py def is_standard_pin ( pin : str ) -> bool : \"\"\"Returns `True` if pin is valid ZenML pin, else False. Args: pin: potential ZenML pin like 'zenml_0.1.1' \"\"\" if pin . startswith ( f \" { APP_NAME } _\" ): return True return False is_standard_source ( source ) Returns True if source is a standard ZenML source. Parameters: Name Type Description Default source str class_source e.g. this.module.Class[@pin]. required Source code in zenml/utils/source_utils.py def is_standard_source ( source : str ) -> bool : \"\"\"Returns `True` if source is a standard ZenML source. Args: source: class_source e.g. this.module.Class[@pin]. \"\"\" if source . split ( \".\" )[ 0 ] == \"zenml\" : return True return False is_third_party_module ( file_path ) Returns whether a file belongs to a third party package. Source code in zenml/utils/source_utils.py def is_third_party_module ( file_path : str ) -> bool : \"\"\"Returns whether a file belongs to a third party package.\"\"\" absolute_file_path = pathlib . Path ( file_path ) . resolve () for path in site . getsitepackages () + [ site . getusersitepackages ()]: if pathlib . Path ( path ) . resolve () in absolute_file_path . parents : return True return False load_source_path_class ( source ) Loads a Python class from the source. Parameters: Name Type Description Default source str class_source e.g. this.module.Class[@sha] required Source code in zenml/utils/source_utils.py def load_source_path_class ( source : str ) -> Type [ Any ]: \"\"\"Loads a Python class from the source. Args: source: class_source e.g. this.module.Class[@sha] \"\"\" if \"@\" in source : source = source . split ( \"@\" )[ 0 ] logger . debug ( \"Unpinned step found with no git sha. Attempting to \" \"load class from current repository state.\" ) class_ = import_class_by_path ( source ) return class_ resolve_class ( class_ ) Resolves a class into a serializable source string. Parameters: Name Type Description Default class_ Type[Any] A Python Class reference. required Returns: source_path e.g. this.module.Class. Source code in zenml/utils/source_utils.py def resolve_class ( class_ : Type [ Any ]) -> str : \"\"\"Resolves a class into a serializable source string. Args: class_: A Python Class reference. Returns: source_path e.g. this.module.Class. \"\"\" initial_source = class_ . __module__ + \".\" + class_ . __name__ if is_standard_source ( initial_source ): return resolve_standard_source ( initial_source ) try : file_path = inspect . getfile ( class_ ) except TypeError : # builtin file return initial_source if ( initial_source . startswith ( \"__main__\" ) or not is_inside_repository ( file_path ) or is_third_party_module ( file_path ) ): return initial_source # Regular user file inside the repository -> get the full module # path relative to the repository module_source = get_module_source_from_file_path ( file_path ) # ENG-123 Sanitize for Windows OS # module_source = module_source.replace(\"\\\\\", \".\") return module_source + \".\" + class_ . __name__ resolve_standard_source ( source ) Creates a ZenML pin for source pinning from release version. Parameters: Name Type Description Default source str class_source e.g. this.module.Class. required Source code in zenml/utils/source_utils.py def resolve_standard_source ( source : str ) -> str : \"\"\"Creates a ZenML pin for source pinning from release version. Args: source: class_source e.g. this.module.Class. \"\"\" if \"@\" in source : raise AssertionError ( f \"source { source } is already pinned.\" ) pin = create_zenml_pin () return f \" { source } @ { pin } \" string_utils get_human_readable_filesize ( bytes_ ) Convert a file size in bytes into a human-readable string. Source code in zenml/utils/string_utils.py def get_human_readable_filesize ( bytes_ : int ) -> str : \"\"\"Convert a file size in bytes into a human-readable string.\"\"\" size = abs ( float ( bytes_ )) for unit in [ \"B\" , \"KiB\" , \"MiB\" , \"GiB\" ]: if size < 1024.0 or unit == \"GiB\" : break size /= 1024.0 return f \" { size : .2f } { unit } \" get_human_readable_time ( seconds ) Convert seconds into a human-readable string. Source code in zenml/utils/string_utils.py def get_human_readable_time ( seconds : float ) -> str : \"\"\"Convert seconds into a human-readable string.\"\"\" prefix = \"-\" if seconds < 0 else \"\" seconds = abs ( seconds ) int_seconds = int ( seconds ) days , int_seconds = divmod ( int_seconds , 86400 ) hours , int_seconds = divmod ( int_seconds , 3600 ) minutes , int_seconds = divmod ( int_seconds , 60 ) if days > 0 : time_string = f \" { days } d { hours } h { minutes } m { int_seconds } s\" elif hours > 0 : time_string = f \" { hours } h { minutes } m { int_seconds } s\" elif minutes > 0 : time_string = f \" { minutes } m { int_seconds } s\" else : time_string = f \" { seconds : .3f } s\" return prefix + time_string yaml_utils is_yaml ( file_path ) Returns True if file_path is YAML, else False Parameters: Name Type Description Default file_path str Path to YAML file. required Returns: Type Description bool True if is yaml, else False. Source code in zenml/utils/yaml_utils.py def is_yaml ( file_path : str ) -> bool : \"\"\"Returns True if file_path is YAML, else False Args: file_path: Path to YAML file. Returns: True if is yaml, else False. \"\"\" if file_path . endswith ( \"yaml\" ) or file_path . endswith ( \"yml\" ): return True return False read_json ( file_path ) Read JSON on file path and returns contents as dict. Parameters: Name Type Description Default file_path str Path to JSON file. required Source code in zenml/utils/yaml_utils.py def read_json ( file_path : str ) -> Any : \"\"\"Read JSON on file path and returns contents as dict. Args: file_path: Path to JSON file. \"\"\" if fileio . file_exists ( file_path ): contents = zenml . io . utils . read_file_contents_as_string ( file_path ) return json . loads ( contents ) else : raise FileNotFoundError ( f \" { file_path } does not exist.\" ) read_yaml ( file_path ) Read YAML on file path and returns contents as dict. Parameters: Name Type Description Default file_path str Path to YAML file. required Returns: Type Description Any Contents of the file in a dict. Source code in zenml/utils/yaml_utils.py def read_yaml ( file_path : str ) -> Any : \"\"\"Read YAML on file path and returns contents as dict. Args: file_path: Path to YAML file. Returns: Contents of the file in a dict. Raises: FileNotFoundError if file does not exist. \"\"\" if fileio . file_exists ( file_path ): contents = zenml . io . utils . read_file_contents_as_string ( file_path ) return yaml . safe_load ( contents ) else : raise FileNotFoundError ( f \" { file_path } does not exist.\" ) write_json ( file_path , contents ) Write contents as JSON format to file_path. Parameters: Name Type Description Default file_path str Path to JSON file. required contents Dict[str, Any] Contents of JSON file as dict. required Returns: Type Description None Contents of the file in a dict. Source code in zenml/utils/yaml_utils.py def write_json ( file_path : str , contents : Dict [ str , Any ]) -> None : \"\"\"Write contents as JSON format to file_path. Args: file_path: Path to JSON file. contents: Contents of JSON file as dict. Returns: Contents of the file in a dict. Raises: FileNotFoundError if directory does not exist. \"\"\" if not fileio . is_remote ( file_path ): dir_ = str ( Path ( file_path ) . parent ) if not fileio . is_dir ( dir_ ): # If it is a local path and it doesn't exist, raise Exception. raise FileNotFoundError ( f \"Directory { dir_ } does not exist.\" ) zenml . io . utils . write_file_contents_as_string ( file_path , json . dumps ( contents ) ) write_yaml ( file_path , contents ) Write contents as YAML format to file_path. Parameters: Name Type Description Default file_path str Path to YAML file. required contents Dict[Any, Any] Contents of YAML file as dict. required Source code in zenml/utils/yaml_utils.py def write_yaml ( file_path : str , contents : Dict [ Any , Any ]) -> None : \"\"\"Write contents as YAML format to file_path. Args: file_path: Path to YAML file. contents: Contents of YAML file as dict. Raises: FileNotFoundError if directory does not exist. \"\"\" if not fileio . is_remote ( file_path ): dir_ = str ( Path ( file_path ) . parent ) if not fileio . is_dir ( dir_ ): raise FileNotFoundError ( f \"Directory { dir_ } does not exist.\" ) zenml . io . utils . write_file_contents_as_string ( file_path , yaml . dump ( contents ))","title":"Utils"},{"location":"api_docs/utils/#utils","text":"","title":"Utils"},{"location":"api_docs/utils/#zenml.utils","text":"The utils module contains utility functions handling analytics, reading and writing YAML data as well as other general purpose functions.","title":"utils"},{"location":"api_docs/utils/#zenml.utils.analytics_utils","text":"Analytics code for ZenML","title":"analytics_utils"},{"location":"api_docs/utils/#zenml.utils.analytics_utils.get_environment","text":"Returns a string representing the execution environment of the pipeline. Currently, one of docker , paperspace , 'colab', or native Source code in zenml/utils/analytics_utils.py def get_environment () -> str : \"\"\"Returns a string representing the execution environment of the pipeline. Currently, one of `docker`, `paperspace`, 'colab', or `native`\"\"\" if in_docker (): return \"docker\" elif in_google_colab (): return \"colab\" elif in_paperspace_gradient (): return \"paperspace\" else : return \"native\"","title":"get_environment()"},{"location":"api_docs/utils/#zenml.utils.analytics_utils.get_segment_key","text":"Get key for authorizing to Segment backend. Returns: Type Description str Segment key as a string. Source code in zenml/utils/analytics_utils.py def get_segment_key () -> str : \"\"\"Get key for authorizing to Segment backend. Returns: Segment key as a string. \"\"\" if IS_DEBUG_ENV : return SEGMENT_KEY_DEV else : return SEGMENT_KEY_PROD","title":"get_segment_key()"},{"location":"api_docs/utils/#zenml.utils.analytics_utils.get_system_info","text":"Returns system info as a dict. Returns: Type Description Dict[str, Any] A dict of system information. Source code in zenml/utils/analytics_utils.py def get_system_info () -> Dict [ str , Any ]: \"\"\"Returns system info as a dict. Returns: A dict of system information. \"\"\" system = platform . system () if system == \"Windows\" : release , version , csd , ptype = platform . win32_ver () return { \"os\" : \"windows\" , \"windows_version_release\" : release , \"windows_version\" : version , \"windows_version_service_pack\" : csd , \"windows_version_os_type\" : ptype , } if system == \"Darwin\" : return { \"os\" : \"mac\" , \"mac_version\" : platform . mac_ver ()[ 0 ]} if system == \"Linux\" : return { \"os\" : \"linux\" , \"linux_distro\" : distro . id (), \"linux_distro_like\" : distro . like (), \"linux_distro_version\" : distro . version (), } # We don't collect data for any other system. return { \"os\" : \"unknown\" }","title":"get_system_info()"},{"location":"api_docs/utils/#zenml.utils.analytics_utils.in_docker","text":"Returns: True if running in a Docker container, else False Source code in zenml/utils/analytics_utils.py def in_docker () -> bool : \"\"\"Returns: True if running in a Docker container, else False\"\"\" # TODO [ENG-167]: Make this more reliable and add test. try : with open ( \"/proc/1/cgroup\" , \"rt\" ) as ifh : info = ifh . read () return \"docker\" in info or \"kubepod\" in info except ( FileNotFoundError , Exception ): return False","title":"in_docker()"},{"location":"api_docs/utils/#zenml.utils.analytics_utils.in_google_colab","text":"Returns: True if running in a Google Colab env, else False Source code in zenml/utils/analytics_utils.py def in_google_colab () -> bool : \"\"\"Returns: True if running in a Google Colab env, else False\"\"\" if \"COLAB_GPU\" in os . environ : return True return False","title":"in_google_colab()"},{"location":"api_docs/utils/#zenml.utils.analytics_utils.in_paperspace_gradient","text":"Returns: True if running in a Paperspace Gradient env, else False Source code in zenml/utils/analytics_utils.py def in_paperspace_gradient () -> bool : \"\"\"Returns: True if running in a Paperspace Gradient env, else False\"\"\" if \"PAPERSPACE_NOTEBOOK_REPO_ID\" in os . environ : return True return False","title":"in_paperspace_gradient()"},{"location":"api_docs/utils/#zenml.utils.analytics_utils.parametrized","text":"This is a meta-decorator, that is, a decorator for decorators. As a decorator is a function, it actually works as a regular decorator with arguments: Source code in zenml/utils/analytics_utils.py def parametrized ( dec : Callable [ ... , Callable [ ... , Any ]] ) -> Callable [ ... , Callable [[ Callable [ ... , Any ]], Callable [ ... , Any ]]]: \"\"\"This is a meta-decorator, that is, a decorator for decorators. As a decorator is a function, it actually works as a regular decorator with arguments:\"\"\" def layer ( * args : Any , ** kwargs : Any ) -> Callable [[ Callable [ ... , Any ]], Callable [ ... , Any ]]: \"\"\"Internal layer\"\"\" def repl ( f : Callable [ ... , Any ]) -> Callable [ ... , Any ]: \"\"\"Internal repl\"\"\" return dec ( f , * args , ** kwargs ) return repl return layer","title":"parametrized()"},{"location":"api_docs/utils/#zenml.utils.analytics_utils.track","text":"Internal layer Source code in zenml/utils/analytics_utils.py def layer ( * args : Any , ** kwargs : Any ) -> Callable [[ Callable [ ... , Any ]], Callable [ ... , Any ]]: \"\"\"Internal layer\"\"\" def repl ( f : Callable [ ... , Any ]) -> Callable [ ... , Any ]: \"\"\"Internal repl\"\"\" return dec ( f , * args , ** kwargs ) return repl","title":"track()"},{"location":"api_docs/utils/#zenml.utils.analytics_utils.track_event","text":"Track segment event if user opted-in. Parameters: Name Type Description Default event str Name of event to track in segment. required metadata Optional[Dict[str, Any]] Dict of metadata to track. None Returns: Type Description bool True if event is sent successfully, False is not. Source code in zenml/utils/analytics_utils.py def track_event ( event : str , metadata : Optional [ Dict [ str , Any ]] = None ) -> bool : \"\"\" Track segment event if user opted-in. Args: event: Name of event to track in segment. metadata: Dict of metadata to track. Returns: True if event is sent successfully, False is not. \"\"\" try : import analytics from zenml.config.global_config import GlobalConfig if analytics . write_key is None : analytics . write_key = get_segment_key () assert ( analytics . write_key is not None ), \"Analytics key not set but trying to make telemetry call.\" # Set this to 1 to avoid backoff loop analytics . max_retries = 1 gc = GlobalConfig () logger . debug ( f \"Attempting analytics: User: { gc . user_id } , \" f \"Event: { event } ,\" f \"Metadata: { metadata } \" ) if not gc . analytics_opt_in and event not in [ OPT_OUT_ANALYTICS , OPT_IN_ANALYTICS , ]: return False if metadata is None : metadata = {} # add basics metadata . update ( get_system_info ()) metadata . update ( { \"environment\" : get_environment (), \"version\" : __version__ , } ) analytics . track ( str ( gc . user_id ), event , metadata ) logger . debug ( f \"Analytics sent: User: { gc . user_id } , Event: { event } , Metadata: \" f \" { metadata } \" ) return True except Exception as e : # We should never fail main thread logger . debug ( f \"Analytics failed due to: { e } \" ) return False","title":"track_event()"},{"location":"api_docs/utils/#zenml.utils.daemon","text":"Utility functions to start/stop daemon processes. This is only implemented for UNIX systems and therefore doesn't work on Windows. Based on https://www.jejik.com/articles/2007/02/a_simple_unix_linux_daemon_in_python/","title":"daemon"},{"location":"api_docs/utils/#zenml.utils.daemon.check_if_daemon_is_running","text":"Checks whether a daemon process indicated by the PID file is running. Parameters: Name Type Description Default pid_file str Path to file containing the PID of the daemon process to check. required Source code in zenml/utils/daemon.py def check_if_daemon_is_running ( pid_file : str ) -> bool : \"\"\"Checks whether a daemon process indicated by the PID file is running. Args: pid_file: Path to file containing the PID of the daemon process to check. \"\"\" try : with open ( pid_file , \"r\" ) as f : pid = int ( f . read () . strip ()) except ( IOError , FileNotFoundError ): return False return psutil . pid_exists ( pid )","title":"check_if_daemon_is_running()"},{"location":"api_docs/utils/#zenml.utils.daemon.run_as_daemon","text":"Runs a function as a daemon process. Parameters: Name Type Description Default daemon_function Callable[..., Any] The function to run as a daemon. required pid_file str Path to file in which to store the PID of the daemon process. required log_file Optional[str] Optional file to which the daemons stdout/stderr will be redirected to. None working_directory str Working directory for the daemon process, defaults to the root directory. '/' Exceptions: Type Description FileExistsError If the PID file already exists. Source code in zenml/utils/daemon.py def run_as_daemon ( daemon_function : Callable [ ... , Any ], pid_file : str , log_file : Optional [ str ] = None , working_directory : str = \"/\" , ) -> None : \"\"\"Runs a function as a daemon process. Args: daemon_function: The function to run as a daemon. pid_file: Path to file in which to store the PID of the daemon process. log_file: Optional file to which the daemons stdout/stderr will be redirected to. working_directory: Working directory for the daemon process, defaults to the root directory. Raises: FileExistsError: If the PID file already exists. \"\"\" # convert to absolute path as we will change working directory later pid_file = os . path . abspath ( pid_file ) if log_file : log_file = os . path . abspath ( log_file ) # check if PID file exists if os . path . exists ( pid_file ): raise FileExistsError ( f \"The PID file ' { pid_file } ' already exists, either the daemon \" f \"process is already running or something went wrong.\" ) # first fork try : pid = os . fork () if pid > 0 : # this is the process that called `run_as_daemon` so we # simply return so it can keep running return except OSError as e : logger . error ( \"Unable to fork (error code: %d )\" , e . errno ) sys . exit ( 1 ) # decouple from parent environment os . chdir ( working_directory ) os . setsid () os . umask ( 0 ) # second fork try : pid = os . fork () if pid > 0 : # this is the parent of the future daemon process, kill it # so the daemon gets adopted by the init process sys . exit ( 0 ) except OSError as e : sys . stderr . write ( f \"Unable to fork (error code: { e . errno } )\" ) sys . exit ( 1 ) # redirect standard file descriptors to devnull (or the given logfile) devnull = \"/dev/null\" if hasattr ( os , \"devnull\" ): devnull = os . devnull devnull_fd = os . open ( devnull , os . O_RDWR ) log_fd = os . open ( log_file , os . O_CREAT | os . O_RDWR ) if log_file else None out_fd = log_fd or devnull_fd os . dup2 ( devnull_fd , sys . stdin . fileno ()) os . dup2 ( out_fd , sys . stdout . fileno ()) os . dup2 ( out_fd , sys . stderr . fileno ()) # write the PID file with open ( pid_file , \"w+\" ) as f : f . write ( f \" { os . getpid () } \\n \" ) # register actions in case this process exits/gets killed def sigterm ( signum : int , frame : Optional [ types . FrameType ]) -> None : \"\"\"Removes the PID file.\"\"\" os . remove ( pid_file ) def cleanup () -> None : \"\"\"Removes the PID file.\"\"\" os . remove ( pid_file ) signal . signal ( signal . SIGTERM , sigterm ) atexit . register ( cleanup ) # finally run the actual daemon code daemon_function ()","title":"run_as_daemon()"},{"location":"api_docs/utils/#zenml.utils.daemon.stop_daemon","text":"Stops a daemon process. Parameters: Name Type Description Default pid_file str Path to file containing the PID of the daemon process to kill. required kill_children bool If True , all child processes of the daemon process will be killed as well. True Source code in zenml/utils/daemon.py def stop_daemon ( pid_file : str , kill_children : bool = True ) -> None : \"\"\"Stops a daemon process. Args: pid_file: Path to file containing the PID of the daemon process to kill. kill_children: If `True`, all child processes of the daemon process will be killed as well. \"\"\" try : with open ( pid_file , \"r\" ) as f : pid = int ( f . read () . strip ()) except ( IOError , FileNotFoundError ): logger . warning ( \"Daemon PID file ' %s ' does not exist.\" , pid_file ) return if psutil . pid_exists ( pid ): process = psutil . Process ( pid ) if kill_children : for child in process . children ( recursive = True ): child . kill () process . kill () else : logger . warning ( \"PID from ' %s ' does not exist.\" , pid_file )","title":"stop_daemon()"},{"location":"api_docs/utils/#zenml.utils.networking_utils","text":"","title":"networking_utils"},{"location":"api_docs/utils/#zenml.utils.networking_utils.find_available_port","text":"Finds a local unoccupied port. Source code in zenml/utils/networking_utils.py def find_available_port () -> int : \"\"\"Finds a local unoccupied port.\"\"\" with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . bind (( \"127.0.0.1\" , 0 )) _ , port = s . getsockname () return cast ( int , port )","title":"find_available_port()"},{"location":"api_docs/utils/#zenml.utils.networking_utils.port_available","text":"Checks if a local port is available. Source code in zenml/utils/networking_utils.py def port_available ( port : int ) -> bool : \"\"\"Checks if a local port is available.\"\"\" try : with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . bind (( \"127.0.0.1\" , port )) except socket . error as e : logger . debug ( \"Port %d unavailable: %s \" , port , e ) return False return True","title":"port_available()"},{"location":"api_docs/utils/#zenml.utils.source_utils","text":"These utils are predicated on the following definitions: class_source: This is a python-import type path to a class, e.g. some.mod.class module_source: This is a python-import type path to a module, e.g. some.mod file_path, relative_path, absolute_path: These are file system paths. source: This is a class_source or module_source. If it is a class_source, it can also be optionally pinned. pin: Whatever comes after the @ symbol from a source, usually the git sha or the version of zenml as a string.","title":"source_utils"},{"location":"api_docs/utils/#zenml.utils.source_utils.create_zenml_pin","text":"Creates a ZenML pin for source pinning from release version. Source code in zenml/utils/source_utils.py def create_zenml_pin () -> str : \"\"\"Creates a ZenML pin for source pinning from release version.\"\"\" return f \" { APP_NAME } _ { __version__ } \"","title":"create_zenml_pin()"},{"location":"api_docs/utils/#zenml.utils.source_utils.get_absolute_path_from_module_source","text":"Get a directory path from module source. E.g. zenml.core.step will return full/path/to/zenml/core/step . Parameters: Name Type Description Default module str A module e.g. zenml.core.step . required Source code in zenml/utils/source_utils.py def get_absolute_path_from_module_source ( module : str ) -> str : \"\"\"Get a directory path from module source. E.g. `zenml.core.step` will return `full/path/to/zenml/core/step`. Args: module: A module e.g. `zenml.core.step`. \"\"\" mod = importlib . import_module ( module ) return mod . __path__ [ 0 ] # type: ignore[no-any-return, attr-defined]","title":"get_absolute_path_from_module_source()"},{"location":"api_docs/utils/#zenml.utils.source_utils.get_class_source_from_source","text":"Gets class source from source, i.e. module.path@version, returns version. Parameters: Name Type Description Default source str source pointing to potentially pinned sha. required Source code in zenml/utils/source_utils.py def get_class_source_from_source ( source : str ) -> str : \"\"\"Gets class source from source, i.e. module.path@version, returns version. Args: source: source pointing to potentially pinned sha. \"\"\" # source need not even be pinned return source . split ( \"@\" )[ 0 ]","title":"get_class_source_from_source()"},{"location":"api_docs/utils/#zenml.utils.source_utils.get_module_source_from_class","text":"Takes class input and returns module_source. If class is already string then returns the same. Parameters: Name Type Description Default class_ Union[Type[Any], str] object of type class. required Source code in zenml/utils/source_utils.py def get_module_source_from_class ( class_ : Union [ Type [ Any ], str ] ) -> Optional [ str ]: \"\"\"Takes class input and returns module_source. If class is already string then returns the same. Args: class_: object of type class. \"\"\" if isinstance ( class_ , str ): module_source = class_ else : # Infer it from the class provided if not inspect . isclass ( class_ ): raise AssertionError ( \"step_type is neither string nor class.\" ) module_source = class_ . __module__ + \".\" + class_ . __name__ return module_source","title":"get_module_source_from_class()"},{"location":"api_docs/utils/#zenml.utils.source_utils.get_module_source_from_file_path","text":"Gets module_source from a file_path. E.g. /home/myrepo/step/trainer.py returns myrepo.step.trainer if myrepo is the root of the repo. Parameters: Name Type Description Default file_path str Absolute file path to a file within the module. required Source code in zenml/utils/source_utils.py def get_module_source_from_file_path ( file_path : str ) -> str : \"\"\"Gets module_source from a file_path. E.g. `/home/myrepo/step/trainer.py` returns `myrepo.step.trainer` if `myrepo` is the root of the repo. Args: file_path: Absolute file path to a file within the module. \"\"\" from zenml.repository import Repository repo_path = str ( Repository () . root ) # Replace repo_path with file_path to get relative path left over relative_file_path = file_path . replace ( repo_path , \"\" )[ 1 :] # Kick out the .py and replace `/` with `.` to get the module source relative_file_path = relative_file_path . replace ( \".py\" , \"\" ) module_source = relative_file_path . replace ( \"/\" , \".\" ) return module_source","title":"get_module_source_from_file_path()"},{"location":"api_docs/utils/#zenml.utils.source_utils.get_module_source_from_source","text":"Gets module source from source. E.g. some.module.file.class@version , returns some.module . Parameters: Name Type Description Default source str source pointing to potentially pinned sha. required Source code in zenml/utils/source_utils.py def get_module_source_from_source ( source : str ) -> str : \"\"\"Gets module source from source. E.g. `some.module.file.class@version`, returns `some.module`. Args: source: source pointing to potentially pinned sha. \"\"\" class_source = get_class_source_from_source ( source ) return \".\" . join ( class_source . split ( \".\" )[: - 2 ])","title":"get_module_source_from_source()"},{"location":"api_docs/utils/#zenml.utils.source_utils.get_relative_path_from_module_source","text":"Get a directory path from module, relative to root of repository. E.g. zenml.core.step will return zenml/core/step. Parameters: Name Type Description Default module_source str A module e.g. zenml.core.step required Source code in zenml/utils/source_utils.py def get_relative_path_from_module_source ( module_source : str ) -> str : \"\"\"Get a directory path from module, relative to root of repository. E.g. zenml.core.step will return zenml/core/step. Args: module_source: A module e.g. zenml.core.step \"\"\" return module_source . replace ( \".\" , \"/\" )","title":"get_relative_path_from_module_source()"},{"location":"api_docs/utils/#zenml.utils.source_utils.import_class_by_path","text":"Imports a class based on a given path Parameters: Name Type Description Default class_path str str, class_source e.g. this.module.Class required Returns: the given class Source code in zenml/utils/source_utils.py def import_class_by_path ( class_path : str ) -> Type [ Any ]: \"\"\"Imports a class based on a given path Args: class_path: str, class_source e.g. this.module.Class Returns: the given class \"\"\" classname = class_path . split ( \".\" )[ - 1 ] modulename = \".\" . join ( class_path . split ( \".\" )[ 0 : - 1 ]) mod = importlib . import_module ( modulename ) return getattr ( mod , classname ) # type: ignore[no-any-return]","title":"import_class_by_path()"},{"location":"api_docs/utils/#zenml.utils.source_utils.import_python_file","text":"Imports a python file. Parameters: Name Type Description Default file_path str Path to python file that should be imported. required Returns: Type Description module The imported module. Source code in zenml/utils/source_utils.py def import_python_file ( file_path : str ) -> types . ModuleType : \"\"\"Imports a python file. Args: file_path: Path to python file that should be imported. Returns: The imported module. \"\"\" # Add directory of python file to PYTHONPATH so we can import it file_path = os . path . abspath ( file_path ) sys . path . append ( os . path . dirname ( file_path )) module_name = os . path . splitext ( os . path . basename ( file_path ))[ 0 ] return importlib . import_module ( module_name )","title":"import_python_file()"},{"location":"api_docs/utils/#zenml.utils.source_utils.is_inside_repository","text":"Returns whether a file is inside a zenml repository. Source code in zenml/utils/source_utils.py def is_inside_repository ( file_path : str ) -> bool : \"\"\"Returns whether a file is inside a zenml repository.\"\"\" from zenml.repository import Repository repo_path = Repository () . root . resolve () absolute_file_path = pathlib . Path ( file_path ) . resolve () return repo_path in absolute_file_path . parents","title":"is_inside_repository()"},{"location":"api_docs/utils/#zenml.utils.source_utils.is_standard_pin","text":"Returns True if pin is valid ZenML pin, else False. Parameters: Name Type Description Default pin str potential ZenML pin like 'zenml_0.1.1' required Source code in zenml/utils/source_utils.py def is_standard_pin ( pin : str ) -> bool : \"\"\"Returns `True` if pin is valid ZenML pin, else False. Args: pin: potential ZenML pin like 'zenml_0.1.1' \"\"\" if pin . startswith ( f \" { APP_NAME } _\" ): return True return False","title":"is_standard_pin()"},{"location":"api_docs/utils/#zenml.utils.source_utils.is_standard_source","text":"Returns True if source is a standard ZenML source. Parameters: Name Type Description Default source str class_source e.g. this.module.Class[@pin]. required Source code in zenml/utils/source_utils.py def is_standard_source ( source : str ) -> bool : \"\"\"Returns `True` if source is a standard ZenML source. Args: source: class_source e.g. this.module.Class[@pin]. \"\"\" if source . split ( \".\" )[ 0 ] == \"zenml\" : return True return False","title":"is_standard_source()"},{"location":"api_docs/utils/#zenml.utils.source_utils.is_third_party_module","text":"Returns whether a file belongs to a third party package. Source code in zenml/utils/source_utils.py def is_third_party_module ( file_path : str ) -> bool : \"\"\"Returns whether a file belongs to a third party package.\"\"\" absolute_file_path = pathlib . Path ( file_path ) . resolve () for path in site . getsitepackages () + [ site . getusersitepackages ()]: if pathlib . Path ( path ) . resolve () in absolute_file_path . parents : return True return False","title":"is_third_party_module()"},{"location":"api_docs/utils/#zenml.utils.source_utils.load_source_path_class","text":"Loads a Python class from the source. Parameters: Name Type Description Default source str class_source e.g. this.module.Class[@sha] required Source code in zenml/utils/source_utils.py def load_source_path_class ( source : str ) -> Type [ Any ]: \"\"\"Loads a Python class from the source. Args: source: class_source e.g. this.module.Class[@sha] \"\"\" if \"@\" in source : source = source . split ( \"@\" )[ 0 ] logger . debug ( \"Unpinned step found with no git sha. Attempting to \" \"load class from current repository state.\" ) class_ = import_class_by_path ( source ) return class_","title":"load_source_path_class()"},{"location":"api_docs/utils/#zenml.utils.source_utils.resolve_class","text":"Resolves a class into a serializable source string. Parameters: Name Type Description Default class_ Type[Any] A Python Class reference. required Returns: source_path e.g. this.module.Class. Source code in zenml/utils/source_utils.py def resolve_class ( class_ : Type [ Any ]) -> str : \"\"\"Resolves a class into a serializable source string. Args: class_: A Python Class reference. Returns: source_path e.g. this.module.Class. \"\"\" initial_source = class_ . __module__ + \".\" + class_ . __name__ if is_standard_source ( initial_source ): return resolve_standard_source ( initial_source ) try : file_path = inspect . getfile ( class_ ) except TypeError : # builtin file return initial_source if ( initial_source . startswith ( \"__main__\" ) or not is_inside_repository ( file_path ) or is_third_party_module ( file_path ) ): return initial_source # Regular user file inside the repository -> get the full module # path relative to the repository module_source = get_module_source_from_file_path ( file_path ) # ENG-123 Sanitize for Windows OS # module_source = module_source.replace(\"\\\\\", \".\") return module_source + \".\" + class_ . __name__","title":"resolve_class()"},{"location":"api_docs/utils/#zenml.utils.source_utils.resolve_standard_source","text":"Creates a ZenML pin for source pinning from release version. Parameters: Name Type Description Default source str class_source e.g. this.module.Class. required Source code in zenml/utils/source_utils.py def resolve_standard_source ( source : str ) -> str : \"\"\"Creates a ZenML pin for source pinning from release version. Args: source: class_source e.g. this.module.Class. \"\"\" if \"@\" in source : raise AssertionError ( f \"source { source } is already pinned.\" ) pin = create_zenml_pin () return f \" { source } @ { pin } \"","title":"resolve_standard_source()"},{"location":"api_docs/utils/#zenml.utils.string_utils","text":"","title":"string_utils"},{"location":"api_docs/utils/#zenml.utils.string_utils.get_human_readable_filesize","text":"Convert a file size in bytes into a human-readable string. Source code in zenml/utils/string_utils.py def get_human_readable_filesize ( bytes_ : int ) -> str : \"\"\"Convert a file size in bytes into a human-readable string.\"\"\" size = abs ( float ( bytes_ )) for unit in [ \"B\" , \"KiB\" , \"MiB\" , \"GiB\" ]: if size < 1024.0 or unit == \"GiB\" : break size /= 1024.0 return f \" { size : .2f } { unit } \"","title":"get_human_readable_filesize()"},{"location":"api_docs/utils/#zenml.utils.string_utils.get_human_readable_time","text":"Convert seconds into a human-readable string. Source code in zenml/utils/string_utils.py def get_human_readable_time ( seconds : float ) -> str : \"\"\"Convert seconds into a human-readable string.\"\"\" prefix = \"-\" if seconds < 0 else \"\" seconds = abs ( seconds ) int_seconds = int ( seconds ) days , int_seconds = divmod ( int_seconds , 86400 ) hours , int_seconds = divmod ( int_seconds , 3600 ) minutes , int_seconds = divmod ( int_seconds , 60 ) if days > 0 : time_string = f \" { days } d { hours } h { minutes } m { int_seconds } s\" elif hours > 0 : time_string = f \" { hours } h { minutes } m { int_seconds } s\" elif minutes > 0 : time_string = f \" { minutes } m { int_seconds } s\" else : time_string = f \" { seconds : .3f } s\" return prefix + time_string","title":"get_human_readable_time()"},{"location":"api_docs/utils/#zenml.utils.yaml_utils","text":"","title":"yaml_utils"},{"location":"api_docs/utils/#zenml.utils.yaml_utils.is_yaml","text":"Returns True if file_path is YAML, else False Parameters: Name Type Description Default file_path str Path to YAML file. required Returns: Type Description bool True if is yaml, else False. Source code in zenml/utils/yaml_utils.py def is_yaml ( file_path : str ) -> bool : \"\"\"Returns True if file_path is YAML, else False Args: file_path: Path to YAML file. Returns: True if is yaml, else False. \"\"\" if file_path . endswith ( \"yaml\" ) or file_path . endswith ( \"yml\" ): return True return False","title":"is_yaml()"},{"location":"api_docs/utils/#zenml.utils.yaml_utils.read_json","text":"Read JSON on file path and returns contents as dict. Parameters: Name Type Description Default file_path str Path to JSON file. required Source code in zenml/utils/yaml_utils.py def read_json ( file_path : str ) -> Any : \"\"\"Read JSON on file path and returns contents as dict. Args: file_path: Path to JSON file. \"\"\" if fileio . file_exists ( file_path ): contents = zenml . io . utils . read_file_contents_as_string ( file_path ) return json . loads ( contents ) else : raise FileNotFoundError ( f \" { file_path } does not exist.\" )","title":"read_json()"},{"location":"api_docs/utils/#zenml.utils.yaml_utils.read_yaml","text":"Read YAML on file path and returns contents as dict. Parameters: Name Type Description Default file_path str Path to YAML file. required Returns: Type Description Any Contents of the file in a dict. Source code in zenml/utils/yaml_utils.py def read_yaml ( file_path : str ) -> Any : \"\"\"Read YAML on file path and returns contents as dict. Args: file_path: Path to YAML file. Returns: Contents of the file in a dict. Raises: FileNotFoundError if file does not exist. \"\"\" if fileio . file_exists ( file_path ): contents = zenml . io . utils . read_file_contents_as_string ( file_path ) return yaml . safe_load ( contents ) else : raise FileNotFoundError ( f \" { file_path } does not exist.\" )","title":"read_yaml()"},{"location":"api_docs/utils/#zenml.utils.yaml_utils.write_json","text":"Write contents as JSON format to file_path. Parameters: Name Type Description Default file_path str Path to JSON file. required contents Dict[str, Any] Contents of JSON file as dict. required Returns: Type Description None Contents of the file in a dict. Source code in zenml/utils/yaml_utils.py def write_json ( file_path : str , contents : Dict [ str , Any ]) -> None : \"\"\"Write contents as JSON format to file_path. Args: file_path: Path to JSON file. contents: Contents of JSON file as dict. Returns: Contents of the file in a dict. Raises: FileNotFoundError if directory does not exist. \"\"\" if not fileio . is_remote ( file_path ): dir_ = str ( Path ( file_path ) . parent ) if not fileio . is_dir ( dir_ ): # If it is a local path and it doesn't exist, raise Exception. raise FileNotFoundError ( f \"Directory { dir_ } does not exist.\" ) zenml . io . utils . write_file_contents_as_string ( file_path , json . dumps ( contents ) )","title":"write_json()"},{"location":"api_docs/utils/#zenml.utils.yaml_utils.write_yaml","text":"Write contents as YAML format to file_path. Parameters: Name Type Description Default file_path str Path to YAML file. required contents Dict[Any, Any] Contents of YAML file as dict. required Source code in zenml/utils/yaml_utils.py def write_yaml ( file_path : str , contents : Dict [ Any , Any ]) -> None : \"\"\"Write contents as YAML format to file_path. Args: file_path: Path to YAML file. contents: Contents of YAML file as dict. Raises: FileNotFoundError if directory does not exist. \"\"\" if not fileio . is_remote ( file_path ): dir_ = str ( Path ( file_path ) . parent ) if not fileio . is_dir ( dir_ ): raise FileNotFoundError ( f \"Directory { dir_ } does not exist.\" ) zenml . io . utils . write_file_contents_as_string ( file_path , yaml . dump ( contents ))","title":"write_yaml()"},{"location":"api_docs/visualizers/","text":"Visualizers zenml.visualizers special The visualizers module offers a way of constructing and displaying visualizations of steps and pipeline results. The BaseVisualizer class is at the root of all the other visualizers, including options to view the results of pipeline runs, steps and pipelines themselves. base_pipeline_run_visualizer BasePipelineRunVisualizer ( BaseVisualizer ) The base implementation of a ZenML Pipeline Run Visualizer. Source code in zenml/visualizers/base_pipeline_run_visualizer.py class BasePipelineRunVisualizer ( BaseVisualizer ): \"\"\"The base implementation of a ZenML Pipeline Run Visualizer.\"\"\" @abstractmethod def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize pipeline runs.\"\"\" visualize ( self , object , * args , ** kwargs ) Method to visualize pipeline runs. Source code in zenml/visualizers/base_pipeline_run_visualizer.py @abstractmethod def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize pipeline runs.\"\"\" base_pipeline_visualizer BasePipelineVisualizer ( BaseVisualizer ) The base implementation of a ZenML Pipeline Visualizer. Source code in zenml/visualizers/base_pipeline_visualizer.py class BasePipelineVisualizer ( BaseVisualizer ): \"\"\"The base implementation of a ZenML Pipeline Visualizer.\"\"\" @abstractmethod def visualize ( self , object : PipelineView , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Method to visualize pipelines.\"\"\" visualize ( self , object , * args , ** kwargs ) Method to visualize pipelines. Source code in zenml/visualizers/base_pipeline_visualizer.py @abstractmethod def visualize ( self , object : PipelineView , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Method to visualize pipelines.\"\"\" base_step_visualizer BaseStepVisualizer ( BaseVisualizer ) The base implementation of a ZenML Step Visualizer. Source code in zenml/visualizers/base_step_visualizer.py class BaseStepVisualizer ( BaseVisualizer ): \"\"\"The base implementation of a ZenML Step Visualizer.\"\"\" @abstractmethod def visualize ( self , object : StepView , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Method to visualize steps.\"\"\" @staticmethod def running_in_notebook () -> bool : \"\"\"Detect whether we're running in a Jupyter notebook or not\"\"\" try : from IPython import get_ipython # type: ignore if get_ipython () is None : # IPython is installed but not running from a notebook return False else : return True except ImportError : # We do not even have IPython installed return False running_in_notebook () staticmethod Detect whether we're running in a Jupyter notebook or not Source code in zenml/visualizers/base_step_visualizer.py @staticmethod def running_in_notebook () -> bool : \"\"\"Detect whether we're running in a Jupyter notebook or not\"\"\" try : from IPython import get_ipython # type: ignore if get_ipython () is None : # IPython is installed but not running from a notebook return False else : return True except ImportError : # We do not even have IPython installed return False visualize ( self , object , * args , ** kwargs ) Method to visualize steps. Source code in zenml/visualizers/base_step_visualizer.py @abstractmethod def visualize ( self , object : StepView , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Method to visualize steps.\"\"\" base_visualizer BaseVisualizer Base class for all ZenML Visualizers. Source code in zenml/visualizers/base_visualizer.py class BaseVisualizer : \"\"\"Base class for all ZenML Visualizers.\"\"\" @abstractmethod def visualize ( self , object : Any , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize objects.\"\"\" visualize ( self , object , * args , ** kwargs ) Method to visualize objects. Source code in zenml/visualizers/base_visualizer.py @abstractmethod def visualize ( self , object : Any , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize objects.\"\"\"","title":"Visualizers"},{"location":"api_docs/visualizers/#visualizers","text":"","title":"Visualizers"},{"location":"api_docs/visualizers/#zenml.visualizers","text":"The visualizers module offers a way of constructing and displaying visualizations of steps and pipeline results. The BaseVisualizer class is at the root of all the other visualizers, including options to view the results of pipeline runs, steps and pipelines themselves.","title":"visualizers"},{"location":"api_docs/visualizers/#zenml.visualizers.base_pipeline_run_visualizer","text":"","title":"base_pipeline_run_visualizer"},{"location":"api_docs/visualizers/#zenml.visualizers.base_pipeline_run_visualizer.BasePipelineRunVisualizer","text":"The base implementation of a ZenML Pipeline Run Visualizer. Source code in zenml/visualizers/base_pipeline_run_visualizer.py class BasePipelineRunVisualizer ( BaseVisualizer ): \"\"\"The base implementation of a ZenML Pipeline Run Visualizer.\"\"\" @abstractmethod def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize pipeline runs.\"\"\"","title":"BasePipelineRunVisualizer"},{"location":"api_docs/visualizers/#zenml.visualizers.base_pipeline_run_visualizer.BasePipelineRunVisualizer.visualize","text":"Method to visualize pipeline runs. Source code in zenml/visualizers/base_pipeline_run_visualizer.py @abstractmethod def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize pipeline runs.\"\"\"","title":"visualize()"},{"location":"api_docs/visualizers/#zenml.visualizers.base_pipeline_visualizer","text":"","title":"base_pipeline_visualizer"},{"location":"api_docs/visualizers/#zenml.visualizers.base_pipeline_visualizer.BasePipelineVisualizer","text":"The base implementation of a ZenML Pipeline Visualizer. Source code in zenml/visualizers/base_pipeline_visualizer.py class BasePipelineVisualizer ( BaseVisualizer ): \"\"\"The base implementation of a ZenML Pipeline Visualizer.\"\"\" @abstractmethod def visualize ( self , object : PipelineView , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Method to visualize pipelines.\"\"\"","title":"BasePipelineVisualizer"},{"location":"api_docs/visualizers/#zenml.visualizers.base_pipeline_visualizer.BasePipelineVisualizer.visualize","text":"Method to visualize pipelines. Source code in zenml/visualizers/base_pipeline_visualizer.py @abstractmethod def visualize ( self , object : PipelineView , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Method to visualize pipelines.\"\"\"","title":"visualize()"},{"location":"api_docs/visualizers/#zenml.visualizers.base_step_visualizer","text":"","title":"base_step_visualizer"},{"location":"api_docs/visualizers/#zenml.visualizers.base_step_visualizer.BaseStepVisualizer","text":"The base implementation of a ZenML Step Visualizer. Source code in zenml/visualizers/base_step_visualizer.py class BaseStepVisualizer ( BaseVisualizer ): \"\"\"The base implementation of a ZenML Step Visualizer.\"\"\" @abstractmethod def visualize ( self , object : StepView , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Method to visualize steps.\"\"\" @staticmethod def running_in_notebook () -> bool : \"\"\"Detect whether we're running in a Jupyter notebook or not\"\"\" try : from IPython import get_ipython # type: ignore if get_ipython () is None : # IPython is installed but not running from a notebook return False else : return True except ImportError : # We do not even have IPython installed return False","title":"BaseStepVisualizer"},{"location":"api_docs/visualizers/#zenml.visualizers.base_step_visualizer.BaseStepVisualizer.running_in_notebook","text":"Detect whether we're running in a Jupyter notebook or not Source code in zenml/visualizers/base_step_visualizer.py @staticmethod def running_in_notebook () -> bool : \"\"\"Detect whether we're running in a Jupyter notebook or not\"\"\" try : from IPython import get_ipython # type: ignore if get_ipython () is None : # IPython is installed but not running from a notebook return False else : return True except ImportError : # We do not even have IPython installed return False","title":"running_in_notebook()"},{"location":"api_docs/visualizers/#zenml.visualizers.base_step_visualizer.BaseStepVisualizer.visualize","text":"Method to visualize steps. Source code in zenml/visualizers/base_step_visualizer.py @abstractmethod def visualize ( self , object : StepView , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Method to visualize steps.\"\"\"","title":"visualize()"},{"location":"api_docs/visualizers/#zenml.visualizers.base_visualizer","text":"","title":"base_visualizer"},{"location":"api_docs/visualizers/#zenml.visualizers.base_visualizer.BaseVisualizer","text":"Base class for all ZenML Visualizers. Source code in zenml/visualizers/base_visualizer.py class BaseVisualizer : \"\"\"Base class for all ZenML Visualizers.\"\"\" @abstractmethod def visualize ( self , object : Any , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize objects.\"\"\"","title":"BaseVisualizer"},{"location":"api_docs/visualizers/#zenml.visualizers.base_visualizer.BaseVisualizer.visualize","text":"Method to visualize objects. Source code in zenml/visualizers/base_visualizer.py @abstractmethod def visualize ( self , object : Any , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize objects.\"\"\"","title":"visualize()"},{"location":"api_docs/cli/base/","text":"Base zenml.cli.base","title":"Base"},{"location":"api_docs/cli/base/#base","text":"","title":"Base"},{"location":"api_docs/cli/base/#zenml.cli.base","text":"","title":"base"},{"location":"api_docs/cli/cli/","text":"Cli zenml.cli.cli .. currentmodule:: ce_cli.cli .. moduleauthor:: ZenML GmbH support@zenml.io","title":"Cli"},{"location":"api_docs/cli/cli/#cli","text":"","title":"Cli"},{"location":"api_docs/cli/cli/#zenml.cli.cli","text":".. currentmodule:: ce_cli.cli .. moduleauthor:: ZenML GmbH support@zenml.io","title":"cli"},{"location":"api_docs/cli/config/","text":"Config zenml.cli.config CLI for manipulating ZenML local and global config file.","title":"Config"},{"location":"api_docs/cli/config/#config","text":"","title":"Config"},{"location":"api_docs/cli/config/#zenml.cli.config","text":"CLI for manipulating ZenML local and global config file.","title":"config"},{"location":"api_docs/cli/example/","text":"Example zenml.cli.example Example Class for all example objects. Source code in zenml/cli/example.py class Example : \"\"\"Class for all example objects.\"\"\" def __init__ ( self , name : str , path_in_repo : Path ) -> None : \"\"\"Create a new Example instance. Args: name: The name of the example, specifically the name of the folder on git path_in_repo: Path to the local example within the global zenml folder. \"\"\" self . name = name self . path_in_repo = path_in_repo @property def readme_content ( self ) -> str : \"\"\"Returns the readme content associated with a particular example.\"\"\" readme_file = os . path . join ( self . path_in_repo , \"README.md\" ) try : with open ( readme_file ) as readme : readme_content = readme . read () return readme_content except FileNotFoundError : if fileio . file_exists ( str ( self . path_in_repo )) and fileio . is_dir ( str ( self . path_in_repo ) ): raise ValueError ( f \"No README.md file found in \" f \" { self . path_in_repo } \" ) else : raise FileNotFoundError ( f \"Example { self . name } is not one of the available options.\" f \" \\n \" f \"To list all available examples, type: `zenml example \" f \"list`\" ) readme_content : str property readonly Returns the readme content associated with a particular example. __init__ ( self , name , path_in_repo ) special Create a new Example instance. Parameters: Name Type Description Default name str The name of the example, specifically the name of the folder on git required path_in_repo Path Path to the local example within the global zenml folder. required Source code in zenml/cli/example.py def __init__ ( self , name : str , path_in_repo : Path ) -> None : \"\"\"Create a new Example instance. Args: name: The name of the example, specifically the name of the folder on git path_in_repo: Path to the local example within the global zenml folder. \"\"\" self . name = name self . path_in_repo = path_in_repo ExamplesRepo Class for the examples repository object. Source code in zenml/cli/example.py class ExamplesRepo : \"\"\"Class for the examples repository object.\"\"\" def __init__ ( self , cloning_path : Path ) -> None : \"\"\"Create a new ExamplesRepo instance.\"\"\" self . cloning_path = cloning_path try : self . repo = Repo ( self . cloning_path ) except NoSuchPathError or InvalidGitRepositoryError : self . repo = None # type: ignore logger . debug ( f \"`Cloning_path`: { self . cloning_path } was empty, \" \"Automatically cloning the examples.\" ) self . clone () self . checkout_latest_release () @property def active_version ( self ) -> Optional [ str ]: \"\"\"In case a release branch is checked out, this property returns that version, else None is returned\"\"\" for branch in self . repo . heads : branch_name = cast ( str , branch . name ) if ( branch_name . startswith ( \"release/\" ) and branch . commit == self . repo . head . commit ): return branch_name [ len ( \"release/\" ) :] return None @property def latest_release_branch ( self ) -> str : \"\"\"Returns the name of the latest release branch.\"\"\" tags = sorted ( self . repo . tags , key = lambda t : t . commit . committed_datetime , # type: ignore ) latest_tag = parse ( tags [ - 1 ] . name ) if type ( latest_tag ) is not Version : return \"main\" latest_release_version : str = tags [ - 1 ] . name return f \"release/ { latest_release_version } \" @property def is_cloned ( self ) -> bool : \"\"\"Returns whether we have already cloned the examples repository.\"\"\" return self . cloning_path . exists () @property def examples_dir ( self ) -> str : \"\"\"Returns the path for the examples directory.\"\"\" return os . path . join ( self . cloning_path , \"examples\" ) @property def examples_run_bash_script ( self ) -> str : \"\"\"Path to the bash script that runs the example.\"\"\" return os . path . join ( self . examples_dir , EXAMPLES_RUN_SCRIPT ) def clone ( self ) -> None : \"\"\"Clones repo to cloning_path. If you break off the operation with a `KeyBoardInterrupt` before the cloning is completed, this method will delete whatever was partially downloaded from your system.\"\"\" self . cloning_path . mkdir ( parents = True , exist_ok = False ) try : logger . info ( f \"Cloning repo { GIT_REPO_URL } to { self . cloning_path } \" ) self . repo = Repo . clone_from ( GIT_REPO_URL , self . cloning_path , branch = \"main\" ) except KeyboardInterrupt : self . delete () logger . error ( \"Canceled download of repository.. Rolled back.\" ) def delete ( self ) -> None : \"\"\"Delete `cloning_path` if it exists.\"\"\" if self . cloning_path . exists (): shutil . rmtree ( self . cloning_path ) else : raise AssertionError ( f \"Cannot delete the examples repository from \" f \" { self . cloning_path } as it does not exist.\" ) def checkout ( self , branch : str ) -> None : \"\"\"Checks out a specific branch or tag of the examples repository Raises: GitCommandError: if branch doesn't exist. \"\"\" logger . info ( f \"Checking out branch: { branch } \" ) self . repo . git . checkout ( branch ) def checkout_latest_release ( self ) -> None : \"\"\"Checks out the latest release of the examples repository.\"\"\" self . checkout ( branch = self . latest_release_branch ) active_version : Optional [ str ] property readonly In case a release branch is checked out, this property returns that version, else None is returned examples_dir : str property readonly Returns the path for the examples directory. examples_run_bash_script : str property readonly Path to the bash script that runs the example. is_cloned : bool property readonly Returns whether we have already cloned the examples repository. latest_release_branch : str property readonly Returns the name of the latest release branch. __init__ ( self , cloning_path ) special Create a new ExamplesRepo instance. Source code in zenml/cli/example.py def __init__ ( self , cloning_path : Path ) -> None : \"\"\"Create a new ExamplesRepo instance.\"\"\" self . cloning_path = cloning_path try : self . repo = Repo ( self . cloning_path ) except NoSuchPathError or InvalidGitRepositoryError : self . repo = None # type: ignore logger . debug ( f \"`Cloning_path`: { self . cloning_path } was empty, \" \"Automatically cloning the examples.\" ) self . clone () self . checkout_latest_release () checkout ( self , branch ) Checks out a specific branch or tag of the examples repository Exceptions: Type Description GitCommandError if branch doesn't exist. Source code in zenml/cli/example.py def checkout ( self , branch : str ) -> None : \"\"\"Checks out a specific branch or tag of the examples repository Raises: GitCommandError: if branch doesn't exist. \"\"\" logger . info ( f \"Checking out branch: { branch } \" ) self . repo . git . checkout ( branch ) checkout_latest_release ( self ) Checks out the latest release of the examples repository. Source code in zenml/cli/example.py def checkout_latest_release ( self ) -> None : \"\"\"Checks out the latest release of the examples repository.\"\"\" self . checkout ( branch = self . latest_release_branch ) clone ( self ) Clones repo to cloning_path. If you break off the operation with a KeyBoardInterrupt before the cloning is completed, this method will delete whatever was partially downloaded from your system. Source code in zenml/cli/example.py def clone ( self ) -> None : \"\"\"Clones repo to cloning_path. If you break off the operation with a `KeyBoardInterrupt` before the cloning is completed, this method will delete whatever was partially downloaded from your system.\"\"\" self . cloning_path . mkdir ( parents = True , exist_ok = False ) try : logger . info ( f \"Cloning repo { GIT_REPO_URL } to { self . cloning_path } \" ) self . repo = Repo . clone_from ( GIT_REPO_URL , self . cloning_path , branch = \"main\" ) except KeyboardInterrupt : self . delete () logger . error ( \"Canceled download of repository.. Rolled back.\" ) delete ( self ) Delete cloning_path if it exists. Source code in zenml/cli/example.py def delete ( self ) -> None : \"\"\"Delete `cloning_path` if it exists.\"\"\" if self . cloning_path . exists (): shutil . rmtree ( self . cloning_path ) else : raise AssertionError ( f \"Cannot delete the examples repository from \" f \" { self . cloning_path } as it does not exist.\" ) GitExamplesHandler Class for the GitExamplesHandler that interfaces with the CLI tool. Source code in zenml/cli/example.py class GitExamplesHandler ( object ): \"\"\"Class for the GitExamplesHandler that interfaces with the CLI tool.\"\"\" def __init__ ( self ) -> None : \"\"\"Create a new GitExamplesHandler instance.\"\"\" self . repo_dir = zenml . io . utils . get_global_config_directory () self . examples_dir = Path ( os . path . join ( self . repo_dir , EXAMPLES_GITHUB_REPO ) ) self . examples_repo = ExamplesRepo ( self . examples_dir ) @property def examples ( self ) -> List [ Example ]: \"\"\"Property that contains a list of examples\"\"\" return [ Example ( name , Path ( os . path . join ( self . examples_repo . examples_dir , name )) ) for name in sorted ( os . listdir ( self . examples_repo . examples_dir )) if ( not name . startswith ( \".\" ) and not name . startswith ( \"__\" ) and not name . startswith ( \"README\" ) and not name . endswith ( \".sh\" ) ) ] @property def is_matching_versions ( self ) -> bool : \"\"\"Returns a boolean whether the checked out examples are on the same code version as zenml\"\"\" return zenml_version_installed == str ( self . examples_repo . active_version ) def is_example ( self , example_name : Optional [ str ] = None ) -> bool : \"\"\"Checks if the supplied example_name corresponds to an example\"\"\" example_dict = { e . name : e for e in self . examples } if example_name : if example_name in example_dict . keys (): return True return False def get_examples ( self , example_name : Optional [ str ] = None ) -> List [ Example ]: \"\"\"Method that allows you to get an example by name. If no example is supplied, all examples are returned Args: example_name: Name of an example. \"\"\" example_dict = { e . name : e for e in self . examples } if example_name : if example_name in example_dict . keys (): return [ example_dict [ example_name ]] else : raise KeyError ( f \"Example { example_name } does not exist! \" f \"Available examples: { [ example_dict . keys ()] } \" ) else : return self . examples def pull ( self , branch : str , force : bool = False , ) -> None : \"\"\"Pulls the examples from the main git examples repository.\"\"\" if not self . examples_repo . is_cloned : self . examples_repo . clone () elif force : self . examples_repo . delete () self . examples_repo . clone () try : self . examples_repo . checkout ( branch = branch ) except GitCommandError : warning ( f \"The specified branch { branch } not found in \" \"repo, falling back to the latest release.\" ) self . examples_repo . checkout_latest_release () def pull_latest_examples ( self ) -> None : \"\"\"Pulls the latest examples from the examples repository.\"\"\" self . pull ( branch = self . examples_repo . latest_release_branch , force = True ) def copy_example ( self , example : Example , destination_dir : str ) -> None : \"\"\"Copies an example to the destination_dir.\"\"\" fileio . create_dir_if_not_exists ( destination_dir ) fileio . copy_dir ( str ( example . path_in_repo ), destination_dir , overwrite = True ) def clean_current_examples ( self ) -> None : \"\"\"Deletes the ZenML examples directory from your current working directory.\"\"\" examples_directory = os . path . join ( os . getcwd (), \"zenml_examples\" ) shutil . rmtree ( examples_directory ) examples : List [ zenml . cli . example . Example ] property readonly Property that contains a list of examples is_matching_versions : bool property readonly Returns a boolean whether the checked out examples are on the same code version as zenml __init__ ( self ) special Create a new GitExamplesHandler instance. Source code in zenml/cli/example.py def __init__ ( self ) -> None : \"\"\"Create a new GitExamplesHandler instance.\"\"\" self . repo_dir = zenml . io . utils . get_global_config_directory () self . examples_dir = Path ( os . path . join ( self . repo_dir , EXAMPLES_GITHUB_REPO ) ) self . examples_repo = ExamplesRepo ( self . examples_dir ) clean_current_examples ( self ) Deletes the ZenML examples directory from your current working directory. Source code in zenml/cli/example.py def clean_current_examples ( self ) -> None : \"\"\"Deletes the ZenML examples directory from your current working directory.\"\"\" examples_directory = os . path . join ( os . getcwd (), \"zenml_examples\" ) shutil . rmtree ( examples_directory ) copy_example ( self , example , destination_dir ) Copies an example to the destination_dir. Source code in zenml/cli/example.py def copy_example ( self , example : Example , destination_dir : str ) -> None : \"\"\"Copies an example to the destination_dir.\"\"\" fileio . create_dir_if_not_exists ( destination_dir ) fileio . copy_dir ( str ( example . path_in_repo ), destination_dir , overwrite = True ) get_examples ( self , example_name = None ) Method that allows you to get an example by name. If no example is supplied, all examples are returned Parameters: Name Type Description Default example_name Optional[str] Name of an example. None Source code in zenml/cli/example.py def get_examples ( self , example_name : Optional [ str ] = None ) -> List [ Example ]: \"\"\"Method that allows you to get an example by name. If no example is supplied, all examples are returned Args: example_name: Name of an example. \"\"\" example_dict = { e . name : e for e in self . examples } if example_name : if example_name in example_dict . keys (): return [ example_dict [ example_name ]] else : raise KeyError ( f \"Example { example_name } does not exist! \" f \"Available examples: { [ example_dict . keys ()] } \" ) else : return self . examples is_example ( self , example_name = None ) Checks if the supplied example_name corresponds to an example Source code in zenml/cli/example.py def is_example ( self , example_name : Optional [ str ] = None ) -> bool : \"\"\"Checks if the supplied example_name corresponds to an example\"\"\" example_dict = { e . name : e for e in self . examples } if example_name : if example_name in example_dict . keys (): return True return False pull ( self , branch , force = False ) Pulls the examples from the main git examples repository. Source code in zenml/cli/example.py def pull ( self , branch : str , force : bool = False , ) -> None : \"\"\"Pulls the examples from the main git examples repository.\"\"\" if not self . examples_repo . is_cloned : self . examples_repo . clone () elif force : self . examples_repo . delete () self . examples_repo . clone () try : self . examples_repo . checkout ( branch = branch ) except GitCommandError : warning ( f \"The specified branch { branch } not found in \" \"repo, falling back to the latest release.\" ) self . examples_repo . checkout_latest_release () pull_latest_examples ( self ) Pulls the latest examples from the examples repository. Source code in zenml/cli/example.py def pull_latest_examples ( self ) -> None : \"\"\"Pulls the latest examples from the examples repository.\"\"\" self . pull ( branch = self . examples_repo . latest_release_branch , force = True ) LocalExample Class to encapsulate all properties and methods of the local example that can be run from the CLI Source code in zenml/cli/example.py class LocalExample : \"\"\"Class to encapsulate all properties and methods of the local example that can be run from the CLI\"\"\" def __init__ ( self , path : Path , name : str ) -> None : \"\"\"Create a new LocalExample instance. Args: name: The name of the example, specifically the name of the folder on git path: Path at which the example is installed \"\"\" self . name = name self . path = path @property def python_files_in_dir ( self ) -> List [ str ]: \"\"\"List of all python files in the drectl in local example directory the __init__.py file is excluded from this list\"\"\" py_in_dir = fileio . find_files ( str ( self . path ), \"*.py\" ) py_files = [] for file in py_in_dir : # Make sure only files directly in dir are considered, not files # in sub dirs if self . path == Path ( file ) . parent : if Path ( file ) . name != \"__init__.py\" : py_files . append ( file ) return py_files @property def has_single_python_file ( self ) -> bool : \"\"\"Boolean that states if only one python file is present\"\"\" return len ( self . python_files_in_dir ) == 1 @property def has_any_python_file ( self ) -> bool : \"\"\"Boolean that states if any python file is present\"\"\" return len ( self . python_files_in_dir ) > 0 @property def executable_python_example ( self ) -> str : \"\"\"Return the python file for the example\"\"\" if self . has_single_python_file : return self . python_files_in_dir [ 0 ] elif self . has_any_python_file : logger . warning ( \"This example has multiple executable python files. \" \"The last one in alphanumerical order is taken.\" ) return sorted ( self . python_files_in_dir )[ - 1 ] else : raise RuntimeError ( \"No pipeline runner script found in example. \" f \"Files found: { self . python_files_in_dir } \" ) def is_present ( self ) -> bool : \"\"\"Checks if the example is installed at the given path.\"\"\" return fileio . file_exists ( str ( self . path )) and fileio . is_dir ( str ( self . path ) ) def run_example ( self , example_runner : List [ str ], force : bool ) -> None : \"\"\"Run the local example using the bash script at the supplied location Args: example_runner: Sequence of locations of executable file(s) to run the example force: Whether to force the install \"\"\" if all ( map ( fileio . file_exists , example_runner )): call = ( example_runner + [ \"--executable\" , self . executable_python_example ] + [ \"-f\" ] * force ) try : # TODO [ENG-271]: Catch errors that might be thrown # in subprocess subprocess . check_call ( call , cwd = str ( self . path ), shell = click . _compat . WIN ) except RuntimeError : raise NotImplementedError ( f \"Currently the example { self . name } \" \"has no implementation for the \" \"run method\" ) except subprocess . CalledProcessError as e : if e . returncode == 38 : raise NotImplementedError ( f \"Currently the example { self . name } \" \"has no implementation for the \" \"run method\" ) else : raise FileNotFoundError ( \"Bash File(s) to run Examples not found at\" f \" { example_runner } \" ) # Telemetry track_event ( RUN_EXAMPLE , { \"name\" : self . name }) executable_python_example : str property readonly Return the python file for the example has_any_python_file : bool property readonly Boolean that states if any python file is present has_single_python_file : bool property readonly Boolean that states if only one python file is present python_files_in_dir : List [ str ] property readonly List of all python files in the drectl in local example directory the init .py file is excluded from this list __init__ ( self , path , name ) special Create a new LocalExample instance. Parameters: Name Type Description Default name str The name of the example, specifically the name of the folder on git required path Path Path at which the example is installed required Source code in zenml/cli/example.py def __init__ ( self , path : Path , name : str ) -> None : \"\"\"Create a new LocalExample instance. Args: name: The name of the example, specifically the name of the folder on git path: Path at which the example is installed \"\"\" self . name = name self . path = path is_present ( self ) Checks if the example is installed at the given path. Source code in zenml/cli/example.py def is_present ( self ) -> bool : \"\"\"Checks if the example is installed at the given path.\"\"\" return fileio . file_exists ( str ( self . path )) and fileio . is_dir ( str ( self . path ) ) run_example ( self , example_runner , force ) Run the local example using the bash script at the supplied location Parameters: Name Type Description Default example_runner List[str] Sequence of locations of executable file(s) to run the example required force bool Whether to force the install required Source code in zenml/cli/example.py def run_example ( self , example_runner : List [ str ], force : bool ) -> None : \"\"\"Run the local example using the bash script at the supplied location Args: example_runner: Sequence of locations of executable file(s) to run the example force: Whether to force the install \"\"\" if all ( map ( fileio . file_exists , example_runner )): call = ( example_runner + [ \"--executable\" , self . executable_python_example ] + [ \"-f\" ] * force ) try : # TODO [ENG-271]: Catch errors that might be thrown # in subprocess subprocess . check_call ( call , cwd = str ( self . path ), shell = click . _compat . WIN ) except RuntimeError : raise NotImplementedError ( f \"Currently the example { self . name } \" \"has no implementation for the \" \"run method\" ) except subprocess . CalledProcessError as e : if e . returncode == 38 : raise NotImplementedError ( f \"Currently the example { self . name } \" \"has no implementation for the \" \"run method\" ) else : raise FileNotFoundError ( \"Bash File(s) to run Examples not found at\" f \" { example_runner } \" ) # Telemetry track_event ( RUN_EXAMPLE , { \"name\" : self . name }) check_for_version_mismatch ( git_examples_handler ) Prints a warning if the example version and ZenML version don't match. Source code in zenml/cli/example.py def check_for_version_mismatch ( git_examples_handler : GitExamplesHandler , ) -> None : \"\"\"Prints a warning if the example version and ZenML version don't match.\"\"\" if git_examples_handler . is_matching_versions : return else : if git_examples_handler . examples_repo . active_version : warning ( \"The examples you have installed are installed with Version \" f \" { git_examples_handler . examples_repo . active_version } \" f \"of ZenML. However your code is at { zenml_version_installed } \" \"Consider using `zenml example pull` to download \" \"examples matching your zenml installation.\" ) else : warning ( \"The examples you have installed are downloaded from a \" \"development branch of ZenML. Full functionality is not \" \"guaranteed. Use `zenml example pull` to \" \"get examples using your zenml version.\" )","title":"Example"},{"location":"api_docs/cli/example/#example","text":"","title":"Example"},{"location":"api_docs/cli/example/#zenml.cli.example","text":"","title":"example"},{"location":"api_docs/cli/example/#zenml.cli.example.Example","text":"Class for all example objects. Source code in zenml/cli/example.py class Example : \"\"\"Class for all example objects.\"\"\" def __init__ ( self , name : str , path_in_repo : Path ) -> None : \"\"\"Create a new Example instance. Args: name: The name of the example, specifically the name of the folder on git path_in_repo: Path to the local example within the global zenml folder. \"\"\" self . name = name self . path_in_repo = path_in_repo @property def readme_content ( self ) -> str : \"\"\"Returns the readme content associated with a particular example.\"\"\" readme_file = os . path . join ( self . path_in_repo , \"README.md\" ) try : with open ( readme_file ) as readme : readme_content = readme . read () return readme_content except FileNotFoundError : if fileio . file_exists ( str ( self . path_in_repo )) and fileio . is_dir ( str ( self . path_in_repo ) ): raise ValueError ( f \"No README.md file found in \" f \" { self . path_in_repo } \" ) else : raise FileNotFoundError ( f \"Example { self . name } is not one of the available options.\" f \" \\n \" f \"To list all available examples, type: `zenml example \" f \"list`\" )","title":"Example"},{"location":"api_docs/cli/example/#zenml.cli.example.Example.readme_content","text":"Returns the readme content associated with a particular example.","title":"readme_content"},{"location":"api_docs/cli/example/#zenml.cli.example.Example.__init__","text":"Create a new Example instance. Parameters: Name Type Description Default name str The name of the example, specifically the name of the folder on git required path_in_repo Path Path to the local example within the global zenml folder. required Source code in zenml/cli/example.py def __init__ ( self , name : str , path_in_repo : Path ) -> None : \"\"\"Create a new Example instance. Args: name: The name of the example, specifically the name of the folder on git path_in_repo: Path to the local example within the global zenml folder. \"\"\" self . name = name self . path_in_repo = path_in_repo","title":"__init__()"},{"location":"api_docs/cli/example/#zenml.cli.example.ExamplesRepo","text":"Class for the examples repository object. Source code in zenml/cli/example.py class ExamplesRepo : \"\"\"Class for the examples repository object.\"\"\" def __init__ ( self , cloning_path : Path ) -> None : \"\"\"Create a new ExamplesRepo instance.\"\"\" self . cloning_path = cloning_path try : self . repo = Repo ( self . cloning_path ) except NoSuchPathError or InvalidGitRepositoryError : self . repo = None # type: ignore logger . debug ( f \"`Cloning_path`: { self . cloning_path } was empty, \" \"Automatically cloning the examples.\" ) self . clone () self . checkout_latest_release () @property def active_version ( self ) -> Optional [ str ]: \"\"\"In case a release branch is checked out, this property returns that version, else None is returned\"\"\" for branch in self . repo . heads : branch_name = cast ( str , branch . name ) if ( branch_name . startswith ( \"release/\" ) and branch . commit == self . repo . head . commit ): return branch_name [ len ( \"release/\" ) :] return None @property def latest_release_branch ( self ) -> str : \"\"\"Returns the name of the latest release branch.\"\"\" tags = sorted ( self . repo . tags , key = lambda t : t . commit . committed_datetime , # type: ignore ) latest_tag = parse ( tags [ - 1 ] . name ) if type ( latest_tag ) is not Version : return \"main\" latest_release_version : str = tags [ - 1 ] . name return f \"release/ { latest_release_version } \" @property def is_cloned ( self ) -> bool : \"\"\"Returns whether we have already cloned the examples repository.\"\"\" return self . cloning_path . exists () @property def examples_dir ( self ) -> str : \"\"\"Returns the path for the examples directory.\"\"\" return os . path . join ( self . cloning_path , \"examples\" ) @property def examples_run_bash_script ( self ) -> str : \"\"\"Path to the bash script that runs the example.\"\"\" return os . path . join ( self . examples_dir , EXAMPLES_RUN_SCRIPT ) def clone ( self ) -> None : \"\"\"Clones repo to cloning_path. If you break off the operation with a `KeyBoardInterrupt` before the cloning is completed, this method will delete whatever was partially downloaded from your system.\"\"\" self . cloning_path . mkdir ( parents = True , exist_ok = False ) try : logger . info ( f \"Cloning repo { GIT_REPO_URL } to { self . cloning_path } \" ) self . repo = Repo . clone_from ( GIT_REPO_URL , self . cloning_path , branch = \"main\" ) except KeyboardInterrupt : self . delete () logger . error ( \"Canceled download of repository.. Rolled back.\" ) def delete ( self ) -> None : \"\"\"Delete `cloning_path` if it exists.\"\"\" if self . cloning_path . exists (): shutil . rmtree ( self . cloning_path ) else : raise AssertionError ( f \"Cannot delete the examples repository from \" f \" { self . cloning_path } as it does not exist.\" ) def checkout ( self , branch : str ) -> None : \"\"\"Checks out a specific branch or tag of the examples repository Raises: GitCommandError: if branch doesn't exist. \"\"\" logger . info ( f \"Checking out branch: { branch } \" ) self . repo . git . checkout ( branch ) def checkout_latest_release ( self ) -> None : \"\"\"Checks out the latest release of the examples repository.\"\"\" self . checkout ( branch = self . latest_release_branch )","title":"ExamplesRepo"},{"location":"api_docs/cli/example/#zenml.cli.example.ExamplesRepo.active_version","text":"In case a release branch is checked out, this property returns that version, else None is returned","title":"active_version"},{"location":"api_docs/cli/example/#zenml.cli.example.ExamplesRepo.examples_dir","text":"Returns the path for the examples directory.","title":"examples_dir"},{"location":"api_docs/cli/example/#zenml.cli.example.ExamplesRepo.examples_run_bash_script","text":"Path to the bash script that runs the example.","title":"examples_run_bash_script"},{"location":"api_docs/cli/example/#zenml.cli.example.ExamplesRepo.is_cloned","text":"Returns whether we have already cloned the examples repository.","title":"is_cloned"},{"location":"api_docs/cli/example/#zenml.cli.example.ExamplesRepo.latest_release_branch","text":"Returns the name of the latest release branch.","title":"latest_release_branch"},{"location":"api_docs/cli/example/#zenml.cli.example.ExamplesRepo.__init__","text":"Create a new ExamplesRepo instance. Source code in zenml/cli/example.py def __init__ ( self , cloning_path : Path ) -> None : \"\"\"Create a new ExamplesRepo instance.\"\"\" self . cloning_path = cloning_path try : self . repo = Repo ( self . cloning_path ) except NoSuchPathError or InvalidGitRepositoryError : self . repo = None # type: ignore logger . debug ( f \"`Cloning_path`: { self . cloning_path } was empty, \" \"Automatically cloning the examples.\" ) self . clone () self . checkout_latest_release ()","title":"__init__()"},{"location":"api_docs/cli/example/#zenml.cli.example.ExamplesRepo.checkout","text":"Checks out a specific branch or tag of the examples repository Exceptions: Type Description GitCommandError if branch doesn't exist. Source code in zenml/cli/example.py def checkout ( self , branch : str ) -> None : \"\"\"Checks out a specific branch or tag of the examples repository Raises: GitCommandError: if branch doesn't exist. \"\"\" logger . info ( f \"Checking out branch: { branch } \" ) self . repo . git . checkout ( branch )","title":"checkout()"},{"location":"api_docs/cli/example/#zenml.cli.example.ExamplesRepo.checkout_latest_release","text":"Checks out the latest release of the examples repository. Source code in zenml/cli/example.py def checkout_latest_release ( self ) -> None : \"\"\"Checks out the latest release of the examples repository.\"\"\" self . checkout ( branch = self . latest_release_branch )","title":"checkout_latest_release()"},{"location":"api_docs/cli/example/#zenml.cli.example.ExamplesRepo.clone","text":"Clones repo to cloning_path. If you break off the operation with a KeyBoardInterrupt before the cloning is completed, this method will delete whatever was partially downloaded from your system. Source code in zenml/cli/example.py def clone ( self ) -> None : \"\"\"Clones repo to cloning_path. If you break off the operation with a `KeyBoardInterrupt` before the cloning is completed, this method will delete whatever was partially downloaded from your system.\"\"\" self . cloning_path . mkdir ( parents = True , exist_ok = False ) try : logger . info ( f \"Cloning repo { GIT_REPO_URL } to { self . cloning_path } \" ) self . repo = Repo . clone_from ( GIT_REPO_URL , self . cloning_path , branch = \"main\" ) except KeyboardInterrupt : self . delete () logger . error ( \"Canceled download of repository.. Rolled back.\" )","title":"clone()"},{"location":"api_docs/cli/example/#zenml.cli.example.ExamplesRepo.delete","text":"Delete cloning_path if it exists. Source code in zenml/cli/example.py def delete ( self ) -> None : \"\"\"Delete `cloning_path` if it exists.\"\"\" if self . cloning_path . exists (): shutil . rmtree ( self . cloning_path ) else : raise AssertionError ( f \"Cannot delete the examples repository from \" f \" { self . cloning_path } as it does not exist.\" )","title":"delete()"},{"location":"api_docs/cli/example/#zenml.cli.example.GitExamplesHandler","text":"Class for the GitExamplesHandler that interfaces with the CLI tool. Source code in zenml/cli/example.py class GitExamplesHandler ( object ): \"\"\"Class for the GitExamplesHandler that interfaces with the CLI tool.\"\"\" def __init__ ( self ) -> None : \"\"\"Create a new GitExamplesHandler instance.\"\"\" self . repo_dir = zenml . io . utils . get_global_config_directory () self . examples_dir = Path ( os . path . join ( self . repo_dir , EXAMPLES_GITHUB_REPO ) ) self . examples_repo = ExamplesRepo ( self . examples_dir ) @property def examples ( self ) -> List [ Example ]: \"\"\"Property that contains a list of examples\"\"\" return [ Example ( name , Path ( os . path . join ( self . examples_repo . examples_dir , name )) ) for name in sorted ( os . listdir ( self . examples_repo . examples_dir )) if ( not name . startswith ( \".\" ) and not name . startswith ( \"__\" ) and not name . startswith ( \"README\" ) and not name . endswith ( \".sh\" ) ) ] @property def is_matching_versions ( self ) -> bool : \"\"\"Returns a boolean whether the checked out examples are on the same code version as zenml\"\"\" return zenml_version_installed == str ( self . examples_repo . active_version ) def is_example ( self , example_name : Optional [ str ] = None ) -> bool : \"\"\"Checks if the supplied example_name corresponds to an example\"\"\" example_dict = { e . name : e for e in self . examples } if example_name : if example_name in example_dict . keys (): return True return False def get_examples ( self , example_name : Optional [ str ] = None ) -> List [ Example ]: \"\"\"Method that allows you to get an example by name. If no example is supplied, all examples are returned Args: example_name: Name of an example. \"\"\" example_dict = { e . name : e for e in self . examples } if example_name : if example_name in example_dict . keys (): return [ example_dict [ example_name ]] else : raise KeyError ( f \"Example { example_name } does not exist! \" f \"Available examples: { [ example_dict . keys ()] } \" ) else : return self . examples def pull ( self , branch : str , force : bool = False , ) -> None : \"\"\"Pulls the examples from the main git examples repository.\"\"\" if not self . examples_repo . is_cloned : self . examples_repo . clone () elif force : self . examples_repo . delete () self . examples_repo . clone () try : self . examples_repo . checkout ( branch = branch ) except GitCommandError : warning ( f \"The specified branch { branch } not found in \" \"repo, falling back to the latest release.\" ) self . examples_repo . checkout_latest_release () def pull_latest_examples ( self ) -> None : \"\"\"Pulls the latest examples from the examples repository.\"\"\" self . pull ( branch = self . examples_repo . latest_release_branch , force = True ) def copy_example ( self , example : Example , destination_dir : str ) -> None : \"\"\"Copies an example to the destination_dir.\"\"\" fileio . create_dir_if_not_exists ( destination_dir ) fileio . copy_dir ( str ( example . path_in_repo ), destination_dir , overwrite = True ) def clean_current_examples ( self ) -> None : \"\"\"Deletes the ZenML examples directory from your current working directory.\"\"\" examples_directory = os . path . join ( os . getcwd (), \"zenml_examples\" ) shutil . rmtree ( examples_directory )","title":"GitExamplesHandler"},{"location":"api_docs/cli/example/#zenml.cli.example.GitExamplesHandler.examples","text":"Property that contains a list of examples","title":"examples"},{"location":"api_docs/cli/example/#zenml.cli.example.GitExamplesHandler.is_matching_versions","text":"Returns a boolean whether the checked out examples are on the same code version as zenml","title":"is_matching_versions"},{"location":"api_docs/cli/example/#zenml.cli.example.GitExamplesHandler.__init__","text":"Create a new GitExamplesHandler instance. Source code in zenml/cli/example.py def __init__ ( self ) -> None : \"\"\"Create a new GitExamplesHandler instance.\"\"\" self . repo_dir = zenml . io . utils . get_global_config_directory () self . examples_dir = Path ( os . path . join ( self . repo_dir , EXAMPLES_GITHUB_REPO ) ) self . examples_repo = ExamplesRepo ( self . examples_dir )","title":"__init__()"},{"location":"api_docs/cli/example/#zenml.cli.example.GitExamplesHandler.clean_current_examples","text":"Deletes the ZenML examples directory from your current working directory. Source code in zenml/cli/example.py def clean_current_examples ( self ) -> None : \"\"\"Deletes the ZenML examples directory from your current working directory.\"\"\" examples_directory = os . path . join ( os . getcwd (), \"zenml_examples\" ) shutil . rmtree ( examples_directory )","title":"clean_current_examples()"},{"location":"api_docs/cli/example/#zenml.cli.example.GitExamplesHandler.copy_example","text":"Copies an example to the destination_dir. Source code in zenml/cli/example.py def copy_example ( self , example : Example , destination_dir : str ) -> None : \"\"\"Copies an example to the destination_dir.\"\"\" fileio . create_dir_if_not_exists ( destination_dir ) fileio . copy_dir ( str ( example . path_in_repo ), destination_dir , overwrite = True )","title":"copy_example()"},{"location":"api_docs/cli/example/#zenml.cli.example.GitExamplesHandler.get_examples","text":"Method that allows you to get an example by name. If no example is supplied, all examples are returned Parameters: Name Type Description Default example_name Optional[str] Name of an example. None Source code in zenml/cli/example.py def get_examples ( self , example_name : Optional [ str ] = None ) -> List [ Example ]: \"\"\"Method that allows you to get an example by name. If no example is supplied, all examples are returned Args: example_name: Name of an example. \"\"\" example_dict = { e . name : e for e in self . examples } if example_name : if example_name in example_dict . keys (): return [ example_dict [ example_name ]] else : raise KeyError ( f \"Example { example_name } does not exist! \" f \"Available examples: { [ example_dict . keys ()] } \" ) else : return self . examples","title":"get_examples()"},{"location":"api_docs/cli/example/#zenml.cli.example.GitExamplesHandler.is_example","text":"Checks if the supplied example_name corresponds to an example Source code in zenml/cli/example.py def is_example ( self , example_name : Optional [ str ] = None ) -> bool : \"\"\"Checks if the supplied example_name corresponds to an example\"\"\" example_dict = { e . name : e for e in self . examples } if example_name : if example_name in example_dict . keys (): return True return False","title":"is_example()"},{"location":"api_docs/cli/example/#zenml.cli.example.GitExamplesHandler.pull","text":"Pulls the examples from the main git examples repository. Source code in zenml/cli/example.py def pull ( self , branch : str , force : bool = False , ) -> None : \"\"\"Pulls the examples from the main git examples repository.\"\"\" if not self . examples_repo . is_cloned : self . examples_repo . clone () elif force : self . examples_repo . delete () self . examples_repo . clone () try : self . examples_repo . checkout ( branch = branch ) except GitCommandError : warning ( f \"The specified branch { branch } not found in \" \"repo, falling back to the latest release.\" ) self . examples_repo . checkout_latest_release ()","title":"pull()"},{"location":"api_docs/cli/example/#zenml.cli.example.GitExamplesHandler.pull_latest_examples","text":"Pulls the latest examples from the examples repository. Source code in zenml/cli/example.py def pull_latest_examples ( self ) -> None : \"\"\"Pulls the latest examples from the examples repository.\"\"\" self . pull ( branch = self . examples_repo . latest_release_branch , force = True )","title":"pull_latest_examples()"},{"location":"api_docs/cli/example/#zenml.cli.example.LocalExample","text":"Class to encapsulate all properties and methods of the local example that can be run from the CLI Source code in zenml/cli/example.py class LocalExample : \"\"\"Class to encapsulate all properties and methods of the local example that can be run from the CLI\"\"\" def __init__ ( self , path : Path , name : str ) -> None : \"\"\"Create a new LocalExample instance. Args: name: The name of the example, specifically the name of the folder on git path: Path at which the example is installed \"\"\" self . name = name self . path = path @property def python_files_in_dir ( self ) -> List [ str ]: \"\"\"List of all python files in the drectl in local example directory the __init__.py file is excluded from this list\"\"\" py_in_dir = fileio . find_files ( str ( self . path ), \"*.py\" ) py_files = [] for file in py_in_dir : # Make sure only files directly in dir are considered, not files # in sub dirs if self . path == Path ( file ) . parent : if Path ( file ) . name != \"__init__.py\" : py_files . append ( file ) return py_files @property def has_single_python_file ( self ) -> bool : \"\"\"Boolean that states if only one python file is present\"\"\" return len ( self . python_files_in_dir ) == 1 @property def has_any_python_file ( self ) -> bool : \"\"\"Boolean that states if any python file is present\"\"\" return len ( self . python_files_in_dir ) > 0 @property def executable_python_example ( self ) -> str : \"\"\"Return the python file for the example\"\"\" if self . has_single_python_file : return self . python_files_in_dir [ 0 ] elif self . has_any_python_file : logger . warning ( \"This example has multiple executable python files. \" \"The last one in alphanumerical order is taken.\" ) return sorted ( self . python_files_in_dir )[ - 1 ] else : raise RuntimeError ( \"No pipeline runner script found in example. \" f \"Files found: { self . python_files_in_dir } \" ) def is_present ( self ) -> bool : \"\"\"Checks if the example is installed at the given path.\"\"\" return fileio . file_exists ( str ( self . path )) and fileio . is_dir ( str ( self . path ) ) def run_example ( self , example_runner : List [ str ], force : bool ) -> None : \"\"\"Run the local example using the bash script at the supplied location Args: example_runner: Sequence of locations of executable file(s) to run the example force: Whether to force the install \"\"\" if all ( map ( fileio . file_exists , example_runner )): call = ( example_runner + [ \"--executable\" , self . executable_python_example ] + [ \"-f\" ] * force ) try : # TODO [ENG-271]: Catch errors that might be thrown # in subprocess subprocess . check_call ( call , cwd = str ( self . path ), shell = click . _compat . WIN ) except RuntimeError : raise NotImplementedError ( f \"Currently the example { self . name } \" \"has no implementation for the \" \"run method\" ) except subprocess . CalledProcessError as e : if e . returncode == 38 : raise NotImplementedError ( f \"Currently the example { self . name } \" \"has no implementation for the \" \"run method\" ) else : raise FileNotFoundError ( \"Bash File(s) to run Examples not found at\" f \" { example_runner } \" ) # Telemetry track_event ( RUN_EXAMPLE , { \"name\" : self . name })","title":"LocalExample"},{"location":"api_docs/cli/example/#zenml.cli.example.LocalExample.executable_python_example","text":"Return the python file for the example","title":"executable_python_example"},{"location":"api_docs/cli/example/#zenml.cli.example.LocalExample.has_any_python_file","text":"Boolean that states if any python file is present","title":"has_any_python_file"},{"location":"api_docs/cli/example/#zenml.cli.example.LocalExample.has_single_python_file","text":"Boolean that states if only one python file is present","title":"has_single_python_file"},{"location":"api_docs/cli/example/#zenml.cli.example.LocalExample.python_files_in_dir","text":"List of all python files in the drectl in local example directory the init .py file is excluded from this list","title":"python_files_in_dir"},{"location":"api_docs/cli/example/#zenml.cli.example.LocalExample.__init__","text":"Create a new LocalExample instance. Parameters: Name Type Description Default name str The name of the example, specifically the name of the folder on git required path Path Path at which the example is installed required Source code in zenml/cli/example.py def __init__ ( self , path : Path , name : str ) -> None : \"\"\"Create a new LocalExample instance. Args: name: The name of the example, specifically the name of the folder on git path: Path at which the example is installed \"\"\" self . name = name self . path = path","title":"__init__()"},{"location":"api_docs/cli/example/#zenml.cli.example.LocalExample.is_present","text":"Checks if the example is installed at the given path. Source code in zenml/cli/example.py def is_present ( self ) -> bool : \"\"\"Checks if the example is installed at the given path.\"\"\" return fileio . file_exists ( str ( self . path )) and fileio . is_dir ( str ( self . path ) )","title":"is_present()"},{"location":"api_docs/cli/example/#zenml.cli.example.LocalExample.run_example","text":"Run the local example using the bash script at the supplied location Parameters: Name Type Description Default example_runner List[str] Sequence of locations of executable file(s) to run the example required force bool Whether to force the install required Source code in zenml/cli/example.py def run_example ( self , example_runner : List [ str ], force : bool ) -> None : \"\"\"Run the local example using the bash script at the supplied location Args: example_runner: Sequence of locations of executable file(s) to run the example force: Whether to force the install \"\"\" if all ( map ( fileio . file_exists , example_runner )): call = ( example_runner + [ \"--executable\" , self . executable_python_example ] + [ \"-f\" ] * force ) try : # TODO [ENG-271]: Catch errors that might be thrown # in subprocess subprocess . check_call ( call , cwd = str ( self . path ), shell = click . _compat . WIN ) except RuntimeError : raise NotImplementedError ( f \"Currently the example { self . name } \" \"has no implementation for the \" \"run method\" ) except subprocess . CalledProcessError as e : if e . returncode == 38 : raise NotImplementedError ( f \"Currently the example { self . name } \" \"has no implementation for the \" \"run method\" ) else : raise FileNotFoundError ( \"Bash File(s) to run Examples not found at\" f \" { example_runner } \" ) # Telemetry track_event ( RUN_EXAMPLE , { \"name\" : self . name })","title":"run_example()"},{"location":"api_docs/cli/example/#zenml.cli.example.check_for_version_mismatch","text":"Prints a warning if the example version and ZenML version don't match. Source code in zenml/cli/example.py def check_for_version_mismatch ( git_examples_handler : GitExamplesHandler , ) -> None : \"\"\"Prints a warning if the example version and ZenML version don't match.\"\"\" if git_examples_handler . is_matching_versions : return else : if git_examples_handler . examples_repo . active_version : warning ( \"The examples you have installed are installed with Version \" f \" { git_examples_handler . examples_repo . active_version } \" f \"of ZenML. However your code is at { zenml_version_installed } \" \"Consider using `zenml example pull` to download \" \"examples matching your zenml installation.\" ) else : warning ( \"The examples you have installed are downloaded from a \" \"development branch of ZenML. Full functionality is not \" \"guaranteed. Use `zenml example pull` to \" \"get examples using your zenml version.\" )","title":"check_for_version_mismatch()"},{"location":"api_docs/cli/integration/","text":"Integration zenml.cli.integration","title":"Integration"},{"location":"api_docs/cli/integration/#integration","text":"","title":"Integration"},{"location":"api_docs/cli/integration/#zenml.cli.integration","text":"","title":"integration"},{"location":"api_docs/cli/pipeline/","text":"Pipeline zenml.cli.pipeline CLI to interact with pipelines.","title":"Pipeline"},{"location":"api_docs/cli/pipeline/#pipeline","text":"","title":"Pipeline"},{"location":"api_docs/cli/pipeline/#zenml.cli.pipeline","text":"CLI to interact with pipelines.","title":"pipeline"},{"location":"api_docs/cli/stack/","text":"Stack zenml.cli.stack CLI for manipulating ZenML local and global config file.","title":"Stack"},{"location":"api_docs/cli/stack/#stack","text":"","title":"Stack"},{"location":"api_docs/cli/stack/#zenml.cli.stack","text":"CLI for manipulating ZenML local and global config file.","title":"stack"},{"location":"api_docs/cli/stack_components/","text":"Stack Components zenml.cli.stack_components generate_stack_component_delete_command ( component_type ) Generates a delete command for the specific stack component type. Source code in zenml/cli/stack_components.py def generate_stack_component_delete_command ( component_type : StackComponentType , ) -> Callable [[ str ], None ]: \"\"\"Generates a `delete` command for the specific stack component type.\"\"\" @click . argument ( \"name\" , type = str ) def delete_stack_component_command ( name : str ) -> None : \"\"\"Deletes a stack component.\"\"\" Repository () . deregister_stack_component ( component_type = component_type , name = name , ) display_name = _component_display_name ( component_type ) cli_utils . declare ( f \"Deleted { display_name } : { name } \" ) return delete_stack_component_command generate_stack_component_describe_command ( component_type ) Generates a describe command for the specific stack component type. Source code in zenml/cli/stack_components.py def generate_stack_component_describe_command ( component_type : StackComponentType , ) -> Callable [[ Optional [ str ]], None ]: \"\"\"Generates a `describe` command for the specific stack component type.\"\"\" @click . argument ( \"name\" , type = str , required = False , ) @cli_utils . activate_integrations def describe_stack_component_command ( name : Optional [ str ]) -> None : \"\"\"Prints details about the active/specified component.\"\"\" singular_display_name = _component_display_name ( component_type ) plural_display_name = _component_display_name ( component_type , plural = True ) repo = Repository () components = repo . get_stack_components ( component_type ) if len ( components ) == 0 : cli_utils . warning ( f \"No { plural_display_name } registered.\" ) return active_component_name = repo . active_stack . components [ component_type ] . name component = _get_stack_component ( component_type , component_name = name ) is_active = active_component_name == component . name cli_utils . title ( f \" { singular_display_name } :\" ) cli_utils . declare ( \"**ACTIVE** \\n \" if is_active else \"\" ) cli_utils . print_stack_component_configuration ( component ) return describe_stack_component_command generate_stack_component_down_command ( component_type ) Generates a down command for the specific stack component type. Source code in zenml/cli/stack_components.py def generate_stack_component_down_command ( component_type : StackComponentType , ) -> Callable [[ Optional [ str ], bool ], None ]: \"\"\"Generates a `down` command for the specific stack component type.\"\"\" @click . argument ( \"name\" , type = str , required = False ) @click . option ( \"--force\" , \"-f\" , is_flag = True , help = \"Deprovisions local resources instead of suspending them.\" , ) def down_stack_component_command ( name : Optional [ str ] = None , force : bool = False ) -> None : \"\"\"Stops/Tears down the local deployment of a stack component.\"\"\" component = _get_stack_component ( component_type , component_name = name ) display_name = _component_display_name ( component_type ) if component . is_running and not force : cli_utils . declare ( f \"Suspending local resources for { display_name } \" f \"' { component . name } '.\" ) try : component . suspend () except NotImplementedError : cli_utils . error ( f \"Provisioning local resources not implemented for \" f \" { display_name } ' { component . name } '. If you want to \" f \"deprovision all resources for this component, use the \" f \"`--force/-f` flag.\" ) elif component . is_provisioned and force : cli_utils . declare ( f \"Deprovisioning resources for { display_name } \" f \"' { component . name } '.\" ) component . deprovision () else : cli_utils . declare ( f \"No provisioned resources found for { display_name } \" f \"' { component . name } '.\" ) return down_stack_component_command generate_stack_component_get_command ( component_type ) Generates a get command for the specific stack component type. Source code in zenml/cli/stack_components.py def generate_stack_component_get_command ( component_type : StackComponentType , ) -> Callable [[], None ]: \"\"\"Generates a `get` command for the specific stack component type.\"\"\" @cli_utils . activate_integrations def get_stack_component_command () -> None : \"\"\"Prints the name of the active component.\"\"\" active_stack = Repository () . active_stack component = active_stack . components . get ( component_type , None ) display_name = _component_display_name ( component_type ) if component : cli_utils . declare ( f \"Active { display_name } : { component . name } \" ) else : cli_utils . warning ( f \"No { display_name } set for active stack ( { active_stack . name } ).\" ) return get_stack_component_command generate_stack_component_list_command ( component_type ) Generates a list command for the specific stack component type. Source code in zenml/cli/stack_components.py def generate_stack_component_list_command ( component_type : StackComponentType , ) -> Callable [[], None ]: \"\"\"Generates a `list` command for the specific stack component type.\"\"\" @cli_utils . activate_integrations def list_stack_components_command () -> None : \"\"\"Prints a table of stack components.\"\"\" repo = Repository () components = repo . get_stack_components ( component_type ) display_name = _component_display_name ( component_type , plural = True ) if len ( components ) == 0 : cli_utils . warning ( f \"No { display_name } registered.\" ) return active_component_name = repo . active_stack . components [ component_type ] . name cli_utils . title ( f \" { display_name } :\" ) cli_utils . print_stack_component_list ( components , active_component_name = active_component_name ) return list_stack_components_command generate_stack_component_logs_command ( component_type ) Generates a logs command for the specific stack component type. Source code in zenml/cli/stack_components.py def generate_stack_component_logs_command ( component_type : StackComponentType , ) -> Callable [[ Optional [ str ], bool ], None ]: \"\"\"Generates a `logs` command for the specific stack component type.\"\"\" @click . argument ( \"name\" , type = str , required = False ) @click . option ( \"--follow\" , \"-f\" , is_flag = True , help = \"Follow the log file instead of just displaying the current logs.\" , ) def stack_component_logs_command ( name : Optional [ str ] = None , follow : bool = False ) -> None : \"\"\"Displays stack component logs.\"\"\" component = _get_stack_component ( component_type , component_name = name ) display_name = _component_display_name ( component_type ) log_file = component . log_file if not log_file or not fileio . file_exists ( log_file ): cli_utils . warning ( f \"Unable to find log file for { display_name } \" f \"' { component . name } '.\" ) return if follow : try : with open ( log_file , \"r\" ) as f : # seek to the end of the file f . seek ( 0 , 2 ) while True : line = f . readline () if not line : time . sleep ( 0.1 ) continue line = line . rstrip ( \" \\n \" ) click . echo ( line ) except KeyboardInterrupt : cli_utils . declare ( f \"Stopped following { display_name } logs.\" ) else : with open ( log_file , \"r\" ) as f : click . echo ( f . read ()) return stack_component_logs_command generate_stack_component_register_command ( component_type ) Generates a register command for the specific stack component type. Source code in zenml/cli/stack_components.py def generate_stack_component_register_command ( component_type : StackComponentType , ) -> Callable [[ str , str , List [ str ]], None ]: \"\"\"Generates a `register` command for the specific stack component type.\"\"\" display_name = _component_display_name ( component_type ) @click . argument ( \"name\" , type = str , required = True , ) @click . option ( \"--type\" , \"-t\" , \"flavor\" , help = f \"The type of the { display_name } to register.\" , required = True , type = str , ) @click . argument ( \"args\" , nargs =- 1 , type = click . UNPROCESSED ) @cli_utils . activate_integrations def register_stack_component_command ( name : str , flavor : str , args : List [ str ] ) -> None : \"\"\"Registers a stack component.\"\"\" try : parsed_args = cli_utils . parse_unknown_options ( args ) except AssertionError as e : cli_utils . error ( str ( e )) return from zenml.stack.stack_component_class_registry import ( StackComponentClassRegistry , ) component_class = StackComponentClassRegistry . get_class ( component_type = component_type , component_flavor = flavor ) component = component_class ( name = name , ** parsed_args ) Repository () . register_stack_component ( component ) cli_utils . declare ( f \"Successfully registered { display_name } ` { name } `.\" ) return register_stack_component_command generate_stack_component_up_command ( component_type ) Generates a up command for the specific stack component type. Source code in zenml/cli/stack_components.py def generate_stack_component_up_command ( component_type : StackComponentType , ) -> Callable [[ Optional [ str ]], None ]: \"\"\"Generates a `up` command for the specific stack component type.\"\"\" @click . argument ( \"name\" , type = str , required = False ) def up_stack_component_command ( name : Optional [ str ] = None ) -> None : \"\"\"Deploys a stack component locally.\"\"\" component = _get_stack_component ( component_type , component_name = name ) display_name = _component_display_name ( component_type ) if component . is_running : cli_utils . declare ( f \"Local deployment is already running for { display_name } \" f \"' { component . name } '.\" ) return if not component . is_provisioned : cli_utils . declare ( f \"Provisioning local resources for { display_name } \" f \"' { component . name } '.\" ) try : component . provision () except NotImplementedError : cli_utils . error ( f \"Provisioning local resources not implemented for \" f \" { display_name } ' { component . name } '.\" ) if not component . is_running : cli_utils . declare ( f \"Resuming local resources for { display_name } \" f \"' { component . name } '.\" ) component . resume () return up_stack_component_command register_all_stack_component_cli_commands () Registers CLI commands for all stack components. Source code in zenml/cli/stack_components.py def register_all_stack_component_cli_commands () -> None : \"\"\"Registers CLI commands for all stack components.\"\"\" for component_type in StackComponentType : register_single_stack_component_cli_commands ( component_type , parent_group = cli ) register_single_stack_component_cli_commands ( component_type , parent_group ) Registers all basic stack component CLI commands. Source code in zenml/cli/stack_components.py def register_single_stack_component_cli_commands ( component_type : StackComponentType , parent_group : click . Group ) -> None : \"\"\"Registers all basic stack component CLI commands.\"\"\" command_name = component_type . value . replace ( \"_\" , \"-\" ) singular_display_name = _component_display_name ( component_type ) plural_display_name = _component_display_name ( component_type , plural = True ) @parent_group . group ( command_name , help = f \"Commands to interact with { plural_display_name } .\" ) def command_group () -> None : \"\"\"Group commands for a single stack component type.\"\"\" # zenml stack-component get get_command = generate_stack_component_get_command ( component_type ) command_group . command ( \"get\" , help = f \"Get the name of the active { singular_display_name } .\" )( get_command ) # zenml stack-component describe describe_command = generate_stack_component_describe_command ( component_type ) command_group . command ( \"describe\" , help = f \"Show details about the (active) { singular_display_name } .\" , )( describe_command ) # zenml stack-component list list_command = generate_stack_component_list_command ( component_type ) command_group . command ( \"list\" , help = f \"List all registered { plural_display_name } .\" )( list_command ) # zenml stack-component register register_command = generate_stack_component_register_command ( component_type ) context_settings = { \"ignore_unknown_options\" : True } command_group . command ( \"register\" , context_settings = context_settings , help = f \"Register a new { singular_display_name } .\" , )( register_command ) # zenml stack-component delete delete_command = generate_stack_component_delete_command ( component_type ) command_group . command ( \"delete\" , help = f \"Delete a registered { singular_display_name } .\" )( delete_command ) # zenml stack-component up up_command = generate_stack_component_up_command ( component_type ) command_group . command ( \"up\" , help = f \"Deploys the { singular_display_name } locally if possible.\" )( up_command ) # zenml stack-component down down_command = generate_stack_component_down_command ( component_type ) command_group . command ( \"down\" , help = f \"Stops the local { singular_display_name } deployment.\" )( down_command ) # zenml stack-component logs logs_command = generate_stack_component_logs_command ( component_type ) command_group . command ( \"logs\" , help = f \"Display { singular_display_name } logs.\" )( logs_command )","title":"Stack Components"},{"location":"api_docs/cli/stack_components/#stack-components","text":"","title":"Stack Components"},{"location":"api_docs/cli/stack_components/#zenml.cli.stack_components","text":"","title":"stack_components"},{"location":"api_docs/cli/stack_components/#zenml.cli.stack_components.generate_stack_component_delete_command","text":"Generates a delete command for the specific stack component type. Source code in zenml/cli/stack_components.py def generate_stack_component_delete_command ( component_type : StackComponentType , ) -> Callable [[ str ], None ]: \"\"\"Generates a `delete` command for the specific stack component type.\"\"\" @click . argument ( \"name\" , type = str ) def delete_stack_component_command ( name : str ) -> None : \"\"\"Deletes a stack component.\"\"\" Repository () . deregister_stack_component ( component_type = component_type , name = name , ) display_name = _component_display_name ( component_type ) cli_utils . declare ( f \"Deleted { display_name } : { name } \" ) return delete_stack_component_command","title":"generate_stack_component_delete_command()"},{"location":"api_docs/cli/stack_components/#zenml.cli.stack_components.generate_stack_component_describe_command","text":"Generates a describe command for the specific stack component type. Source code in zenml/cli/stack_components.py def generate_stack_component_describe_command ( component_type : StackComponentType , ) -> Callable [[ Optional [ str ]], None ]: \"\"\"Generates a `describe` command for the specific stack component type.\"\"\" @click . argument ( \"name\" , type = str , required = False , ) @cli_utils . activate_integrations def describe_stack_component_command ( name : Optional [ str ]) -> None : \"\"\"Prints details about the active/specified component.\"\"\" singular_display_name = _component_display_name ( component_type ) plural_display_name = _component_display_name ( component_type , plural = True ) repo = Repository () components = repo . get_stack_components ( component_type ) if len ( components ) == 0 : cli_utils . warning ( f \"No { plural_display_name } registered.\" ) return active_component_name = repo . active_stack . components [ component_type ] . name component = _get_stack_component ( component_type , component_name = name ) is_active = active_component_name == component . name cli_utils . title ( f \" { singular_display_name } :\" ) cli_utils . declare ( \"**ACTIVE** \\n \" if is_active else \"\" ) cli_utils . print_stack_component_configuration ( component ) return describe_stack_component_command","title":"generate_stack_component_describe_command()"},{"location":"api_docs/cli/stack_components/#zenml.cli.stack_components.generate_stack_component_down_command","text":"Generates a down command for the specific stack component type. Source code in zenml/cli/stack_components.py def generate_stack_component_down_command ( component_type : StackComponentType , ) -> Callable [[ Optional [ str ], bool ], None ]: \"\"\"Generates a `down` command for the specific stack component type.\"\"\" @click . argument ( \"name\" , type = str , required = False ) @click . option ( \"--force\" , \"-f\" , is_flag = True , help = \"Deprovisions local resources instead of suspending them.\" , ) def down_stack_component_command ( name : Optional [ str ] = None , force : bool = False ) -> None : \"\"\"Stops/Tears down the local deployment of a stack component.\"\"\" component = _get_stack_component ( component_type , component_name = name ) display_name = _component_display_name ( component_type ) if component . is_running and not force : cli_utils . declare ( f \"Suspending local resources for { display_name } \" f \"' { component . name } '.\" ) try : component . suspend () except NotImplementedError : cli_utils . error ( f \"Provisioning local resources not implemented for \" f \" { display_name } ' { component . name } '. If you want to \" f \"deprovision all resources for this component, use the \" f \"`--force/-f` flag.\" ) elif component . is_provisioned and force : cli_utils . declare ( f \"Deprovisioning resources for { display_name } \" f \"' { component . name } '.\" ) component . deprovision () else : cli_utils . declare ( f \"No provisioned resources found for { display_name } \" f \"' { component . name } '.\" ) return down_stack_component_command","title":"generate_stack_component_down_command()"},{"location":"api_docs/cli/stack_components/#zenml.cli.stack_components.generate_stack_component_get_command","text":"Generates a get command for the specific stack component type. Source code in zenml/cli/stack_components.py def generate_stack_component_get_command ( component_type : StackComponentType , ) -> Callable [[], None ]: \"\"\"Generates a `get` command for the specific stack component type.\"\"\" @cli_utils . activate_integrations def get_stack_component_command () -> None : \"\"\"Prints the name of the active component.\"\"\" active_stack = Repository () . active_stack component = active_stack . components . get ( component_type , None ) display_name = _component_display_name ( component_type ) if component : cli_utils . declare ( f \"Active { display_name } : { component . name } \" ) else : cli_utils . warning ( f \"No { display_name } set for active stack ( { active_stack . name } ).\" ) return get_stack_component_command","title":"generate_stack_component_get_command()"},{"location":"api_docs/cli/stack_components/#zenml.cli.stack_components.generate_stack_component_list_command","text":"Generates a list command for the specific stack component type. Source code in zenml/cli/stack_components.py def generate_stack_component_list_command ( component_type : StackComponentType , ) -> Callable [[], None ]: \"\"\"Generates a `list` command for the specific stack component type.\"\"\" @cli_utils . activate_integrations def list_stack_components_command () -> None : \"\"\"Prints a table of stack components.\"\"\" repo = Repository () components = repo . get_stack_components ( component_type ) display_name = _component_display_name ( component_type , plural = True ) if len ( components ) == 0 : cli_utils . warning ( f \"No { display_name } registered.\" ) return active_component_name = repo . active_stack . components [ component_type ] . name cli_utils . title ( f \" { display_name } :\" ) cli_utils . print_stack_component_list ( components , active_component_name = active_component_name ) return list_stack_components_command","title":"generate_stack_component_list_command()"},{"location":"api_docs/cli/stack_components/#zenml.cli.stack_components.generate_stack_component_logs_command","text":"Generates a logs command for the specific stack component type. Source code in zenml/cli/stack_components.py def generate_stack_component_logs_command ( component_type : StackComponentType , ) -> Callable [[ Optional [ str ], bool ], None ]: \"\"\"Generates a `logs` command for the specific stack component type.\"\"\" @click . argument ( \"name\" , type = str , required = False ) @click . option ( \"--follow\" , \"-f\" , is_flag = True , help = \"Follow the log file instead of just displaying the current logs.\" , ) def stack_component_logs_command ( name : Optional [ str ] = None , follow : bool = False ) -> None : \"\"\"Displays stack component logs.\"\"\" component = _get_stack_component ( component_type , component_name = name ) display_name = _component_display_name ( component_type ) log_file = component . log_file if not log_file or not fileio . file_exists ( log_file ): cli_utils . warning ( f \"Unable to find log file for { display_name } \" f \"' { component . name } '.\" ) return if follow : try : with open ( log_file , \"r\" ) as f : # seek to the end of the file f . seek ( 0 , 2 ) while True : line = f . readline () if not line : time . sleep ( 0.1 ) continue line = line . rstrip ( \" \\n \" ) click . echo ( line ) except KeyboardInterrupt : cli_utils . declare ( f \"Stopped following { display_name } logs.\" ) else : with open ( log_file , \"r\" ) as f : click . echo ( f . read ()) return stack_component_logs_command","title":"generate_stack_component_logs_command()"},{"location":"api_docs/cli/stack_components/#zenml.cli.stack_components.generate_stack_component_register_command","text":"Generates a register command for the specific stack component type. Source code in zenml/cli/stack_components.py def generate_stack_component_register_command ( component_type : StackComponentType , ) -> Callable [[ str , str , List [ str ]], None ]: \"\"\"Generates a `register` command for the specific stack component type.\"\"\" display_name = _component_display_name ( component_type ) @click . argument ( \"name\" , type = str , required = True , ) @click . option ( \"--type\" , \"-t\" , \"flavor\" , help = f \"The type of the { display_name } to register.\" , required = True , type = str , ) @click . argument ( \"args\" , nargs =- 1 , type = click . UNPROCESSED ) @cli_utils . activate_integrations def register_stack_component_command ( name : str , flavor : str , args : List [ str ] ) -> None : \"\"\"Registers a stack component.\"\"\" try : parsed_args = cli_utils . parse_unknown_options ( args ) except AssertionError as e : cli_utils . error ( str ( e )) return from zenml.stack.stack_component_class_registry import ( StackComponentClassRegistry , ) component_class = StackComponentClassRegistry . get_class ( component_type = component_type , component_flavor = flavor ) component = component_class ( name = name , ** parsed_args ) Repository () . register_stack_component ( component ) cli_utils . declare ( f \"Successfully registered { display_name } ` { name } `.\" ) return register_stack_component_command","title":"generate_stack_component_register_command()"},{"location":"api_docs/cli/stack_components/#zenml.cli.stack_components.generate_stack_component_up_command","text":"Generates a up command for the specific stack component type. Source code in zenml/cli/stack_components.py def generate_stack_component_up_command ( component_type : StackComponentType , ) -> Callable [[ Optional [ str ]], None ]: \"\"\"Generates a `up` command for the specific stack component type.\"\"\" @click . argument ( \"name\" , type = str , required = False ) def up_stack_component_command ( name : Optional [ str ] = None ) -> None : \"\"\"Deploys a stack component locally.\"\"\" component = _get_stack_component ( component_type , component_name = name ) display_name = _component_display_name ( component_type ) if component . is_running : cli_utils . declare ( f \"Local deployment is already running for { display_name } \" f \"' { component . name } '.\" ) return if not component . is_provisioned : cli_utils . declare ( f \"Provisioning local resources for { display_name } \" f \"' { component . name } '.\" ) try : component . provision () except NotImplementedError : cli_utils . error ( f \"Provisioning local resources not implemented for \" f \" { display_name } ' { component . name } '.\" ) if not component . is_running : cli_utils . declare ( f \"Resuming local resources for { display_name } \" f \"' { component . name } '.\" ) component . resume () return up_stack_component_command","title":"generate_stack_component_up_command()"},{"location":"api_docs/cli/stack_components/#zenml.cli.stack_components.register_all_stack_component_cli_commands","text":"Registers CLI commands for all stack components. Source code in zenml/cli/stack_components.py def register_all_stack_component_cli_commands () -> None : \"\"\"Registers CLI commands for all stack components.\"\"\" for component_type in StackComponentType : register_single_stack_component_cli_commands ( component_type , parent_group = cli )","title":"register_all_stack_component_cli_commands()"},{"location":"api_docs/cli/stack_components/#zenml.cli.stack_components.register_single_stack_component_cli_commands","text":"Registers all basic stack component CLI commands. Source code in zenml/cli/stack_components.py def register_single_stack_component_cli_commands ( component_type : StackComponentType , parent_group : click . Group ) -> None : \"\"\"Registers all basic stack component CLI commands.\"\"\" command_name = component_type . value . replace ( \"_\" , \"-\" ) singular_display_name = _component_display_name ( component_type ) plural_display_name = _component_display_name ( component_type , plural = True ) @parent_group . group ( command_name , help = f \"Commands to interact with { plural_display_name } .\" ) def command_group () -> None : \"\"\"Group commands for a single stack component type.\"\"\" # zenml stack-component get get_command = generate_stack_component_get_command ( component_type ) command_group . command ( \"get\" , help = f \"Get the name of the active { singular_display_name } .\" )( get_command ) # zenml stack-component describe describe_command = generate_stack_component_describe_command ( component_type ) command_group . command ( \"describe\" , help = f \"Show details about the (active) { singular_display_name } .\" , )( describe_command ) # zenml stack-component list list_command = generate_stack_component_list_command ( component_type ) command_group . command ( \"list\" , help = f \"List all registered { plural_display_name } .\" )( list_command ) # zenml stack-component register register_command = generate_stack_component_register_command ( component_type ) context_settings = { \"ignore_unknown_options\" : True } command_group . command ( \"register\" , context_settings = context_settings , help = f \"Register a new { singular_display_name } .\" , )( register_command ) # zenml stack-component delete delete_command = generate_stack_component_delete_command ( component_type ) command_group . command ( \"delete\" , help = f \"Delete a registered { singular_display_name } .\" )( delete_command ) # zenml stack-component up up_command = generate_stack_component_up_command ( component_type ) command_group . command ( \"up\" , help = f \"Deploys the { singular_display_name } locally if possible.\" )( up_command ) # zenml stack-component down down_command = generate_stack_component_down_command ( component_type ) command_group . command ( \"down\" , help = f \"Stops the local { singular_display_name } deployment.\" )( down_command ) # zenml stack-component logs logs_command = generate_stack_component_logs_command ( component_type ) command_group . command ( \"logs\" , help = f \"Display { singular_display_name } logs.\" )( logs_command )","title":"register_single_stack_component_cli_commands()"},{"location":"api_docs/cli/utils/","text":"Utils zenml.cli.utils activate_integrations ( func ) Decorator that activates all ZenML integrations. Source code in zenml/cli/utils.py def activate_integrations ( func : F ) -> F : \"\"\"Decorator that activates all ZenML integrations.\"\"\" @functools . wraps ( func ) def _wrapper ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function\"\"\" from zenml.integrations.registry import integration_registry integration_registry . activate_integrations () return func ( * args , ** kwargs ) return cast ( F , _wrapper ) confirmation ( text , * args , ** kwargs ) Echo a confirmation string on the CLI. Parameters: Name Type Description Default text str Input text string. required *args Any Args to be passed to click.confirm(). () **kwargs Any Kwargs to be passed to click.confirm(). {} Returns: Type Description bool Boolean based on user response. Source code in zenml/cli/utils.py def confirmation ( text : str , * args : Any , ** kwargs : Any ) -> bool : \"\"\"Echo a confirmation string on the CLI. Args: text: Input text string. *args: Args to be passed to click.confirm(). **kwargs: Kwargs to be passed to click.confirm(). Returns: Boolean based on user response. \"\"\" return click . confirm ( click . style ( text , fg = \"yellow\" ), * args , ** kwargs ) declare ( text ) Echo a declaration on the CLI. Parameters: Name Type Description Default text str Input text string. required Source code in zenml/cli/utils.py def declare ( text : str ) -> None : \"\"\"Echo a declaration on the CLI. Args: text: Input text string. \"\"\" click . echo ( click . style ( text , fg = \"green\" )) error ( text ) Echo an error string on the CLI. Parameters: Name Type Description Default text str Input text string. required Source code in zenml/cli/utils.py def error ( text : str ) -> None : \"\"\"Echo an error string on the CLI. Args: text: Input text string. Raises: click.ClickException when called. \"\"\" raise click . ClickException ( message = click . style ( text , fg = \"red\" , bold = True )) format_date ( dt , format = '%Y-%m- %d %H:%M:%S' ) Format a date into a string. Parameters: Name Type Description Default dt datetime Datetime object to be formatted. required format str The format in string you want the datetime formatted to. '%Y-%m-%d %H:%M:%S' Returns: Type Description str Formatted string according to specification. Source code in zenml/cli/utils.py def format_date ( dt : datetime . datetime , format : str = \"%Y-%m- %d %H:%M:%S\" ) -> str : \"\"\"Format a date into a string. Args: dt: Datetime object to be formatted. format: The format in string you want the datetime formatted to. Returns: Formatted string according to specification. \"\"\" if dt is None : return \"\" # make sure this is UTC dt = dt . replace ( tzinfo = tz . tzutc ()) if sys . platform != \"win32\" : # On non-windows get local time zone. local_zone = tz . tzlocal () dt = dt . astimezone ( local_zone ) else : logger . warning ( \"On Windows, all times are displayed in UTC timezone.\" ) return dt . strftime ( format ) install_package ( package ) Installs pypi package into the current environment with pip Source code in zenml/cli/utils.py def install_package ( package : str ) -> None : \"\"\"Installs pypi package into the current environment with pip\"\"\" subprocess . check_call ([ sys . executable , \"-m\" , \"pip\" , \"install\" , package ]) parse_unknown_options ( args ) Parse unknown options from the CLI. Parameters: Name Type Description Default args List[str] A list of strings from the CLI. required Returns: Type Description Dict[str, Any] Dict of parsed args. Source code in zenml/cli/utils.py def parse_unknown_options ( args : List [ str ]) -> Dict [ str , Any ]: \"\"\"Parse unknown options from the CLI. Args: args: A list of strings from the CLI. Returns: Dict of parsed args. \"\"\" warning_message = ( \"Please provide args with a proper \" \"identifier as the key and the following structure: \" '--custom_argument=\"value\"' ) assert all ( a . startswith ( \"--\" ) for a in args ), warning_message assert all ( len ( a . split ( \"=\" )) == 2 for a in args ), warning_message p_args = [ a . lstrip ( \"--\" ) . split ( \"=\" ) for a in args ] assert all ( k . isidentifier () for k , _ in p_args ), warning_message r_args = { k : v for k , v in p_args } assert len ( p_args ) == len ( r_args ), \"Replicated arguments!\" return r_args pretty_print ( obj ) Pretty print an object on the CLI. Parameters: Name Type Description Default obj Any Any object with a str method defined. required Source code in zenml/cli/utils.py def pretty_print ( obj : Any ) -> None : \"\"\"Pretty print an object on the CLI. Args: obj: Any object with a __str__ method defined. \"\"\" click . echo ( str ( obj )) print_stack_component_configuration ( component ) Prints the configuration options of a stack component. Source code in zenml/cli/utils.py def print_stack_component_configuration ( component : StackComponent ) -> None : \"\"\"Prints the configuration options of a stack component.\"\"\" declare ( f \"NAME: { component . name } \" ) for key , value in component . dict ( exclude = { \"name\" }) . items (): declare ( f \" { key . upper () } : { value } \" ) print_stack_component_list ( components , active_component_name ) Prints a table with configuration options for a list of stack components. If a component is active (its name matches the active_component_name ), it will be highlighted in a separate table column. Parameters: Name Type Description Default components List[zenml.stack.stack_component.StackComponent] List of stack components to print. required active_component_name str Name of the component that is currently active. required Source code in zenml/cli/utils.py def print_stack_component_list ( components : List [ StackComponent ], active_component_name : str ) -> None : \"\"\"Prints a table with configuration options for a list of stack components. If a component is active (its name matches the `active_component_name`), it will be highlighted in a separate table column. Args: components: List of stack components to print. active_component_name: Name of the component that is currently active. \"\"\" configurations = [] for component in components : is_active = component . name == active_component_name component_config = { \"ACTIVE\" : \"*\" if is_active else \"\" , ** { key . upper (): value for key , value in component . dict () . items ()}, } configurations . append ( component_config ) print_table ( configurations ) print_table ( obj ) Echoes the list of dicts in a table format. The input object should be a List of Dicts. Each item in that list represent a line in the Table. Each dict should have the same keys. The keys of the dict will be used as headers of the resulting table. Parameters: Name Type Description Default obj List[Dict[str, Any]] A List containing dictionaries. required Source code in zenml/cli/utils.py def print_table ( obj : List [ Dict [ str , Any ]]) -> None : \"\"\"Echoes the list of dicts in a table format. The input object should be a List of Dicts. Each item in that list represent a line in the Table. Each dict should have the same keys. The keys of the dict will be used as headers of the resulting table. Args: obj: A List containing dictionaries. \"\"\" click . echo ( tabulate ( obj , headers = \"keys\" )) title ( text ) Echo a title formatted string on the CLI. Parameters: Name Type Description Default text str Input text string. required Source code in zenml/cli/utils.py def title ( text : str ) -> None : \"\"\"Echo a title formatted string on the CLI. Args: text: Input text string. \"\"\" click . echo ( click . style ( text . upper (), fg = \"cyan\" , bold = True , underline = True )) uninstall_package ( package ) Uninstalls pypi package from the current environment with pip Source code in zenml/cli/utils.py def uninstall_package ( package : str ) -> None : \"\"\"Uninstalls pypi package from the current environment with pip\"\"\" subprocess . check_call ( [ sys . executable , \"-m\" , \"pip\" , \"uninstall\" , \"-y\" , package ] ) warning ( text ) Echo a warning string on the CLI. Parameters: Name Type Description Default text str Input text string. required Source code in zenml/cli/utils.py def warning ( text : str ) -> None : \"\"\"Echo a warning string on the CLI. Args: text: Input text string. \"\"\" click . echo ( click . style ( text , fg = \"yellow\" , bold = True ))","title":"Utils"},{"location":"api_docs/cli/utils/#utils","text":"","title":"Utils"},{"location":"api_docs/cli/utils/#zenml.cli.utils","text":"","title":"utils"},{"location":"api_docs/cli/utils/#zenml.cli.utils.activate_integrations","text":"Decorator that activates all ZenML integrations. Source code in zenml/cli/utils.py def activate_integrations ( func : F ) -> F : \"\"\"Decorator that activates all ZenML integrations.\"\"\" @functools . wraps ( func ) def _wrapper ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function\"\"\" from zenml.integrations.registry import integration_registry integration_registry . activate_integrations () return func ( * args , ** kwargs ) return cast ( F , _wrapper )","title":"activate_integrations()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.confirmation","text":"Echo a confirmation string on the CLI. Parameters: Name Type Description Default text str Input text string. required *args Any Args to be passed to click.confirm(). () **kwargs Any Kwargs to be passed to click.confirm(). {} Returns: Type Description bool Boolean based on user response. Source code in zenml/cli/utils.py def confirmation ( text : str , * args : Any , ** kwargs : Any ) -> bool : \"\"\"Echo a confirmation string on the CLI. Args: text: Input text string. *args: Args to be passed to click.confirm(). **kwargs: Kwargs to be passed to click.confirm(). Returns: Boolean based on user response. \"\"\" return click . confirm ( click . style ( text , fg = \"yellow\" ), * args , ** kwargs )","title":"confirmation()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.declare","text":"Echo a declaration on the CLI. Parameters: Name Type Description Default text str Input text string. required Source code in zenml/cli/utils.py def declare ( text : str ) -> None : \"\"\"Echo a declaration on the CLI. Args: text: Input text string. \"\"\" click . echo ( click . style ( text , fg = \"green\" ))","title":"declare()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.error","text":"Echo an error string on the CLI. Parameters: Name Type Description Default text str Input text string. required Source code in zenml/cli/utils.py def error ( text : str ) -> None : \"\"\"Echo an error string on the CLI. Args: text: Input text string. Raises: click.ClickException when called. \"\"\" raise click . ClickException ( message = click . style ( text , fg = \"red\" , bold = True ))","title":"error()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.format_date","text":"Format a date into a string. Parameters: Name Type Description Default dt datetime Datetime object to be formatted. required format str The format in string you want the datetime formatted to. '%Y-%m-%d %H:%M:%S' Returns: Type Description str Formatted string according to specification. Source code in zenml/cli/utils.py def format_date ( dt : datetime . datetime , format : str = \"%Y-%m- %d %H:%M:%S\" ) -> str : \"\"\"Format a date into a string. Args: dt: Datetime object to be formatted. format: The format in string you want the datetime formatted to. Returns: Formatted string according to specification. \"\"\" if dt is None : return \"\" # make sure this is UTC dt = dt . replace ( tzinfo = tz . tzutc ()) if sys . platform != \"win32\" : # On non-windows get local time zone. local_zone = tz . tzlocal () dt = dt . astimezone ( local_zone ) else : logger . warning ( \"On Windows, all times are displayed in UTC timezone.\" ) return dt . strftime ( format )","title":"format_date()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.install_package","text":"Installs pypi package into the current environment with pip Source code in zenml/cli/utils.py def install_package ( package : str ) -> None : \"\"\"Installs pypi package into the current environment with pip\"\"\" subprocess . check_call ([ sys . executable , \"-m\" , \"pip\" , \"install\" , package ])","title":"install_package()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.parse_unknown_options","text":"Parse unknown options from the CLI. Parameters: Name Type Description Default args List[str] A list of strings from the CLI. required Returns: Type Description Dict[str, Any] Dict of parsed args. Source code in zenml/cli/utils.py def parse_unknown_options ( args : List [ str ]) -> Dict [ str , Any ]: \"\"\"Parse unknown options from the CLI. Args: args: A list of strings from the CLI. Returns: Dict of parsed args. \"\"\" warning_message = ( \"Please provide args with a proper \" \"identifier as the key and the following structure: \" '--custom_argument=\"value\"' ) assert all ( a . startswith ( \"--\" ) for a in args ), warning_message assert all ( len ( a . split ( \"=\" )) == 2 for a in args ), warning_message p_args = [ a . lstrip ( \"--\" ) . split ( \"=\" ) for a in args ] assert all ( k . isidentifier () for k , _ in p_args ), warning_message r_args = { k : v for k , v in p_args } assert len ( p_args ) == len ( r_args ), \"Replicated arguments!\" return r_args","title":"parse_unknown_options()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.pretty_print","text":"Pretty print an object on the CLI. Parameters: Name Type Description Default obj Any Any object with a str method defined. required Source code in zenml/cli/utils.py def pretty_print ( obj : Any ) -> None : \"\"\"Pretty print an object on the CLI. Args: obj: Any object with a __str__ method defined. \"\"\" click . echo ( str ( obj ))","title":"pretty_print()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.print_stack_component_configuration","text":"Prints the configuration options of a stack component. Source code in zenml/cli/utils.py def print_stack_component_configuration ( component : StackComponent ) -> None : \"\"\"Prints the configuration options of a stack component.\"\"\" declare ( f \"NAME: { component . name } \" ) for key , value in component . dict ( exclude = { \"name\" }) . items (): declare ( f \" { key . upper () } : { value } \" )","title":"print_stack_component_configuration()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.print_stack_component_list","text":"Prints a table with configuration options for a list of stack components. If a component is active (its name matches the active_component_name ), it will be highlighted in a separate table column. Parameters: Name Type Description Default components List[zenml.stack.stack_component.StackComponent] List of stack components to print. required active_component_name str Name of the component that is currently active. required Source code in zenml/cli/utils.py def print_stack_component_list ( components : List [ StackComponent ], active_component_name : str ) -> None : \"\"\"Prints a table with configuration options for a list of stack components. If a component is active (its name matches the `active_component_name`), it will be highlighted in a separate table column. Args: components: List of stack components to print. active_component_name: Name of the component that is currently active. \"\"\" configurations = [] for component in components : is_active = component . name == active_component_name component_config = { \"ACTIVE\" : \"*\" if is_active else \"\" , ** { key . upper (): value for key , value in component . dict () . items ()}, } configurations . append ( component_config ) print_table ( configurations )","title":"print_stack_component_list()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.print_table","text":"Echoes the list of dicts in a table format. The input object should be a List of Dicts. Each item in that list represent a line in the Table. Each dict should have the same keys. The keys of the dict will be used as headers of the resulting table. Parameters: Name Type Description Default obj List[Dict[str, Any]] A List containing dictionaries. required Source code in zenml/cli/utils.py def print_table ( obj : List [ Dict [ str , Any ]]) -> None : \"\"\"Echoes the list of dicts in a table format. The input object should be a List of Dicts. Each item in that list represent a line in the Table. Each dict should have the same keys. The keys of the dict will be used as headers of the resulting table. Args: obj: A List containing dictionaries. \"\"\" click . echo ( tabulate ( obj , headers = \"keys\" ))","title":"print_table()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.title","text":"Echo a title formatted string on the CLI. Parameters: Name Type Description Default text str Input text string. required Source code in zenml/cli/utils.py def title ( text : str ) -> None : \"\"\"Echo a title formatted string on the CLI. Args: text: Input text string. \"\"\" click . echo ( click . style ( text . upper (), fg = \"cyan\" , bold = True , underline = True ))","title":"title()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.uninstall_package","text":"Uninstalls pypi package from the current environment with pip Source code in zenml/cli/utils.py def uninstall_package ( package : str ) -> None : \"\"\"Uninstalls pypi package from the current environment with pip\"\"\" subprocess . check_call ( [ sys . executable , \"-m\" , \"pip\" , \"uninstall\" , \"-y\" , package ] )","title":"uninstall_package()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.warning","text":"Echo a warning string on the CLI. Parameters: Name Type Description Default text str Input text string. required Source code in zenml/cli/utils.py def warning ( text : str ) -> None : \"\"\"Echo a warning string on the CLI. Args: text: Input text string. \"\"\" click . echo ( click . style ( text , fg = \"yellow\" , bold = True ))","title":"warning()"},{"location":"api_docs/cli/version/","text":"Version zenml.cli.version","title":"Version"},{"location":"api_docs/cli/version/#version","text":"","title":"Version"},{"location":"api_docs/cli/version/#zenml.cli.version","text":"","title":"version"}]}